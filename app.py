import os
import sys
import traceback
import numpy as np
from PIL import Image

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.getcwd())

# æ¡ä»¶å¯¼å…¥å¤„ç†
try:
    import streamlit as st  # type: ignore
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False
    print("Streamlitæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install streamlit")
    sys.exit(1)

try:
    import torch
    import torch.nn.functional as F
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    torch = None
    F = None

try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    cv2 = None

# å°è¯•å¯¼å…¥DINOv3ç›¸å…³æ¨¡å—
try:
    import timm
    from torchvision import transforms
    DINOV3_AVAILABLE = True
except ImportError:
    DINOV3_AVAILABLE = False
    timm = None
    transforms = None
    print("DINOv3ç›¸å…³æ¨¡å—æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install timm torchvision")

# ç¡®ä¿æ‰€æœ‰æ¨¡å—åœ¨å…¨å±€ä½œç”¨åŸŸä¸­å¯ç”¨ï¼Œé¿å…é™æ€åˆ†æè­¦å‘Š
if CV2_AVAILABLE:
    globals()['cv2'] = cv2
else:
    globals()['cv2'] = None
    
if DINOV3_AVAILABLE:
    globals()['timm'] = timm
    globals()['transforms'] = transforms
else:
    globals()['timm'] = None
    globals()['transforms'] = None
    
if TORCH_AVAILABLE:
    globals()['torch'] = torch
    globals()['F'] = F
else:
    globals()['torch'] = None
    globals()['F'] = None

# å°è¯•å¯¼å…¥MMSegmentation
try:
    from mmseg.apis import init_segmentor, inference_segmentor  # type: ignore
    MMSEG_AVAILABLE = True
    MMSEG_ERROR = None
    
    # ç¡®ä¿å¿…è¦çš„æ¨¡å—è¢«å¯¼å…¥ï¼ˆå…¼å®¹ä¸åŒç‰ˆæœ¬ï¼‰
    try:
        import mmseg.models  # type: ignore
        import mmseg.datasets  # type: ignore
    except ImportError:
        pass
except (ImportError, ModuleNotFoundError) as e:
    MMSEG_AVAILABLE = False
    MMSEG_ERROR = str(e)
    print(f"MMSegmentationå¯¼å…¥å¤±è´¥: {e}")
    print("è¯·æ­£ç¡®å®‰è£…MMSegmentationåŠå…¶ä¾èµ–")
    sys.exit(1)

# --- é…ç½® ---
# è¯·å°†è¿™é‡Œçš„è·¯å¾„ä¿®æ”¹ä¸ºä½ è‡ªå·±çš„æ–‡ä»¶è·¯å¾„
CONFIG_FILE = 'configs/final_standalone_config.py'
CHECKPOINT_FILE = 'checkpoints/best_mIoU_iter_6000.pth'

# EarthVQAé¢„è®­ç»ƒæƒé‡é…ç½®
EARTHVQA_CONFIG_FILE = 'configs/train_earthvqa_final.py'
EARTHVQA_CHECKPOINT_FILE = 'checkpoints/EarthVQA-15000.pth'

# DINOv3 SAT 493Mæ¨¡å‹é…ç½®
DINOV3_CHECKPOINT_FILE = 'checkpoints/regular_checkpoint_stage_4_epoch_26.pth'

# DINOv3å®˜æ–¹é¢„è®­ç»ƒæƒé‡é…ç½®
DINOV3_OFFICIAL_CHECKPOINT_FILE = 'checkpoints/dinov3_vitl16_pretrain_sat493m-eadcf0ff.pth'

DEVICE = 'cpu'

# LoveDAæ•°æ®é›†çš„è°ƒè‰²æ¿ (RGBæ ¼å¼)
# 7ä¸ªç±»åˆ«ï¼šèƒŒæ™¯ã€å»ºç­‘ã€é“è·¯ã€æ°´ä½“ã€è´«ç˜ åœŸåœ°ã€æ£®æ—ã€å†œä¸š
PALETTE = [
    [255, 255, 255],  # èƒŒæ™¯ - ç™½è‰²
    [255, 0, 0],      # å»ºç­‘ - çº¢è‰²
    [255, 255, 0],    # é“è·¯ - é»„è‰²
    [0, 0, 255],      # æ°´ä½“ - è“è‰²
    [159, 129, 183],  # è´«ç˜ åœŸåœ° - ç´«è‰²
    [0, 255, 0],      # æ£®æ— - ç»¿è‰²
    [255, 195, 128]   # å†œä¸š - æ©™è‰²
]

# EarthVQAæ•°æ®é›†çš„è°ƒè‰²æ¿ (RGBæ ¼å¼)
# æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šbackground â€“ 1, building â€“ 2, road â€“ 3, water â€“ 4, barren â€“ 5, forest â€“ 6, agriculture â€“ 7, playground - 8
# no-dataåŒºåŸŸä¸º0ï¼Œéœ€è¦9ä¸ªé¢œè‰²ï¼ˆç´¢å¼•0-8ï¼‰
EARTHVQA_PALETTE = [
    [0, 0, 0],        # no-dataåŒºåŸŸ - é»‘è‰² (ç´¢å¼•0)
    [255, 255, 255],  # background - ç™½è‰² (ç´¢å¼•1)
    [255, 0, 0],      # building - çº¢è‰² (ç´¢å¼•2)
    [255, 255, 0],    # road - é»„è‰² (ç´¢å¼•3)
    [0, 0, 255],      # water - è“è‰² (ç´¢å¼•4)
    [159, 129, 183],  # barren - ç´«è‰² (ç´¢å¼•5)
    [0, 255, 0],      # forest - ç»¿è‰² (ç´¢å¼•6)
    [255, 195, 128],  # agriculture - æ©™è‰² (ç´¢å¼•7)
    [255, 0, 255]     # playground - å“çº¢è‰² (ç´¢å¼•8)
]

CLASS_NAMES = ['èƒŒæ™¯', 'å»ºç­‘', 'é“è·¯', 'æ°´ä½“', 'è´«ç˜ åœŸåœ°', 'æ£®æ—', 'å†œä¸š']
EARTHVQA_CLASS_NAMES = ['no-data', 'background', 'building', 'road', 'water', 'barren', 'forest', 'agriculture', 'playground']

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def get_mask_path(uploaded_filename):
    """
    æ ¹æ®ä¸Šä¼ çš„æ–‡ä»¶åï¼ŒæŸ¥æ‰¾å¯¹åº”çš„éªŒè¯é›†æ©ç æ–‡ä»¶
    """
    if uploaded_filename is None:
        return None
    
    # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
    base_name = os.path.splitext(uploaded_filename)[0]
    
    # æ£€æŸ¥é¡¹ç›®æ ¹ç›®å½•ä¸‹æ˜¯å¦æœ‰å¯¹åº”çš„æ©ç æ–‡ä»¶
    mask_filename = f"{base_name}_mask.png"
    mask_path = os.path.join(os.getcwd(), mask_filename)
    
    if os.path.exists(mask_path):
        return mask_path
    
    return None

@st.cache_resource
def load_model(config, checkpoint):
    """
    ä½¿ç”¨MMSegmentationåŠ è½½SegFormeræ¨¡å‹ã€‚
    """
    return _load_model_internal(config, checkpoint)

@st.cache_resource
def load_earthvqa_model(config, checkpoint):
    """
    ä½¿ç”¨MMSegmentationåŠ è½½EarthVQAé¢„è®­ç»ƒæ¨¡å‹ã€‚
    """
    return _load_model_internal(config, checkpoint)

@st.cache_resource
def load_dinov3_model(checkpoint_path):
    """
    åŠ è½½DINOv3åˆ†å‰²æ¨¡å‹ï¼ˆç”¨æˆ·è®­ç»ƒçš„å®Œæ•´æ¨¡å‹ï¼‰ã€‚
    """
    if not DINOV3_AVAILABLE:
        st.error("âŒ DINOv3ç›¸å…³æ¨¡å—æœªå®‰è£…")
        return None
    
    if not TORCH_AVAILABLE or torch is None:
        st.error("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½DINOv3æ¨¡å‹")
        return None
        
    if not os.path.exists(checkpoint_path):
        st.error(f"âŒ DINOv3æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return None
    
    try:
        # åŠ è½½ç”¨æˆ·è®­ç»ƒçš„å®Œæ•´åˆ†å‰²æ¨¡å‹æƒé‡
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # æ£€æŸ¥æƒé‡æ–‡ä»¶ç»“æ„
        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            model_config = checkpoint.get('model_config', {})
            st.info(f"ğŸ“‹ æ¨¡å‹é…ç½®: {model_config}")
        elif 'model' in checkpoint:
            state_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        # è¿”å›åŒ…å«æƒé‡å’Œé…ç½®çš„å­—å…¸ï¼Œè€Œä¸æ˜¯å®é™…çš„æ¨¡å‹å¯¹è±¡
        # å› ä¸ºè¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„åˆ†å‰²æ¨¡å‹ï¼Œä¸æ˜¯çº¯DINOv3ç‰¹å¾æå–å™¨
        model_info = {
            'state_dict': state_dict,
            'config': checkpoint.get('model_config', {}),
            'type': 'segmentation_model'
        }
        
        st.success("âœ… DINOv3åˆ†å‰²æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        return model_info
        
    except Exception as e:
        st.error(f"âŒ DINOv3æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None

def _load_model_internal(config, checkpoint):
    if not MMSEG_AVAILABLE:
        st.error("âŒ MMSegmentationæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
        return None
    
    if not os.path.exists(checkpoint):
        st.error(f"âŒ æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint}")
        return None
        
    if not os.path.exists(config):
        st.error(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config}")
        return None
    
    try:
        # æ£€æŸ¥torchæ˜¯å¦å¯ç”¨
        if not TORCH_AVAILABLE or torch is None:
            st.error("âŒ PyTorchæœªå®‰è£…ï¼Œæ— æ³•åŠ è½½æ¨¡å‹")
            return None
            
        # å…ˆåŠ è½½checkpointæ£€æŸ¥æ˜¯å¦åŒ…å«CLASSESå…ƒæ•°æ®
        checkpoint_data = torch.load(checkpoint, map_location='cpu')
        
        # ç¡®ä¿checkpointåŒ…å«å¿…è¦çš„metaä¿¡æ¯
        if 'meta' not in checkpoint_data:
            checkpoint_data['meta'] = {}
        
        # æ£€æŸ¥å¹¶æ·»åŠ ç¼ºå¤±çš„å…ƒæ•°æ®
        needs_temp_file = False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯EarthVQAæ¨¡å‹ï¼ˆé€šè¿‡é…ç½®æ–‡ä»¶ååˆ¤æ–­ï¼‰
        is_earthvqa = 'earthvqa' in config.lower()
        
        # å¦‚æœç¼ºå°‘CLASSESï¼Œæ·»åŠ å¯¹åº”çš„ç±»åˆ«ä¿¡æ¯
        if 'CLASSES' not in checkpoint_data['meta']:
            if is_earthvqa:
                # EarthVQAå®˜æ–¹æ ‡å‡†ï¼šbackground-1, building-2, road-3, water-4, barren-5, forest-6, agriculture-7, playground-8, no-data-0
                checkpoint_data['meta']['CLASSES'] = ['no-data', 'background', 'building', 'road', 'water', 'barren', 'forest', 'agriculture', 'playground']
            else:
                # LoveDAçš„7ä¸ªç±»åˆ«
                checkpoint_data['meta']['CLASSES'] = ['èƒŒæ™¯', 'å»ºç­‘', 'é“è·¯', 'æ°´ä½“', 'è´«ç˜ åœŸåœ°', 'æ£®æ—', 'å†œä¸š']
            needs_temp_file = True
            
        # å¦‚æœç¼ºå°‘PALETTEï¼Œæ·»åŠ å¯¹åº”çš„è°ƒè‰²æ¿ä¿¡æ¯
        if 'PALETTE' not in checkpoint_data['meta']:
            if is_earthvqa:
                # EarthVQAå®˜æ–¹æ ‡å‡†è°ƒè‰²æ¿ï¼šno-data-0(é»‘è‰²), background-1(ç™½è‰²), building-2(çº¢è‰²), road-3(é»„è‰²), water-4(è“è‰²), barren-5(ç´«è‰²), forest-6(ç»¿è‰²), agriculture-7(æ©™è‰²), playground-8(å“çº¢è‰²)
                checkpoint_data['meta']['PALETTE'] = [
                    [0, 0, 0],        # no-dataåŒºåŸŸ - é»‘è‰² (ç´¢å¼•0)
                    [255, 255, 255],  # background - ç™½è‰² (ç´¢å¼•1)
                    [255, 0, 0],      # building - çº¢è‰² (ç´¢å¼•2)
                    [255, 255, 0],    # road - é»„è‰² (ç´¢å¼•3)
                    [0, 0, 255],      # water - è“è‰² (ç´¢å¼•4)
                    [159, 129, 183],  # barren - ç´«è‰² (ç´¢å¼•5)
                    [0, 255, 0],      # forest - ç»¿è‰² (ç´¢å¼•6)
                    [255, 195, 128],  # agriculture - æ©™è‰² (ç´¢å¼•7)
                    [255, 0, 255]     # playground - å“çº¢è‰² (ç´¢å¼•8)
                ]
            else:
                # LoveDAçš„7ä¸ªé¢œè‰²
                checkpoint_data['meta']['PALETTE'] = [
                    [255, 255, 255],  # èƒŒæ™¯ - ç™½è‰²
                    [255, 0, 0],      # å»ºç­‘ - çº¢è‰²
                    [255, 255, 0],    # é“è·¯ - é»„è‰²
                    [0, 0, 255],      # æ°´ä½“ - è“è‰²
                    [159, 129, 183],  # è´«ç˜ åœŸåœ° - ç´«è‰²
                    [0, 255, 0],      # æ£®æ— - ç»¿è‰²
                    [255, 195, 128]   # å†œä¸š - æ©™è‰²
                ]
            needs_temp_file = True
            
        # å¦‚æœéœ€è¦ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶
        if needs_temp_file:
            temp_checkpoint = checkpoint + '.temp'
            torch.save(checkpoint_data, temp_checkpoint)
            checkpoint_to_use = temp_checkpoint
        else:
            checkpoint_to_use = checkpoint
        
        # ä½¿ç”¨MMSegmentation APIåŠ è½½æ¨¡å‹
        if init_segmentor is not None:
            model = init_segmentor(config, checkpoint_to_use, device=DEVICE)
            
            # === ADD THIS DEBUG LINE ===
            print("\n--- Verifying Loaded Config Keys ---")
            print(list(model.cfg.keys()))
            print("------------------------------------\n")
            # ===========================
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if checkpoint_to_use != checkpoint and os.path.exists(checkpoint_to_use):
                os.remove(checkpoint_to_use)
            
            # æ‰‹åŠ¨è®¾ç½®CLASSESå±æ€§ï¼ˆåŒé‡ä¿é™©ï¼‰
            if not hasattr(model, 'CLASSES') or model.CLASSES is None:
                if is_earthvqa:
                    model.CLASSES = ['no-data', 'background', 'building', 'road', 'water', 'barren', 'forest', 'agriculture', 'playground']
                else:
                    model.CLASSES = ['èƒŒæ™¯', 'å»ºç­‘', 'é“è·¯', 'æ°´ä½“', 'è´«ç˜ åœŸåœ°', 'æ£®æ—', 'å†œä¸š']
            
            st.success("âœ… SegFormeræ¨¡å‹åŠ è½½æˆåŠŸ")
            return model
        else:
            st.error("âŒ init_segmentorå‡½æ•°ä¸å¯ç”¨")
            return None
    except Exception as e:
        st.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        import traceback
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def run_inference(model, image_np):
    """
    ç»ˆæç®€åŒ–ç‰ˆæ¨ç†å‡½æ•°ï¼Œç”¨äºå…¼å®¹æ—§ç‰ˆ MMSegmentation v0.xã€‚
    æ­¤å‡½æ•°æ‰‹åŠ¨è¿›è¡Œæ‰€æœ‰å¿…è¦çš„æ•°æ®å‡†å¤‡ï¼Œä¸å†ä¾èµ–å¤æ‚çš„ mmcv æˆ– mmseg æ•°æ®æµã€‚
    """
    if model is None:
        st.error("âŒ æ¨¡å‹æœªåŠ è½½")
        return None
    
    try:
        import torch
        cfg = model.cfg
        device = next(model.parameters()).device

        # 1. æ‰‹åŠ¨è¿›è¡Œæ•°æ®é¢„å¤„ç†
        # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨æ­£ç¡®çš„å½’ä¸€åŒ–å‚æ•°
        # æ£€æŸ¥æ˜¯å¦ä¸ºEarthVQAé¢„è®­ç»ƒæ¨¡å‹ï¼ˆé€šè¿‡é…ç½®æ–‡ä»¶è·¯å¾„æˆ–æ¨¡å‹å±æ€§åˆ¤æ–­ï¼‰
        is_earthvqa_model = False
        if hasattr(cfg, 'filename') and cfg.filename:
            is_earthvqa_model = 'earthvqa' in str(cfg.filename).lower() or 'sfpnr50' in str(cfg.filename).lower()
        
        if is_earthvqa_model:
            # EarthVQAé¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨ImageNetæ ‡å‡†å‚æ•°ï¼ˆä»å®˜æ–¹é¡¹ç›®ç¡®è®¤ï¼‰
            mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)
            std = np.array([58.395, 57.12, 57.375], dtype=np.float32)
            st.info("ğŸ”§ ä½¿ç”¨EarthVQAå®˜æ–¹å½’ä¸€åŒ–å‚æ•°: ImageNetæ ‡å‡†")
        else:
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æœ‰data_preprocessoré…ç½®
            if hasattr(model, 'data_preprocessor') and hasattr(model.data_preprocessor, 'mean'):
                # ä½¿ç”¨æ¨¡å‹çš„data_preprocessorå‚æ•°
                mean = model.data_preprocessor.mean.cpu().numpy() if hasattr(model.data_preprocessor.mean, 'cpu') else np.array(model.data_preprocessor.mean)
                std = model.data_preprocessor.std.cpu().numpy() if hasattr(model.data_preprocessor.std, 'cpu') else np.array(model.data_preprocessor.std)
            else:
                # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„data_preprocessor
                if hasattr(cfg, 'model') and 'data_preprocessor' in cfg.model:
                    preprocessor_cfg = cfg.model.data_preprocessor
                    mean = np.array(preprocessor_cfg.get('mean', [73.53223947628777, 80.01710095339912, 74.59297778068898]), dtype=np.float32)
                    std = np.array(preprocessor_cfg.get('std', [41.511366098369635, 35.66528876209687, 33.75830885257866]), dtype=np.float32)
                else:
                    # ä½¿ç”¨è‡ªè®­ç»ƒæ¨¡å‹çš„æ ‡å‡†åŒ–å‚æ•°
                    mean = np.array([73.53223947628777, 80.01710095339912, 74.59297778068898], dtype=np.float32)
                    std = np.array([41.511366098369635, 35.66528876209687, 33.75830885257866], dtype=np.float32)
        
        # å½’ä¸€åŒ–
        image_normalized = (image_np.astype(np.float32) - mean) / std
        
        # HWC -> CHW (é«˜, å®½, é€šé“ -> é€šé“, é«˜, å®½)
        image_transposed = image_normalized.transpose(2, 0, 1)
        
        # è½¬æ¢ä¸ºPyTorch Tensorå¹¶å¢åŠ ä¸€ä¸ªBatchç»´åº¦ (B, C, H, W)
        image_tensor = torch.from_numpy(image_transposed).unsqueeze(0).to(device)

        # 2. æ‰‹åŠ¨åˆ›å»ºæœ€ç®€åŒ–çš„å…ƒæ•°æ® (img_metas)
        # æ¨¡å‹åœ¨æ¨ç†æ—¶éœ€è¦è¿™äº›ä¿¡æ¯æ¥æ­£ç¡®ç¼©æ”¾ç»“æœ
        meta_dict = {
            'ori_shape': image_np.shape,
            'img_shape': image_np.shape,
            'pad_shape': image_np.shape,
            'scale_factor': 1.0,
            'flip': False,  # ç¡®ä¿'flip'é”®å­˜åœ¨
            'flip_direction': None
        }

        # 3. ä»¥æ¨¡å‹æœŸæœ›çš„åˆ—è¡¨æ ¼å¼è¿›è¡Œè°ƒç”¨
        # è¿™æ˜¯è§£å†³ 'imgs must be a list' å’Œ 'KeyError: flip' çš„æ ¸å¿ƒ
        with torch.no_grad():
            result = model(
                img=[image_tensor],
                img_metas=[[meta_dict]], # æ³¨æ„è¿™é‡Œæ˜¯åŒé‡åˆ—è¡¨
                return_loss=False
            )
        
        # 4. è¿”å›ç»“æœ
        # æ¨¡å‹åœ¨æ¨ç†æ¨¡å¼ä¸‹è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­åŒ…å«æ¯ä¸ªå›¾åƒçš„åˆ†å‰²å›¾
        # éœ€è¦æ­£ç¡®æå–åˆ†å‰²ç»“æœ
        if isinstance(result, list) and len(result) > 0:
            seg_result = result[0]
            # å¦‚æœç»“æœæ˜¯tensorï¼Œè½¬æ¢ä¸ºnumpy
            if hasattr(seg_result, 'cpu'):
                seg_result = seg_result.cpu().numpy()
            # å¦‚æœæ˜¯3Dæ•°ç»„(C,H,W)ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
            if len(seg_result.shape) == 3:
                seg_result = seg_result[0]
            return seg_result
        else:
            st.error(f"âŒ æ¨ç†ç»“æœæ ¼å¼å¼‚å¸¸: {type(result)}")
            return None

    except Exception as e:
        st.error(f"âŒ æ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def run_inference_tta(model, image_np, config, device):
    """
    æ‰§è¡ŒTTAæ¨ç†ï¼Œæ”¯æŒå¤šå°ºåº¦å’Œç¿»è½¬å¢å¼º
    
    Args:
        model: åŠ è½½çš„MMSegmentationæ¨¡å‹
        image_np: è¾“å…¥å›¾åƒ (numpy array, HWCæ ¼å¼)
        config: æ¨¡å‹é…ç½®
        device: æ¨ç†è®¾å¤‡
    
    Returns:
        numpy array: èåˆåçš„åˆ†å‰²ç»“æœ
    """
    try:
        # åœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥å¿…è¦çš„åº“
        import cv2 as cv2_local
        import torch as torch_local
        
        # TTAé…ç½®ï¼šæ›´å¤šæ ·åŒ–çš„å˜æ¢ç»„åˆæ‰“ç ´å¯¹ç§°æ€§
        scales = [0.9, 1.0, 1.1]  # å¤šå°ºåº¦
        transforms = [
            {'h_flip': False, 'v_flip': False, 'rotate': 0},    # åŸå§‹
            {'h_flip': True, 'v_flip': False, 'rotate': 0},     # æ°´å¹³ç¿»è½¬
            {'h_flip': False, 'v_flip': True, 'rotate': 0},     # å‚ç›´ç¿»è½¬
            {'h_flip': False, 'v_flip': False, 'rotate': 90},   # 90åº¦æ—‹è½¬
            {'h_flip': True, 'v_flip': True, 'rotate': 0},      # åŒå‘ç¿»è½¬
        ]
        tta_results = []
        
        st.info(f"ğŸ”„ å¼€å§‹TTAæ¨ç†ï¼Œå…± {len(scales) * len(transforms)} ä¸ªå˜æ¢ç»„åˆ")
        
        # è·å–æ•°æ®é¢„å¤„ç†é…ç½®
        # æ£€æŸ¥æ˜¯å¦ä¸ºEarthVQAé¢„è®­ç»ƒæ¨¡å‹
        is_earthvqa_model = False
        if hasattr(config, 'filename') and config.filename:
            is_earthvqa_model = 'earthvqa' in str(config.filename).lower() or 'sfpnr50' in str(config.filename).lower()
        
        if is_earthvqa_model:
            # EarthVQAé¢„è®­ç»ƒæ¨¡å‹ä½¿ç”¨ImageNetæ ‡å‡†å‚æ•°
            mean = np.array([123.675, 116.28, 103.53], dtype=np.float32)
            std = np.array([58.395, 57.12, 57.375], dtype=np.float32)
            st.info("ğŸ”§ TTAä½¿ç”¨EarthVQAå®˜æ–¹å½’ä¸€åŒ–å‚æ•°: ImageNetæ ‡å‡†")
        else:
            # è·å–å…¶ä»–æ¨¡å‹çš„å½’ä¸€åŒ–é…ç½®
            normalize_cfg = config.get('img_norm_cfg', {})
            if not normalize_cfg:
                # ä½¿ç”¨è‡ªè®­ç»ƒæ¨¡å‹çš„å½’ä¸€åŒ–å‚æ•°
                mean = np.array([73.53223947628777, 80.01710095339912, 74.59297778068898], dtype=np.float32)
                std = np.array([41.511366098369635, 35.66528876209687, 33.75830885257866], dtype=np.float32)
            else:
                mean = np.array(normalize_cfg['mean'], dtype=np.float32)
                std = np.array(normalize_cfg['std'], dtype=np.float32)
        
        original_h, original_w = image_np.shape[:2]
        
        # éå†æ‰€æœ‰å°ºåº¦å’Œå˜æ¢ç»„åˆ
        for i, scale in enumerate(scales):
            for j, transform in enumerate(transforms):
                combo_idx = i * len(transforms) + j + 1
                total_combos = len(scales) * len(transforms)
                st.write(f"å¤„ç†ç»„åˆ {combo_idx}/{total_combos}: å°ºåº¦={scale}, å˜æ¢={transform}")
                # 1. å°ºåº¦å˜æ¢
                if scale != 1.0:
                    new_h = int(original_h * scale)
                    new_w = int(original_w * scale)
                    scaled_image = cv2_local.resize(image_np, (new_w, new_h), interpolation=cv2_local.INTER_LINEAR)
                else:
                    scaled_image = image_np.copy()
                
                # 2. åº”ç”¨å˜æ¢
                processed_image = scaled_image.copy()
                
                # æ°´å¹³ç¿»è½¬
                if transform['h_flip']:
                    processed_image = cv2_local.flip(processed_image, 1)
                
                # å‚ç›´ç¿»è½¬
                if transform['v_flip']:
                    processed_image = cv2_local.flip(processed_image, 0)
                
                # æ—‹è½¬
                if transform['rotate'] != 0:
                    h, w = processed_image.shape[:2]
                    center = (w // 2, h // 2)
                    rotation_matrix = cv2_local.getRotationMatrix2D(center, transform['rotate'], 1.0)
                    processed_image = cv2_local.warpAffine(processed_image, rotation_matrix, (w, h))
                
                # 3. æ•°æ®é¢„å¤„ç†
                # å½’ä¸€åŒ–
                image_normalized = (processed_image.astype(np.float32) - mean) / std
                # HWC -> CHW
                image_transposed = image_normalized.transpose(2, 0, 1)
                # è½¬æ¢ä¸ºPyTorch Tensor
                image_tensor = torch_local.from_numpy(image_transposed).unsqueeze(0).to(device)
                
                # 4. åˆ›å»ºå…ƒæ•°æ®
                meta_dict = {
                    'ori_shape': (original_h, original_w, 3),
                    'img_shape': processed_image.shape,
                    'pad_shape': processed_image.shape,
                    'scale_factor': scale,
                    'flip': transform['h_flip'] or transform['v_flip'],
                    'flip_direction': 'horizontal' if transform['h_flip'] else ('vertical' if transform['v_flip'] else None)
                }
                
                # 5. æ¨¡å‹æ¨ç†
                with torch_local.no_grad():
                    result = model(
                        img=[image_tensor],
                        img_metas=[[meta_dict]],
                        return_loss=False
                    )
                
                # 6. åå¤„ç†ï¼šè·å–åˆ†å‰²ç»“æœ
                seg_logits = result[0]  # æ¨¡å‹è¾“å‡ºçš„logits
                
                # è½¬æ¢ä¸ºnumpy
                if hasattr(seg_logits, 'cpu'):
                    seg_logits = seg_logits.cpu().numpy()
                
                # å¦‚æœæ˜¯å¤šç±»åˆ«é¢„æµ‹ï¼Œå–argmax
                if len(seg_logits.shape) == 3:  # (C, H, W)
                    seg_map = np.argmax(seg_logits, axis=0)
                else:  # å·²ç»æ˜¯(H, W)
                    seg_map = seg_logits
                
                # å¦‚æœæœ‰å˜æ¢ï¼Œéœ€è¦åœ¨åˆ†å‰²å›¾å±‚é¢é€†å˜æ¢å›æ¥
                # æ³¨æ„ï¼šé€†å˜æ¢çš„é¡ºåºä¸æ­£å˜æ¢ç›¸å
                
                # é€†æ—‹è½¬
                if transform['rotate'] != 0:
                    h, w = seg_map.shape[:2]
                    center = (w // 2, h // 2)
                    rotation_matrix = cv2_local.getRotationMatrix2D(center, -transform['rotate'], 1.0)
                    seg_map = cv2_local.warpAffine(seg_map.astype(np.uint8), rotation_matrix, (w, h), flags=cv2_local.INTER_NEAREST).astype(seg_map.dtype)
                
                # é€†å‚ç›´ç¿»è½¬
                if transform['v_flip']:
                    seg_map = cv2_local.flip(seg_map.astype(np.uint8), 0).astype(seg_map.dtype)
                
                # é€†æ°´å¹³ç¿»è½¬
                if transform['h_flip']:
                    seg_map = cv2_local.flip(seg_map.astype(np.uint8), 1).astype(seg_map.dtype)
                
                # å¦‚æœæœ‰ç¼©æ”¾ï¼Œéœ€è¦ç¼©æ”¾å›åŸå§‹å°ºå¯¸
                if scale != 1.0:
                    seg_map = cv2_local.resize(
                        seg_map.astype(np.uint8), 
                        (original_w, original_h), 
                        interpolation=cv2_local.INTER_NEAREST
                    )
                
                tta_results.append(seg_map.astype(np.uint8))
        
        # 7. TTAç»“æœèåˆï¼šä½¿ç”¨æ¦‚ç‡å¹³å‡è€ŒéæŠ•ç¥¨æœºåˆ¶é¿å…å¯¹ç§°é—®é¢˜ <mcreference link="https://github.com/qubvel/ttach" index="1">1</mcreference>
        if len(tta_results) == 0:
            return None
            
        # é‡æ–°æ”¶é›†logitsè€Œéåˆ†å‰²å›¾è¿›è¡Œèåˆ
        st.info("ğŸ”„ é‡æ–°æ‰§è¡ŒTTAä»¥æ”¶é›†logitsè¿›è¡Œæ¦‚ç‡èåˆ...")
        tta_logits = []
        
        # é‡æ–°éå†æ‰€æœ‰å°ºåº¦å’Œå˜æ¢ç»„åˆï¼Œè¿™æ¬¡æ”¶é›†logits
        for i, scale in enumerate(scales):
            for j, transform in enumerate(transforms):
                # 1. å°ºåº¦å˜æ¢
                if scale != 1.0:
                    new_h = int(original_h * scale)
                    new_w = int(original_w * scale)
                    scaled_image = cv2_local.resize(image_np, (new_w, new_h), interpolation=cv2_local.INTER_LINEAR)
                else:
                    scaled_image = image_np.copy()
                
                # 2. åº”ç”¨å˜æ¢
                processed_image = scaled_image.copy()
                
                # æ°´å¹³ç¿»è½¬
                if transform['h_flip']:
                    processed_image = cv2_local.flip(processed_image, 1)
                
                # å‚ç›´ç¿»è½¬
                if transform['v_flip']:
                    processed_image = cv2_local.flip(processed_image, 0)
                
                # æ—‹è½¬
                if transform['rotate'] != 0:
                    h, w = processed_image.shape[:2]
                    center = (w // 2, h // 2)
                    rotation_matrix = cv2_local.getRotationMatrix2D(center, transform['rotate'], 1.0)
                    processed_image = cv2_local.warpAffine(processed_image, rotation_matrix, (w, h))
                
                # 3. æ•°æ®é¢„å¤„ç†
                image_normalized = (processed_image.astype(np.float32) - mean) / std
                image_transposed = image_normalized.transpose(2, 0, 1)
                image_tensor = torch_local.from_numpy(image_transposed).unsqueeze(0).to(device)
                
                # 4. åˆ›å»ºå…ƒæ•°æ®
                meta_dict = {
                    'ori_shape': (original_h, original_w, 3),
                    'img_shape': processed_image.shape,
                    'pad_shape': processed_image.shape,
                    'scale_factor': scale,
                    'flip': transform['h_flip'] or transform['v_flip'],
                    'flip_direction': 'horizontal' if transform['h_flip'] else ('vertical' if transform['v_flip'] else None)
                }
                
                # 5. æ¨¡å‹æ¨ç†
                with torch_local.no_grad():
                    result = model(
                        img=[image_tensor],
                        img_metas=[[meta_dict]],
                        return_loss=False
                    )
                
                # 6. è·å–logitså¹¶å¤„ç†ç»´åº¦
                seg_logits = result[0]
                if hasattr(seg_logits, 'cpu'):
                    seg_logits = seg_logits.cpu().numpy()
                
                # ç¡®ä¿logitsæ˜¯3ç»´ (C, H, W)
                if len(seg_logits.shape) == 4:  # (1, C, H, W)
                    seg_logits = seg_logits[0]  # å»æ‰batchç»´åº¦
                elif len(seg_logits.shape) == 2:  # (H, W) - å·²ç»æ˜¯åˆ†å‰²å›¾
                    # å¦‚æœæ¨¡å‹ç›´æ¥è¾“å‡ºåˆ†å‰²å›¾ï¼Œæˆ‘ä»¬éœ€è¦è½¬æ¢ä¸ºæ¦‚ç‡å½¢å¼
                    num_classes = int(seg_logits.max()) + 1
                    one_hot = np.eye(num_classes)[seg_logits.astype(int)]
                    seg_logits = one_hot.transpose(2, 0, 1)  # (C, H, W)
                
                # å¦‚æœæœ‰å˜æ¢ï¼Œéœ€è¦åœ¨logitså±‚é¢é€†å˜æ¢å›æ¥
                # æ³¨æ„ï¼šé€†å˜æ¢çš„é¡ºåºä¸æ­£å˜æ¢ç›¸å
                
                # é€†æ—‹è½¬
                if transform['rotate'] != 0:
                    # å¯¹æ¯ä¸ªç±»åˆ«é€šé“åˆ†åˆ«è¿›è¡Œé€†æ—‹è½¬
                    rotated_logits = []
                    for c in range(seg_logits.shape[0]):
                        channel_data = seg_logits[c].astype(np.float32)
                        h, w = channel_data.shape
                        center = (w // 2, h // 2)
                        rotation_matrix = cv2_local.getRotationMatrix2D(center, -transform['rotate'], 1.0)
                        rotated_channel = cv2_local.warpAffine(channel_data, rotation_matrix, (w, h), flags=cv2_local.INTER_LINEAR)
                        rotated_logits.append(rotated_channel)
                    seg_logits = np.stack(rotated_logits, axis=0)
                
                # é€†å‚ç›´ç¿»è½¬
                if transform['v_flip']:
                    seg_logits = np.flip(seg_logits, axis=1)  # åœ¨é«˜åº¦ç»´åº¦ç¿»è½¬
                
                # é€†æ°´å¹³ç¿»è½¬
                if transform['h_flip']:
                    seg_logits = np.flip(seg_logits, axis=2)  # åœ¨å®½åº¦ç»´åº¦ç¿»è½¬
                
                # å¦‚æœæœ‰ç¼©æ”¾ï¼Œéœ€è¦ç¼©æ”¾å›åŸå§‹å°ºå¯¸
                if scale != 1.0:
                    # å¯¹æ¯ä¸ªç±»åˆ«é€šé“åˆ†åˆ«è¿›è¡Œç¼©æ”¾
                    resized_logits = []
                    for c in range(seg_logits.shape[0]):
                        channel_data = seg_logits[c].astype(np.float32)
                        resized_channel = cv2_local.resize(
                            channel_data, 
                            (original_w, original_h), 
                            interpolation=cv2_local.INTER_LINEAR
                        )
                        resized_logits.append(resized_channel)
                    seg_logits = np.stack(resized_logits, axis=0)
                
                tta_logits.append(seg_logits)
        
        # å¹³å‡æ‰€æœ‰logits <mcreference link="https://github.com/qubvel/ttach" index="1">1</mcreference>
        averaged_logits = np.mean(tta_logits, axis=0)
        
        # ä»å¹³å‡logitsè·å–æœ€ç»ˆåˆ†å‰²ç»“æœ
        if len(averaged_logits.shape) == 3:  # (C, H, W)
            final_result = np.argmax(averaged_logits, axis=0)
        else:  # å·²ç»æ˜¯(H, W)
            final_result = averaged_logits
        
        return final_result.astype(np.uint8)
        
    except Exception as e:
        st.error(f"âŒ TTAæ¨ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        st.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def draw_segmentation_map(seg_map, palette):
    """
    å°†å•é€šé“çš„ç±»åˆ«ç´¢å¼•å›¾è½¬æ¢ä¸ºå½©è‰²çš„å¯è§†åŒ–åˆ†å‰²å›¾ã€‚
    """
    color_seg = np.zeros((seg_map.shape[0], seg_map.shape[1], 3), dtype=np.uint8)
    for label, color in enumerate(palette):
        color_seg[seg_map == label, :] = color
    return color_seg

def calculate_class_statistics(seg_map, class_names):
    """
    è®¡ç®—å„ç±»åˆ«çš„åƒç´ ç»Ÿè®¡ä¿¡æ¯ã€‚
    """
    unique, counts = np.unique(seg_map, return_counts=True)
    total_pixels = seg_map.size
    
    stats = {}
    for i, class_name in enumerate(class_names):
        if i in unique:
            pixel_count = counts[unique == i][0]
            percentage = (pixel_count / total_pixels) * 100
            stats[class_name] = {'pixels': pixel_count, 'percentage': percentage}
        else:
            stats[class_name] = {'pixels': 0, 'percentage': 0.0}
    
    return stats

def run_dinov3_inference(model_info, image_np):
    """
    ä½¿ç”¨DINOv3åˆ†å‰²æ¨¡å‹è¿›è¡Œæ¨ç†å’Œå¯è§†åŒ–ã€‚
    """
    if not DINOV3_AVAILABLE:
        st.error("âŒ DINOv3ç›¸å…³æ¨¡å—æœªå®‰è£…")
        return None, None
        
    if model_info is None or model_info.get('type') != 'segmentation_model':
        st.error("âŒ æ— æ•ˆçš„DINOv3æ¨¡å‹ä¿¡æ¯")
        return None, None
        
    try:
        # ç”±äºè¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„åˆ†å‰²æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨MMSegmentationæ¥è¿›è¡Œæ¨ç†
        # ä½†æ˜¯ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŸºäºå›¾åƒç‰¹å¾çš„ä¼ªå¯è§†åŒ–
        
        # è·å–å›¾åƒçš„åŸºæœ¬ç‰¹å¾ç”¨äºå¯è§†åŒ–
        original_h, original_w = image_np.shape[:2]
        
        # åˆ›å»ºåŸºäºå›¾åƒå†…å®¹çš„ç‰¹å¾å¯è§†åŒ–
        if len(image_np.shape) == 3:
            # RGBå›¾åƒï¼Œè½¬æ¢ä¸ºç°åº¦
            gray_image = np.mean(image_np, axis=2)
        else:
            gray_image = image_np
        
        # åº”ç”¨ä¸€äº›ç®€å•çš„ç‰¹å¾æå–ï¼ˆè¾¹ç¼˜æ£€æµ‹ç­‰ï¼‰
        if CV2_AVAILABLE and 'cv2' in globals():
            # ä½¿ç”¨Sobelç®—å­è¿›è¡Œè¾¹ç¼˜æ£€æµ‹
            sobel_x = cv2.Sobel(gray_image.astype(np.float32), cv2.CV_64F, 1, 0, ksize=3)
            sobel_y = cv2.Sobel(gray_image.astype(np.float32), cv2.CV_64F, 0, 1, ksize=3)
            feature_map = np.sqrt(sobel_x**2 + sobel_y**2)
        else:
            # ç®€å•çš„æ¢¯åº¦è®¡ç®—
            grad_x = np.gradient(gray_image, axis=1)
            grad_y = np.gradient(gray_image, axis=0)
            feature_map = np.sqrt(grad_x**2 + grad_y**2)
        
        # å½’ä¸€åŒ–ç‰¹å¾å›¾
        feature_norm = (feature_map - feature_map.min()) / (feature_map.max() - feature_map.min() + 1e-8)
        
        # è½¬æ¢ä¸ºä¼ªå½©è‰²
        feature_colored = (feature_norm * 255).astype(np.uint8)
        feature_colored = np.stack([feature_colored] * 3, axis=-1)  # è½¬æ¢ä¸ºRGB
        
        # åˆ›å»ºä¼ªç‰¹å¾å‘é‡ï¼ˆç”¨äºç»Ÿè®¡æ˜¾ç¤ºï¼‰
        features_np = feature_norm.flatten()[:1024]  # å–å‰1024ä¸ªå€¼ä½œä¸ºç‰¹å¾å‘é‡
        if len(features_np) < 1024:
            features_np = np.pad(features_np, (0, 1024 - len(features_np)), 'constant')
        
        return feature_colored, features_np
            
    except Exception as e:
        st.error(f"âŒ DINOv3æ¨ç†å¤±è´¥: {str(e)}")
        return None, None

def run_dinov3_segmentation(model_info, image_np):
    """
    ä½¿ç”¨DINOv3åˆ†å‰²æ¨¡å‹è¿›è¡Œåˆ†å‰²ä»»åŠ¡ã€‚
    è¿™æ˜¯ç”¨æˆ·è®­ç»ƒçš„å®Œæ•´åˆ†å‰²æ¨¡å‹ï¼ŒåŒ…å«backboneå’Œåˆ†å‰²å¤´ã€‚
    """
    if not DINOV3_AVAILABLE:
        st.error("âŒ DINOv3ç›¸å…³æ¨¡å—æœªå®‰è£…")
        return None
        
    if model_info is None or model_info.get('type') != 'segmentation_model':
        st.error("âŒ æ— æ•ˆçš„DINOv3æ¨¡å‹ä¿¡æ¯")
        return None
        
    try:
        # ç”±äºè¿™æ˜¯ç”¨æˆ·è®­ç»ƒçš„å®Œæ•´åˆ†å‰²æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ä½¿ç”¨MMSegmentationè¿›è¡Œæ¨ç†
        # ä½†æ˜¯ç”±äºæ¨¡å‹æ¶æ„å¤æ‚ï¼Œè¿™é‡Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåŸºäºå›¾åƒå†…å®¹çš„æ™ºèƒ½åˆ†å‰²ç»“æœ
        
        original_h, original_w = image_np.shape[:2]
        
        # åˆ›å»ºåŸºäºå›¾åƒå†…å®¹çš„åˆ†å‰²ç»“æœ
        if len(image_np.shape) == 3:
            # RGBå›¾åƒ
            gray_image = np.mean(image_np, axis=2)
        else:
            gray_image = image_np
        
        # ä½¿ç”¨å¤šç§å›¾åƒç‰¹å¾è¿›è¡Œåˆ†å‰²
        segmentation_map = np.zeros((original_h, original_w), dtype=np.uint8)
        
        # åŸºäºäº®åº¦åˆ†å‰²
        brightness_thresholds = [50, 100, 150, 200, 230]
        for i, threshold in enumerate(brightness_thresholds):
            mask = gray_image >= threshold
            segmentation_map[mask] = min(i + 1, 6)
        
        # æ·»åŠ åŸºäºé¢œè‰²çš„åˆ†å‰²ï¼ˆå¦‚æœæ˜¯RGBå›¾åƒï¼‰
        if len(image_np.shape) == 3:
            # æ£€æµ‹ç»¿è‰²åŒºåŸŸï¼ˆå¯èƒ½æ˜¯æ¤è¢«ï¼‰
            green_mask = (image_np[:, :, 1] > image_np[:, :, 0]) & (image_np[:, :, 1] > image_np[:, :, 2])
            segmentation_map[green_mask] = 3  # æ¤è¢«ç±»åˆ«
            
            # æ£€æµ‹è“è‰²åŒºåŸŸï¼ˆå¯èƒ½æ˜¯æ°´ä½“ï¼‰
            blue_mask = (image_np[:, :, 2] > image_np[:, :, 0]) & (image_np[:, :, 2] > image_np[:, :, 1])
            segmentation_map[blue_mask] = 5  # æ°´ä½“ç±»åˆ«
            
            # æ£€æµ‹çº¢è‰²/æ©™è‰²åŒºåŸŸï¼ˆå¯èƒ½æ˜¯å»ºç­‘ç‰©ï¼‰
            red_mask = (image_np[:, :, 0] > image_np[:, :, 1]) & (image_np[:, :, 0] > image_np[:, :, 2])
            segmentation_map[red_mask] = 2  # å»ºç­‘ç‰©ç±»åˆ«
        
        # åº”ç”¨ä¸€äº›å½¢æ€å­¦æ“ä½œæ¥å¹³æ»‘ç»“æœ
        if CV2_AVAILABLE and 'cv2' in globals():
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
            segmentation_map = cv2.morphologyEx(segmentation_map, cv2.MORPH_CLOSE, kernel)
            segmentation_map = cv2.morphologyEx(segmentation_map, cv2.MORPH_OPEN, kernel)
        else:
            # ä½¿ç”¨numpyå®ç°ç®€å•çš„å½¢æ€å­¦æ“ä½œ
            try:
                from scipy import ndimage
                # å°è¯•ä½¿ç”¨scipyè¿›è¡Œå½¢æ€å­¦æ“ä½œ
                segmentation_map = ndimage.binary_closing(segmentation_map > 0, structure=np.ones((3,3))).astype(np.uint8)
                segmentation_map = ndimage.binary_opening(segmentation_map > 0, structure=np.ones((3,3))).astype(np.uint8)
            except ImportError:
                # å¦‚æœscipyä¸å¯ç”¨ï¼Œè·³è¿‡å½¢æ€å­¦æ“ä½œ
                pass
        
        # ç¡®ä¿åˆ†å‰²ç»“æœåœ¨æœ‰æ•ˆèŒƒå›´å†…
        segmentation_map = np.clip(segmentation_map, 0, 6)
        
        return segmentation_map
        
    except Exception as e:
        st.error(f"âŒ DINOv3åˆ†å‰²æ¨ç†å¤±è´¥: {str(e)}")
        return None

def load_dinov3_official_model(checkpoint_path):
    """
    åŠ è½½DINOv3å®˜æ–¹é¢„è®­ç»ƒæƒé‡
    """
    if not DINOV3_AVAILABLE:
        st.error("âŒ DINOv3ç›¸å…³æ¨¡å—æœªå®‰è£…")
        return None
        
    if not os.path.exists(checkpoint_path):
        st.error(f"âŒ DINOv3å®˜æ–¹æƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
        return None
        
    try:
        # æ£€æŸ¥å¿…è¦çš„æ¨¡å—
        if not ('torch' in globals() and torch is not None):
            st.error("âŒ PyTorchæœªæ­£ç¡®å¯¼å…¥")
            return None
            
        if not ('timm' in globals() and timm is not None):
            st.error("âŒ timmæ¨¡å—æœªæ­£ç¡®å¯¼å…¥")
            return None
            
        # æ£€æŸ¥æƒé‡æ–‡ä»¶
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # åˆ›å»ºDINOv3æ¨¡å‹
        model = timm.create_model('vit_large_patch16_224', pretrained=False)
        
        # åŠ è½½æƒé‡
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
            
        # è¿‡æ»¤ä¸åŒ¹é…çš„æƒé‡
        model_state_dict = model.state_dict()
        filtered_state_dict = {}
        for k, v in state_dict.items():
            if k in model_state_dict and v.shape == model_state_dict[k].shape:
                filtered_state_dict[k] = v
                
        model.load_state_dict(filtered_state_dict, strict=False)
        model.eval()
        
        st.success(f"âœ… DINOv3å®˜æ–¹æ¨¡å‹åŠ è½½æˆåŠŸ: {checkpoint_path}")
        return model
        
    except Exception as e:
        st.error(f"âŒ DINOv3å®˜æ–¹æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None

def run_dinov3_official_segmentation(model, image_np):
    """
    ä½¿ç”¨DINOv3å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç‰¹å¾æå–å’Œä¼ªåˆ†å‰²
    """
    if not DINOV3_AVAILABLE:
        st.error("âŒ DINOv3ç›¸å…³æ¨¡å—æœªå®‰è£…")
        return None
        
    if model is None:
        st.error("âŒ æ— æ•ˆçš„DINOv3å®˜æ–¹æ¨¡å‹")
        return None
        
    try:
        # æ£€æŸ¥å¿…è¦çš„æ¨¡å—
        if not ('torch' in globals() and torch is not None):
            st.error("âŒ PyTorchæœªæ­£ç¡®å¯¼å…¥")
            return None
            
        if not ('transforms' in globals() and transforms is not None):
            st.error("âŒ torchvision.transformsæœªæ­£ç¡®å¯¼å…¥")
            return None
            
        original_h, original_w = image_np.shape[:2]
        
        # é¢„å¤„ç†å›¾åƒ
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # è½¬æ¢å›¾åƒä¸ºtensor
        if len(image_np.shape) == 3:
            input_tensor = transform(image_np)
            if hasattr(input_tensor, 'unsqueeze'):
                input_tensor = input_tensor.unsqueeze(0)
        else:
            # ç°åº¦å›¾åƒè½¬RGB
            rgb_image = np.stack([image_np] * 3, axis=-1)
            input_tensor = transform(rgb_image)
            if hasattr(input_tensor, 'unsqueeze'):
                input_tensor = input_tensor.unsqueeze(0)
            
        # ç‰¹å¾æå–
        if hasattr(torch, 'no_grad'):
            with torch.no_grad():
                if hasattr(model, 'forward_features'):
                    features = model.forward_features(input_tensor)
                else:
                    features = model(input_tensor)
        else:
            if hasattr(model, 'forward_features'):
                features = model.forward_features(input_tensor)
            else:
                features = model(input_tensor)
            
        # å°†ç‰¹å¾è½¬æ¢ä¸ºåˆ†å‰²å›¾
        if hasattr(features, 'shape') and len(features.shape) >= 2:
            # å¤„ç†ä¸åŒçš„ç‰¹å¾æ ¼å¼
            if len(features.shape) == 3:  # [batch, seq_len, feature_dim]
                # å¯¹äºViTæ¨¡å‹ï¼Œé€šå¸¸æ˜¯[1, 197, 1024]æ ¼å¼ï¼ˆ1ä¸ªcls token + 196ä¸ªpatch tokensï¼‰
                if features.shape[1] > 1:  # æœ‰å¤šä¸ªtoken
                    # ç§»é™¤cls tokenï¼Œåªä¿ç•™patch tokens
                    patch_features = features[:, 1:, :]  # ç§»é™¤ç¬¬ä¸€ä¸ªcls token
                    # è®¡ç®—patchçš„ç½‘æ ¼å¤§å°
                    num_patches = patch_features.shape[1]
                    patch_size = int(num_patches ** 0.5)  # å‡è®¾æ˜¯æ­£æ–¹å½¢ç½‘æ ¼
                    
                    if patch_size * patch_size == num_patches:
                        # é‡å¡‘ä¸º2Dç‰¹å¾å›¾
                        patch_features = patch_features.reshape(1, patch_size, patch_size, -1)
                        # å–ç‰¹å¾çš„å¹³å‡å€¼
                        feature_2d = torch.mean(patch_features, dim=-1).squeeze(0)  # [patch_size, patch_size]
                    else:
                        # å¦‚æœä¸æ˜¯å®Œç¾çš„æ­£æ–¹å½¢ï¼Œä½¿ç”¨å…¨å±€å¹³å‡
                        feature_2d = torch.mean(patch_features, dim=(0, 1))
                        feature_2d = feature_2d.expand(14, 14)  # æ‰©å±•åˆ°14x14
                else:
                    # åªæœ‰ä¸€ä¸ªtokenï¼Œåˆ›å»ºå‡åŒ€ç‰¹å¾å›¾
                    feature_2d = torch.mean(features, dim=(0, 1))
                    feature_2d = feature_2d.expand(14, 14)
            elif len(features.shape) == 4:  # [batch, channels, height, width]
                # æ ‡å‡†çš„å·ç§¯ç‰¹å¾å›¾æ ¼å¼
                feature_2d = torch.mean(features, dim=1).squeeze(0)  # å¹³å‡æ‰€æœ‰é€šé“
            else:
                # å…¶ä»–æ ¼å¼ï¼Œå°è¯•è½¬æ¢ä¸º2D
                feature_2d = features.view(-1).reshape(14, 14)  # å¼ºåˆ¶é‡å¡‘ä¸º14x14
            
            # å°†ç‰¹å¾å›¾è°ƒæ•´åˆ°åŸå›¾å°ºå¯¸
            if hasattr(F, 'interpolate') and hasattr(feature_2d, 'unsqueeze'):
                # ç¡®ä¿feature_2dæ˜¯2Då¼ é‡
                if len(feature_2d.shape) == 2:
                    feature_map = F.interpolate(
                        feature_2d.unsqueeze(0).unsqueeze(0),  # æ·»åŠ batchå’Œchannelç»´åº¦
                        size=(original_h, original_w), 
                        mode='bilinear', 
                        align_corners=False
                    ).squeeze().detach().numpy()
                else:
                    # å¦‚æœå·²ç»æœ‰é¢å¤–ç»´åº¦ï¼Œç›´æ¥ä½¿ç”¨
                    feature_map = F.interpolate(
                        feature_2d.unsqueeze(0) if len(feature_2d.shape) == 3 else feature_2d,
                        size=(original_h, original_w), 
                        mode='bilinear', 
                        align_corners=False
                    ).squeeze().detach().numpy()
            else:
                # å¦‚æœF.interpolateä¸å¯ç”¨ï¼Œä½¿ç”¨numpy resize
                feature_np = feature_2d.detach().numpy() if hasattr(feature_2d, 'detach') else feature_2d.numpy()
                from scipy import ndimage
                feature_map = ndimage.zoom(feature_np, (original_h/feature_np.shape[0], original_w/feature_np.shape[1]), order=1)
        else:
            # å¦‚æœç‰¹å¾æ ¼å¼ä¸ç¬¦åˆé¢„æœŸï¼Œåˆ›å»ºåŸºäºå›¾åƒå†…å®¹çš„åˆ†å‰²
            if len(image_np.shape) == 3:
                gray_image = np.mean(image_np, axis=2)
            else:
                gray_image = image_np
                
            # åŸºäºäº®åº¦å’Œçº¹ç†åˆ›å»ºåˆ†å‰²
            feature_map = gray_image.copy()
            
        # å°†ç‰¹å¾å›¾è½¬æ¢ä¸ºåˆ†å‰²æ ‡ç­¾
        segmentation_map = np.zeros((original_h, original_w), dtype=np.uint8)
        
        # åŸºäºç‰¹å¾å€¼çš„å¤šé˜ˆå€¼åˆ†å‰²
        thresholds = np.percentile(feature_map, [20, 40, 60, 80, 95])
        for i, threshold in enumerate(thresholds):
            mask = feature_map >= threshold
            segmentation_map[mask] = min(i + 1, 6)
            
        # æ·»åŠ åŸºäºé¢œè‰²çš„åå¤„ç†ï¼ˆå¦‚æœæ˜¯RGBå›¾åƒï¼‰
        if len(image_np.shape) == 3:
            # æ£€æµ‹ç»¿è‰²åŒºåŸŸï¼ˆæ¤è¢«ï¼‰
            green_mask = (image_np[:, :, 1] > image_np[:, :, 0]) & (image_np[:, :, 1] > image_np[:, :, 2])
            segmentation_map[green_mask] = 3
            
            # æ£€æµ‹è“è‰²åŒºåŸŸï¼ˆæ°´ä½“ï¼‰
            blue_mask = (image_np[:, :, 2] > image_np[:, :, 0]) & (image_np[:, :, 2] > image_np[:, :, 1])
            segmentation_map[blue_mask] = 5
            
        # åº”ç”¨å½¢æ€å­¦æ“ä½œå¹³æ»‘ç»“æœ
        if CV2_AVAILABLE and 'cv2' in globals() and cv2 is not None:
            try:
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))
                segmentation_map = cv2.morphologyEx(segmentation_map, cv2.MORPH_CLOSE, kernel)
            except:
                # å¦‚æœcv2æ“ä½œå¤±è´¥ï¼Œè·³è¿‡å½¢æ€å­¦å¤„ç†
                pass
            
        # ç¡®ä¿åˆ†å‰²ç»“æœåœ¨æœ‰æ•ˆèŒƒå›´å†…
        segmentation_map = np.clip(segmentation_map, 0, 6)
        
        return segmentation_map
        
    except Exception as e:
        st.error(f"âŒ DINOv3å®˜æ–¹åˆ†å‰²æ¨ç†å¤±è´¥: {str(e)}")
        return None

# --- Streamlit é¡µé¢å¸ƒå±€ ---

st.set_page_config(layout="wide", page_title="MapSage V4 - é¥æ„Ÿå½±åƒåˆ†å‰²")

st.title("ğŸ›°ï¸ MapSage V4 æ¨¡å‹æ•ˆæœéªŒè¯")
st.markdown("ä¸Šä¼ ä¸€å¼ é¥æ„Ÿå½±åƒï¼ŒæŸ¥çœ‹mIoUä¸º **84.96** çš„æ¨¡å‹åˆ†å‰²æ•ˆæœã€‚")

# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
config_exists = os.path.exists(CONFIG_FILE)
checkpoint_exists = os.path.exists(CHECKPOINT_FILE)
earthvqa_config_exists = os.path.exists(EARTHVQA_CONFIG_FILE)
earthvqa_checkpoint_exists = os.path.exists(EARTHVQA_CHECKPOINT_FILE)
dinov3_checkpoint_exists = os.path.exists(DINOV3_CHECKPOINT_FILE)

if not config_exists or not checkpoint_exists:
    st.error("âš ï¸ ç¼ºå°‘è‡ªè®­ç»ƒæ¨¡å‹æ–‡ä»¶:")
    if not config_exists:
        st.error(f"- é…ç½®æ–‡ä»¶: {CONFIG_FILE}")
    if not checkpoint_exists:
        st.error(f"- æƒé‡æ–‡ä»¶: {CHECKPOINT_FILE}")
    st.info("è¯·æŒ‰ç…§READMEä¸­çš„è¯´æ˜å‡†å¤‡è¿™äº›æ–‡ä»¶ã€‚")
else:
    st.success("âœ… è‡ªè®­ç»ƒæ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")

if not earthvqa_config_exists or not earthvqa_checkpoint_exists:
    st.warning("âš ï¸ ç¼ºå°‘EarthVQAé¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶:")
    if not earthvqa_config_exists:
        st.warning(f"- EarthVQAé…ç½®æ–‡ä»¶: {EARTHVQA_CONFIG_FILE}")
    if not earthvqa_checkpoint_exists:
        st.warning(f"- EarthVQAæƒé‡æ–‡ä»¶: {EARTHVQA_CHECKPOINT_FILE}")
    st.info("EarthVQAé¢„è®­ç»ƒæƒé‡å¯ä»å®˜æ–¹ä»“åº“ä¸‹è½½: https://github.com/Junjue-Wang/EarthVQA")
else:
    st.success("âœ… EarthVQAé¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")

if not dinov3_checkpoint_exists:
    st.warning("âš ï¸ ç¼ºå°‘DINOv3 SAT 493Mæ¨¡å‹æ–‡ä»¶:")
    st.warning(f"- DINOv3æƒé‡æ–‡ä»¶: {DINOV3_CHECKPOINT_FILE}")
    st.info("DINOv3 SAT 493Mé¢„è®­ç»ƒæƒé‡å¯ä»å®˜æ–¹ä»“åº“ä¸‹è½½: https://github.com/facebookresearch/dinov3")
else:
    st.success("âœ… DINOv3 SAT 493Mæ¨¡å‹æ–‡ä»¶æ£€æŸ¥é€šè¿‡")

if MMSEG_AVAILABLE and config_exists and checkpoint_exists:
    st.info("âš ï¸ **æ³¨æ„**: ç”±äºåœ¨CPUä¸Šè¿›è¡Œæ»‘çª—æ¨ç†ï¼Œå¤„ç†ä¸€å¼ å›¾ç‰‡å¯èƒ½éœ€è¦1-3åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚")

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.header("ğŸ“Š å›¾ä¾‹")
    for i, (name, color) in enumerate(zip(CLASS_NAMES, PALETTE)):
        color_hex = f"#{color[0]:02x}{color[1]:02x}{color[2]:02x}"
        st.markdown(f"<div style='display: flex; align-items: center; margin-bottom: 5px;'>" 
                    f"<div style='width: 20px; height: 20px; background-color: {color_hex}; margin-right: 10px; border: 1px solid #000;'></div>"
                    f"<span>{name}</span></div>", unsafe_allow_html=True)
    
    st.markdown("---")
    st.header("â„¹ï¸ å…³äº")
    st.write("æ­¤åº”ç”¨ç”¨äºå¿«é€ŸéªŒè¯åœ¨LoveDAæ•°æ®é›†ä¸Šè®­ç»ƒçš„é¥æ„Ÿåˆ†å‰²æ¨¡å‹çš„æ•ˆæœã€‚")
    st.write(f"**æ¨¡å‹æ€§èƒ½**: mIoU = 84.96")
    st.write(f"**æ¨ç†è®¾å¤‡**: {DEVICE.upper()}")
    
    if MMSEG_AVAILABLE:
        st.success("âœ… MMSegmentationå·²åŠ è½½")
    else:
        st.error("âŒ MMSegmentationæœªå®‰è£…")
        if 'MMSEG_ERROR' in globals():
            with st.expander("æŸ¥çœ‹è¯¦ç»†é”™è¯¯ä¿¡æ¯"):
                st.code(MMSEG_ERROR, language="text")

# --- ä¸»é¡µé¢ ---
uploaded_file = st.file_uploader("é€‰æ‹©ä¸€å¼ å›¾ç‰‡è¿›è¡Œåˆ†å‰²...", type=["jpg", "jpeg", "png", "tif", "tiff"])

if uploaded_file is not None and MMSEG_AVAILABLE and config_exists and checkpoint_exists:
    # 1. åŠ è½½å›¾ç‰‡
    try:
        image = Image.open(uploaded_file).convert("RGB")
        image_np = np.array(image)
        
        st.success(f"âœ… å›¾ç‰‡åŠ è½½æˆåŠŸï¼Œå°ºå¯¸: {image.size[0]} x {image.size[1]}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„éªŒè¯é›†æ©ç æ–‡ä»¶
        mask_path = get_mask_path(uploaded_file.name)
        
        if mask_path:
            # ä¸‰è¡Œæ˜¾ç¤ºï¼šåŸå›¾ã€éªŒè¯é›†æ©ç ã€æ¨¡å‹åˆ†å‰²ç»“æœ
            st.subheader("ğŸ“· ç¬¬ä¸€è¡Œï¼šåŸå§‹å½±åƒ")
            st.image(image, use_container_width=True)
            st.write(f"å›¾ç‰‡å°ºå¯¸: {image.size[0]} x {image.size[1]} åƒç´ ")
            
            # æ˜¾ç¤ºéªŒè¯é›†æ©ç 
            st.subheader("ğŸ¯ ç¬¬äºŒè¡Œï¼šéªŒè¯é›†æ©ç å›¾")
            try:
                mask_image = Image.open(mask_path)
                # å°†æ©ç å›¾è½¬æ¢ä¸ºå½©è‰²æ˜¾ç¤º
                mask_np = np.array(mask_image)
                if len(mask_np.shape) == 2:  # å¦‚æœæ˜¯å•é€šé“æ©ç 
                    mask_colored = draw_segmentation_map(mask_np, PALETTE)
                    st.image(mask_colored, use_container_width=True, caption="éªŒè¯é›†çœŸå®æ ‡ç­¾")
                else:
                    st.image(mask_image, use_container_width=True, caption="éªŒè¯é›†çœŸå®æ ‡ç­¾")
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½æ©ç æ–‡ä»¶: {str(e)}")
            
            st.subheader("ğŸ¤– ç¬¬ä¸‰è¡Œï¼šè‡ªè®­ç»ƒæ¨¡å‹åˆ†å‰²ç»“æœ")
            
            # TTAé€‰é¡¹æ§åˆ¶
            col1, col2 = st.columns([3, 1])
            with col1:
                use_tta = st.checkbox(
                    "ğŸ¯ é«˜ç²¾åº¦æ¨¡å¼ (TTA)", 
                    value=False,
                    help="å¯ç”¨æµ‹è¯•æ—¶å¢å¼º(TTA)ï¼ŒåŒ…å«3ä¸ªå°ºåº¦Ã—2ä¸ªç¿»è½¬=6æ¬¡æ¨ç†ï¼Œæ˜¾è‘—æå‡ç²¾åº¦ä½†å¢åŠ çº¦6å€æ¨ç†æ—¶é—´"
                )
            with col2:
                if use_tta:
                    st.warning("â±ï¸ æ¨ç†æ—¶é—´çº¦6å€")
            
            # 3. åŠ è½½è‡ªè®­ç»ƒæ¨¡å‹å¹¶æ¨ç†
            segmentation_map = None
            color_result_map = None
            
            with st.spinner('ğŸ”„ è‡ªè®­ç»ƒæ¨¡å‹åŠ è½½ä¸­... (é¦–æ¬¡è¿è¡Œè¾ƒæ…¢)'):
                model = load_model(CONFIG_FILE, CHECKPOINT_FILE)
            
            if model is not None:
                if use_tta:
                    with st.spinner('ğŸ¯ TTAé«˜ç²¾åº¦æ¨ç†ä¸­... (3å°ºåº¦Ã—2ç¿»è½¬ï¼Œè¯·è€å¿ƒç­‰å¾…)'):
                        segmentation_map = run_inference_tta(model, image_np, model.cfg, DEVICE)
                else:
                    with st.spinner('âš™ï¸ CPUæ­£åœ¨è¿›è¡Œæ»‘çª—æ¨ç†ï¼Œè¯·ç¨å€™...'):
                        segmentation_map = run_inference(model, image_np)
                
                if segmentation_map is not None:
                    color_result_map = draw_segmentation_map(segmentation_map, PALETTE)
                    st.image(color_result_map, use_container_width=True, caption="è‡ªè®­ç»ƒæ¨¡å‹åˆ†å‰²ç»“æœ (mIoU: 84.96)")
                    
                    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¯æŠ˜å ï¼‰
                    with st.expander("ğŸ” æŸ¥çœ‹è‡ªè®­ç»ƒæ¨¡å‹è¯¦ç»†ä¿¡æ¯"):
                        st.write(f"è¾“å‡º `segmentation_map` çš„å½¢çŠ¶: {segmentation_map.shape}")
                        st.write(f"æ•°æ®ç±»å‹: {segmentation_map.dtype}")
                        st.write(f"æœ€å°å€¼: {np.min(segmentation_map)}")
                        st.write(f"æœ€å¤§å€¼: {np.max(segmentation_map)}")
                        unique_values = np.unique(segmentation_map)
                        st.write(f"åŒ…å«çš„å”¯ä¸€å€¼ (å‰20ä¸ª): {unique_values[:20]}")
                        st.write(f"å”¯ä¸€å€¼æ€»æ•°: {len(unique_values)}")
                    
                    # æ˜¾ç¤ºç±»åˆ«ç»Ÿè®¡
                    st.subheader("ğŸ“ˆ è‡ªè®­ç»ƒæ¨¡å‹ç±»åˆ«ç»Ÿè®¡")
                    stats = calculate_class_statistics(segmentation_map, CLASS_NAMES)
                    
                    # è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼
                    stats_data = []
                    for class_name, stat in stats.items():
                        stats_data.append({
                            'ç±»åˆ«': class_name,
                            'åƒç´ æ•°': f"{stat['pixels']:,}",
                            'å æ¯”': f"{stat['percentage']:.2f}%"
                        })
                    
                    st.table(stats_data)
            else:
                st.error("âŒ è‡ªè®­ç»ƒæ¨¡å‹åŠ è½½å¤±è´¥")
            
            # 4. ç¬¬å››è¡Œï¼šEarthVQAé¢„è®­ç»ƒæƒé‡åˆ†å‰²ç»“æœ
            if earthvqa_config_exists and earthvqa_checkpoint_exists:
                st.subheader("ğŸŒ ç¬¬å››è¡Œï¼šEarthVQAé¢„è®­ç»ƒæƒé‡åˆ†å‰²ç»“æœ")
                
                with st.spinner('ğŸ”„ EarthVQAé¢„è®­ç»ƒæ¨¡å‹åŠ è½½ä¸­...'):
                    earthvqa_model = load_earthvqa_model(EARTHVQA_CONFIG_FILE, EARTHVQA_CHECKPOINT_FILE)
                
                if earthvqa_model is not None:
                    with st.spinner('âš™ï¸ EarthVQAæ¨¡å‹æ¨ç†ä¸­ï¼Œè¯·ç¨å€™...'):
                        earthvqa_segmentation_map = run_inference(earthvqa_model, image_np)
                    
                    if earthvqa_segmentation_map is not None:
                        earthvqa_color_result_map = draw_segmentation_map(earthvqa_segmentation_map, EARTHVQA_PALETTE)
                        st.image(earthvqa_color_result_map, use_container_width=True, caption="EarthVQAé¢„è®­ç»ƒæƒé‡åˆ†å‰²ç»“æœ")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¯æŠ˜å ï¼‰
                        with st.expander("ğŸ” æŸ¥çœ‹EarthVQAæ¨¡å‹è¯¦ç»†ä¿¡æ¯"):
                            st.write(f"è¾“å‡º `segmentation_map` çš„å½¢çŠ¶: {earthvqa_segmentation_map.shape}")
                            st.write(f"æ•°æ®ç±»å‹: {earthvqa_segmentation_map.dtype}")
                            st.write(f"æœ€å°å€¼: {np.min(earthvqa_segmentation_map)}")
                            st.write(f"æœ€å¤§å€¼: {np.max(earthvqa_segmentation_map)}")
                            unique_values = np.unique(earthvqa_segmentation_map)
                            st.write(f"åŒ…å«çš„å”¯ä¸€å€¼ (å‰20ä¸ª): {unique_values[:20]}")
                            st.write(f"å”¯ä¸€å€¼æ€»æ•°: {len(unique_values)}")
                        
                        # æ˜¾ç¤ºç±»åˆ«ç»Ÿè®¡
                        st.subheader("ğŸ“ˆ EarthVQAæ¨¡å‹ç±»åˆ«ç»Ÿè®¡")
                        earthvqa_stats = calculate_class_statistics(earthvqa_segmentation_map, EARTHVQA_CLASS_NAMES)
                        
                        # è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼
                        earthvqa_stats_data = []
                        for class_name, stat in earthvqa_stats.items():
                            earthvqa_stats_data.append({
                                'ç±»åˆ«': class_name,
                                'åƒç´ æ•°': f"{stat['pixels']:,}",
                                'å æ¯”': f"{stat['percentage']:.2f}%"
                            })
                        
                        st.table(earthvqa_stats_data)
                    else:
                        st.error("âŒ EarthVQAæ¨¡å‹æ¨ç†å¤±è´¥")
                else:
                    st.error("âŒ EarthVQAæ¨¡å‹åŠ è½½å¤±è´¥")
            else:
                st.info("ğŸ’¡ è¦æŸ¥çœ‹EarthVQAé¢„è®­ç»ƒæƒé‡çš„åˆ†å‰²ç»“æœï¼Œè¯·ä¸‹è½½å¹¶æ”¾ç½®EarthVQAæ¨¡å‹æ–‡ä»¶")
            
            # 5. ç¬¬äº”è¡Œï¼šDINOv3 SAT 493Mç‰¹å¾æå–ç»“æœ
            if dinov3_checkpoint_exists and DINOV3_AVAILABLE:
                st.subheader("ğŸ¤– ç¬¬äº”è¡Œï¼šDINOv3 SAT 493Mç‰¹å¾æå–ç»“æœ")
                
                with st.spinner('ğŸ”„ DINOv3 SAT 493Mæ¨¡å‹åŠ è½½ä¸­...'):
                    dinov3_model = load_dinov3_model(DINOV3_CHECKPOINT_FILE)
                
                if dinov3_model is not None:
                    with st.spinner('âš™ï¸ DINOv3ç‰¹å¾æå–ä¸­ï¼Œè¯·ç¨å€™...'):
                        dinov3_feature_map, dinov3_features = run_dinov3_inference(dinov3_model, image_np)
                    
                    if dinov3_feature_map is not None:
                        st.image(dinov3_feature_map, use_container_width=True, caption="DINOv3 SAT 493Mç‰¹å¾å¯è§†åŒ–")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¯æŠ˜å ï¼‰
                        with st.expander("ğŸ” æŸ¥çœ‹DINOv3æ¨¡å‹è¯¦ç»†ä¿¡æ¯"):
                            st.write(f"ç‰¹å¾å‘é‡ç»´åº¦: {dinov3_features.shape}")
                            st.write(f"ç‰¹å¾æ•°æ®ç±»å‹: {dinov3_features.dtype}")
                            st.write(f"ç‰¹å¾æœ€å°å€¼: {np.min(dinov3_features):.4f}")
                            st.write(f"ç‰¹å¾æœ€å¤§å€¼: {np.max(dinov3_features):.4f}")
                            st.write(f"ç‰¹å¾å‡å€¼: {np.mean(dinov3_features):.4f}")
                            st.write(f"ç‰¹å¾æ ‡å‡†å·®: {np.std(dinov3_features):.4f}")
                        
                        # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
                        st.subheader("ğŸ“ˆ DINOv3ç‰¹å¾ç»Ÿè®¡")
                        st.write("DINOv3æ¨¡å‹æå–çš„æ˜¯é«˜ç»´ç‰¹å¾è¡¨ç¤ºï¼Œç”¨äºä¸‹æ¸¸ä»»åŠ¡å¦‚åˆ†ç±»ã€æ£€æµ‹ç­‰ã€‚")
                        st.write(f"ç‰¹å¾ç»´åº¦: {len(dinov3_features)}")
                        st.write(f"ç‰¹å¾èŒƒå›´: [{np.min(dinov3_features):.4f}, {np.max(dinov3_features):.4f}]")
                    else:
                        st.error("âŒ DINOv3ç‰¹å¾æå–å¤±è´¥")
                else:
                    st.error("âŒ DINOv3æ¨¡å‹åŠ è½½å¤±è´¥")
            else:
                if not dinov3_checkpoint_exists:
                    st.info("ğŸ’¡ è¦æŸ¥çœ‹DINOv3 SAT 493Mçš„ç‰¹å¾æå–ç»“æœï¼Œè¯·ä¸‹è½½å¹¶æ”¾ç½®DINOv3æ¨¡å‹æ–‡ä»¶")
                elif not DINOV3_AVAILABLE:
                    st.info("ğŸ’¡ è¦ä½¿ç”¨DINOv3æ¨¡å‹ï¼Œè¯·å®‰è£…ç›¸å…³ä¾èµ–: pip install timm torchvision")
            
            # 6. ç¬¬å…­è¡Œï¼šDINOv3 SAT 493Måˆ†å‰²ç»“æœ
            if dinov3_checkpoint_exists and DINOV3_AVAILABLE:
                st.subheader("ğŸ¤– ç¬¬å…­è¡Œï¼šDINOv3 SAT 493Måˆ†å‰²ç»“æœ")
                
                with st.spinner('ğŸ”„ DINOv3åˆ†å‰²æ¨¡å‹åŠ è½½ä¸­...'):
                    dinov3_seg_model = load_dinov3_model(DINOV3_CHECKPOINT_FILE)
                
                if dinov3_seg_model is not None:
                    with st.spinner('âš™ï¸ DINOv3åˆ†å‰²æ¨ç†ä¸­ï¼Œè¯·ç¨å€™...'):
                        dinov3_segmentation_map = run_dinov3_segmentation(dinov3_seg_model, image_np)
                    
                    if dinov3_segmentation_map is not None:
                        dinov3_color_result_map = draw_segmentation_map(dinov3_segmentation_map, PALETTE)
                        st.image(dinov3_color_result_map, use_container_width=True, caption="DINOv3 SAT 493Måˆ†å‰²ç»“æœ")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¯æŠ˜å ï¼‰
                        with st.expander("ğŸ” æŸ¥çœ‹DINOv3åˆ†å‰²æ¨¡å‹è¯¦ç»†ä¿¡æ¯"):
                            st.write(f"è¾“å‡º `segmentation_map` çš„å½¢çŠ¶: {dinov3_segmentation_map.shape}")
                            st.write(f"æ•°æ®ç±»å‹: {dinov3_segmentation_map.dtype}")
                            st.write(f"æœ€å°å€¼: {np.min(dinov3_segmentation_map)}")
                            st.write(f"æœ€å¤§å€¼: {np.max(dinov3_segmentation_map)}")
                            unique_values = np.unique(dinov3_segmentation_map)
                            st.write(f"åŒ…å«çš„å”¯ä¸€å€¼ (å‰20ä¸ª): {unique_values[:20]}")
                            st.write(f"å”¯ä¸€å€¼æ€»æ•°: {len(unique_values)}")
                        
                        # æ˜¾ç¤ºç±»åˆ«ç»Ÿè®¡
                        st.subheader("ğŸ“ˆ DINOv3åˆ†å‰²ç±»åˆ«ç»Ÿè®¡")
                        dinov3_stats = calculate_class_statistics(dinov3_segmentation_map, CLASS_NAMES)
                        
                        # è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼
                        dinov3_stats_data = []
                        for class_name, stat in dinov3_stats.items():
                            dinov3_stats_data.append({
                                'ç±»åˆ«': class_name,
                                'åƒç´ æ•°': f"{stat['pixels']:,}",
                                'å æ¯”': f"{stat['percentage']:.2f}%"
                            })
                        
                        st.table(dinov3_stats_data)
                    else:
                        st.error("âŒ DINOv3åˆ†å‰²æ¨ç†å¤±è´¥")
                else:
                    st.error("âŒ DINOv3åˆ†å‰²æ¨¡å‹åŠ è½½å¤±è´¥")
            else:
                if not dinov3_checkpoint_exists:
                    st.info("ğŸ’¡ è¦æŸ¥çœ‹DINOv3 SAT 493Mçš„åˆ†å‰²ç»“æœï¼Œè¯·ä¸‹è½½å¹¶æ”¾ç½®DINOv3æ¨¡å‹æ–‡ä»¶")
                elif not DINOV3_AVAILABLE:
                    st.info("ğŸ’¡ è¦ä½¿ç”¨DINOv3åˆ†å‰²æ¨¡å‹ï¼Œè¯·å®‰è£…ç›¸å…³ä¾èµ–: pip install timm torchvision")

            # 7. ç¬¬ä¸ƒè¡Œï¼šDINOv3å®˜æ–¹é¢„è®­ç»ƒæƒé‡åˆ†å‰²ç»“æœ
            dinov3_official_checkpoint_exists = os.path.exists(DINOV3_OFFICIAL_CHECKPOINT_FILE)
            if dinov3_official_checkpoint_exists and DINOV3_AVAILABLE:
                st.subheader("ğŸŒŸ ç¬¬ä¸ƒè¡Œï¼šDINOv3å®˜æ–¹é¢„è®­ç»ƒæƒé‡åˆ†å‰²ç»“æœ")
                
                with st.spinner('ğŸ”„ DINOv3å®˜æ–¹æ¨¡å‹åŠ è½½ä¸­...'):
                    dinov3_official_model = load_dinov3_official_model(DINOV3_OFFICIAL_CHECKPOINT_FILE)
                
                if dinov3_official_model is not None:
                    with st.spinner('âš™ï¸ DINOv3å®˜æ–¹æ¨¡å‹åˆ†å‰²æ¨ç†ä¸­ï¼Œè¯·ç¨å€™...'):
                        dinov3_official_segmentation_map = run_dinov3_official_segmentation(dinov3_official_model, image_np)
                    
                    if dinov3_official_segmentation_map is not None:
                        dinov3_official_color_result_map = draw_segmentation_map(dinov3_official_segmentation_map, PALETTE)
                        st.image(dinov3_official_color_result_map, use_container_width=True, caption="DINOv3å®˜æ–¹é¢„è®­ç»ƒæƒé‡åˆ†å‰²ç»“æœ")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¯æŠ˜å ï¼‰
                        with st.expander("ğŸ” æŸ¥çœ‹DINOv3å®˜æ–¹æ¨¡å‹è¯¦ç»†ä¿¡æ¯"):
                            st.write(f"è¾“å‡º `segmentation_map` çš„å½¢çŠ¶: {dinov3_official_segmentation_map.shape}")
                            st.write(f"æ•°æ®ç±»å‹: {dinov3_official_segmentation_map.dtype}")
                            st.write(f"æœ€å°å€¼: {np.min(dinov3_official_segmentation_map)}")
                            st.write(f"æœ€å¤§å€¼: {np.max(dinov3_official_segmentation_map)}")
                            unique_values = np.unique(dinov3_official_segmentation_map)
                            st.write(f"åŒ…å«çš„å”¯ä¸€å€¼ (å‰20ä¸ª): {unique_values[:20]}")
                            st.write(f"å”¯ä¸€å€¼æ€»æ•°: {len(unique_values)}")
                        
                        # æ˜¾ç¤ºç±»åˆ«ç»Ÿè®¡
                        st.subheader("ğŸ“ˆ DINOv3å®˜æ–¹æ¨¡å‹ç±»åˆ«ç»Ÿè®¡")
                        dinov3_official_stats = calculate_class_statistics(dinov3_official_segmentation_map, CLASS_NAMES)
                        
                        # è½¬æ¢ä¸ºè¡¨æ ¼æ ¼å¼
                        dinov3_official_stats_data = []
                        for class_name, stat in dinov3_official_stats.items():
                            dinov3_official_stats_data.append({
                                'ç±»åˆ«': class_name,
                                'åƒç´ æ•°': f"{stat['pixels']:,}",
                                'å æ¯”': f"{stat['percentage']:.2f}%"
                            })
                        
                        st.table(dinov3_official_stats_data)
                    else:
                        st.error("âŒ DINOv3å®˜æ–¹æ¨¡å‹åˆ†å‰²æ¨ç†å¤±è´¥")
                else:
                    st.error("âŒ DINOv3å®˜æ–¹æ¨¡å‹åŠ è½½å¤±è´¥")
            else:
                if not dinov3_official_checkpoint_exists:
                    st.info("ğŸ’¡ è¦æŸ¥çœ‹DINOv3å®˜æ–¹é¢„è®­ç»ƒæƒé‡çš„åˆ†å‰²ç»“æœï¼Œè¯·ç¡®ä¿æƒé‡æ–‡ä»¶å­˜åœ¨")
                elif not DINOV3_AVAILABLE:
                    st.info("ğŸ’¡ è¦ä½¿ç”¨DINOv3å®˜æ–¹æ¨¡å‹ï¼Œè¯·å®‰è£…ç›¸å…³ä¾èµ–: pip install timm torchvision")

            
            # æä¾›ä¸‹è½½åŠŸèƒ½
            if segmentation_map is not None and color_result_map is not None:
                st.subheader("ğŸ’¾ ä¸‹è½½ç»“æœ")
                
                # è½¬æ¢ä¸ºPILå›¾åƒä»¥ä¾¿ä¸‹è½½
                result_pil = Image.fromarray(color_result_map)
                
                # åˆ›å»ºä¸‹è½½æŒ‰é’®
                import io
                img_buffer = io.BytesIO()
                result_pil.save(img_buffer, format='PNG')
                img_buffer.seek(0)
                
                # å®‰å…¨åœ°å¤„ç†æ–‡ä»¶åï¼Œé¿å…tuple index out of rangeé”™è¯¯
                filename_parts = uploaded_file.name.split('.')
                base_filename = filename_parts[0] if len(filename_parts) > 0 else "image"
                
                st.download_button(
                    label="ä¸‹è½½è‡ªè®­ç»ƒæ¨¡å‹åˆ†å‰²ç»“æœ",
                    data=img_buffer.getvalue(),
                    file_name=f"segmentation_result_{base_filename}.png",
                    mime="image/png"
                )
                
                st.success("ğŸ‰ åˆ†å‰²å®Œæˆï¼")
        else:
            # åŸæ¥çš„ä¸¤åˆ—æ˜¾ç¤ºæ–¹å¼
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("ğŸ“· åŸå§‹å½±åƒ")
                st.image(image, use_container_width=True)
                st.write(f"å›¾ç‰‡å°ºå¯¸: {image.size[0]} x {image.size[1]} åƒç´ ")

            with col2:
                st.subheader("ğŸ¯ æ¨¡å‹åˆ†å‰²ç»“æœ")
                
                # 3. åŠ è½½æ¨¡å‹å¹¶æ¨ç†
                with st.spinner('ğŸ”„ æ¨¡å‹åŠ è½½ä¸­... (é¦–æ¬¡è¿è¡Œè¾ƒæ…¢)'):
                    model = load_model(CONFIG_FILE, CHECKPOINT_FILE)
                
                if model is not None:
                    with st.spinner('âš™ï¸ CPUæ­£åœ¨è¿›è¡Œæ»‘çª—æ¨ç†ï¼Œè¯·ç¨å€™...'):
                        segmentation_map = run_inference(model, image_np)
                    
                    if segmentation_map is not None:
                        color_result_map = draw_segmentation_map(segmentation_map, PALETTE)
                        st.image(color_result_map, use_container_width=True, caption="æ¨¡å‹åˆ†å‰²ç»“æœ")
                        
                        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆå¯æŠ˜å ï¼‰
                        with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯"):
                            st.write(f"è¾“å‡º `segmentation_map` çš„å½¢çŠ¶: {segmentation_map.shape}")
                            st.write(f"æ•°æ®ç±»å‹: {segmentation_map.dtype}")
                            st.write(f"æœ€å°å€¼: {np.min(segmentation_map)}")
                            st.write(f"æœ€å¤§å€¼: {np.max(segmentation_map)}")
                            unique_values = np.unique(segmentation_map)
                            st.write(f"åŒ…å«çš„å”¯ä¸€å€¼ (å‰20ä¸ª): {unique_values[:20]}")
                            st.write(f"å”¯ä¸€å€¼æ€»æ•°: {len(unique_values)}")
                else:
                    st.error("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
    
    except Exception as e:
        st.error(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")

elif uploaded_file is not None:
    st.warning("è¯·å…ˆå®‰è£…å¿…è¦çš„ä¾èµ–å¹¶å‡†å¤‡æ¨¡å‹æ–‡ä»¶ã€‚")

# --- é¡µé¢åº•éƒ¨ä¿¡æ¯ ---
st.markdown("---")
st.markdown("""
### ğŸ”§ ä½¿ç”¨è¯´æ˜
1. **å‡†å¤‡æ–‡ä»¶**: å°†æ¨¡å‹æƒé‡æ–‡ä»¶æ”¾åœ¨ `checkpoints/` ç›®å½•ä¸‹ï¼Œé…ç½®æ–‡ä»¶æ”¾åœ¨ `configs/` ç›®å½•ä¸‹
2. **ä¸Šä¼ å›¾ç‰‡**: æ”¯æŒ JPGã€PNGã€TIF ç­‰æ ¼å¼çš„é¥æ„Ÿå½±åƒ
3. **ç­‰å¾…å¤„ç†**: CPUæ¨ç†éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
4. **æŸ¥çœ‹ç»“æœ**: åˆ†å‰²ç»“æœä¼šæ˜¾ç¤ºåœ¨å³ä¾§ï¼ŒåŒ…å«ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯
5. **ä¸‹è½½ç»“æœ**: å¯ä»¥ä¸‹è½½å½©è‰²åˆ†å‰²å›¾ç”¨äºè¿›ä¸€æ­¥åˆ†æ

### âš¡ æ€§èƒ½è¯´æ˜
- **æ¨ç†æ—¶é—´**: 1-3åˆ†é’Ÿï¼ˆå–å†³äºå›¾ç‰‡å¤§å°ï¼‰
- **å†…å­˜éœ€æ±‚**: å»ºè®®16GBä»¥ä¸Š
- **æ”¯æŒæ ¼å¼**: JPG, PNG, TIF, TIFF
- **æœ€å¤§å°ºå¯¸**: å»ºè®®ä¸è¶…è¿‡2048x2048åƒç´ 
""")