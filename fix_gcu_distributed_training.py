#!/usr/bin/env python3
"""
GCUåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒé…ç½®è„šæœ¬
è§£å†³ç‡§åŸT20 GCUåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒé…ç½®é—®é¢˜
"""

import os
import sys
import subprocess
from pathlib import Path

def check_gcu_environment():
    """æ£€æŸ¥GCUç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥GCUç¯å¢ƒ...")
    
    # æ£€æŸ¥torch_gcuæ¨¡å—
    try:
        import torch_gcu  # type: ignore
        print(f"âœ… torch_gcu å·²å®‰è£…")
        
        # æ£€æŸ¥ç‰ˆæœ¬ä¿¡æ¯
        if hasattr(torch_gcu, '__version__'):
            print(f"   ç‰ˆæœ¬: {torch_gcu.__version__}")
        
        # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§
        if hasattr(torch_gcu, 'is_available') and torch_gcu.is_available():
            device_count = getattr(torch_gcu, 'device_count', lambda: 0)()
            print(f"âœ… GCUè®¾å¤‡å¯ç”¨ï¼Œè®¾å¤‡æ•°é‡: {device_count}")
            return True, device_count
        else:
            print("âŒ GCUè®¾å¤‡ä¸å¯ç”¨")
            return False, 0
            
    except ImportError:
        print("âŒ torch_gcu æœªå®‰è£…")
        return False, 0

def check_distributed_backends():
    """æ£€æŸ¥åˆ†å¸ƒå¼åç«¯æ”¯æŒ"""
    print("\nğŸ” æ£€æŸ¥åˆ†å¸ƒå¼åç«¯æ”¯æŒ...")
    
    import torch
    import torch.distributed as dist
    
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"åˆ†å¸ƒå¼å¯ç”¨: {dist.is_available()}")
    
    # æ£€æŸ¥å„ç§åç«¯
    backends = []
    
    # æ£€æŸ¥NCCL
    try:
        if hasattr(dist, 'is_nccl_available') and dist.is_nccl_available():
            backends.append('nccl')
            print("âœ… NCCLåç«¯å¯ç”¨")
        else:
            print("âŒ NCCLåç«¯ä¸å¯ç”¨")
    except Exception as e:
        print(f"âŒ NCCLæ£€æŸ¥å¤±è´¥: {e}")
    
    # æ£€æŸ¥Gloo
    try:
        if hasattr(dist, 'is_gloo_available') and dist.is_gloo_available():
            backends.append('gloo')
            print("âœ… Glooåç«¯å¯ç”¨")
        else:
            print("âŒ Glooåç«¯ä¸å¯ç”¨")
    except Exception as e:
        print(f"âŒ Glooæ£€æŸ¥å¤±è´¥: {e}")
    
    # æ£€æŸ¥ECCLï¼ˆGCUä¸“ç”¨ï¼‰
    try:
        import eccl  # type: ignore
        backends.append('eccl')
        print("âœ… ECCLåç«¯å¯ç”¨")
    except ImportError:
        print("âŒ ECCLåç«¯ä¸å¯ç”¨")
    
    return backends

def configure_gcu_environment():
    """é…ç½®GCUç¯å¢ƒå˜é‡"""
    print("\nğŸ”§ é…ç½®GCUç¯å¢ƒå˜é‡...")
    
    # GCUç›¸å…³ç¯å¢ƒå˜é‡
    gcu_env_vars = {
        'ECCL_BACKEND': 'gloo',  # ä½¿ç”¨glooä½œä¸ºECCLåç«¯
        'ECCL_DEVICE_TYPE': 'GCU',  # è®¾å¤‡ç±»å‹
        'ECCL_DEBUG': '0',  # è°ƒè¯•çº§åˆ«
        'TOPS_VISIBLE_DEVICES': '',  # å°†åœ¨è¿è¡Œæ—¶è®¾ç½®
        'CUDA_VISIBLE_DEVICES': '',  # ç¦ç”¨CUDAè®¾å¤‡
    }
    
    # ç½‘ç»œç›¸å…³ç¯å¢ƒå˜é‡
    network_env_vars = {
        'MASTER_ADDR': '127.0.0.1',  # ä¸»èŠ‚ç‚¹åœ°å€
        'MASTER_PORT': '29500',  # ä¸»èŠ‚ç‚¹ç«¯å£
        'GLOO_SOCKET_IFNAME': 'lo',  # ç½‘ç»œæ¥å£ï¼ˆæœ¬åœ°å›ç¯ï¼‰
        'GLOO_TIMEOUT_SECONDS': '300',  # è¶…æ—¶æ—¶é—´
    }
    
    all_env_vars = {**gcu_env_vars, **network_env_vars}
    
    for key, value in all_env_vars.items():
        current_value = os.environ.get(key)
        if current_value != value:
            os.environ[key] = value
            print(f"âœ… è®¾ç½® {key}={value}")
        else:
            print(f"âœ… {key} å·²æ­£ç¡®è®¾ç½®")
    
    return all_env_vars

def create_gcu_training_script():
    """åˆ›å»ºGCUè®­ç»ƒå¯åŠ¨è„šæœ¬"""
    print("\nğŸ“ åˆ›å»ºGCUè®­ç»ƒå¯åŠ¨è„šæœ¬...")
    
    project_root = Path.cwd()
    script_path = project_root / 'start_gcu_training_fixed.sh'
    
    content = '''#!/bin/bash
# GCUåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬ï¼ˆä¿®å¤ç‰ˆï¼‰

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸš€ å¯åŠ¨GCUåˆ†å¸ƒå¼è®­ç»ƒï¼ˆä¿®å¤ç‰ˆï¼‰"

# æ£€æŸ¥GCUè®¾å¤‡
echo "ğŸ” æ£€æŸ¥GCUè®¾å¤‡..."
python3 -c "
try:
    import torch_gcu
    if torch_gcu.is_available():
        device_count = torch_gcu.device_count()
        print(f'âœ… GCUè®¾å¤‡å¯ç”¨ï¼Œæ•°é‡: {device_count}')
        if device_count < 8:
            print(f'âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°{device_count}ä¸ªGCUè®¾å¤‡ï¼Œå°‘äº8ä¸ª')
    else:
        print('âŒ GCUè®¾å¤‡ä¸å¯ç”¨')
        exit(1)
except ImportError:
    print('âŒ torch_gcuæœªå®‰è£…')
    exit(1)
"

# è®¾ç½®ç¯å¢ƒå˜é‡
export PROJECT_ROOT="$(pwd)"
export PYTHONPATH="$PROJECT_ROOT:$PYTHONPATH"

# GCUç›¸å…³ç¯å¢ƒå˜é‡
export ECCL_BACKEND=gloo
export ECCL_DEVICE_TYPE=GCU
export ECCL_DEBUG=0
export CUDA_VISIBLE_DEVICES=""

# åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå˜é‡
export WORLD_SIZE=8
export MASTER_ADDR=127.0.0.1
export MASTER_PORT=29500

# ç½‘ç»œé…ç½®
export GLOO_SOCKET_IFNAME=lo
export GLOO_TIMEOUT_SECONDS=300

# è®­ç»ƒå‚æ•°
CONFIG_FILE="configs/dinov3/dinov3_vit-l16_mmrs1m_t20_gcu.py"
WORK_DIR="./work_dirs/dinov3_mmrs1m_t20_gcu"
TRAIN_SCRIPT="scripts/train_distributed_pytorch_ddp_8card_gcu.py"

# æ£€æŸ¥å¿…è¦æ–‡ä»¶
if [ ! -f "$CONFIG_FILE" ]; then
    echo "âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $CONFIG_FILE"
    exit 1
fi

if [ ! -f "$TRAIN_SCRIPT" ]; then
    echo "âŒ è®­ç»ƒè„šæœ¬ä¸å­˜åœ¨: $TRAIN_SCRIPT"
    exit 1
fi

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p "$WORK_DIR"

# åœæ­¢ä¹‹å‰çš„è®­ç»ƒè¿›ç¨‹
echo "ğŸ›‘ åœæ­¢ä¹‹å‰çš„è®­ç»ƒè¿›ç¨‹..."
pkill -f "train_distributed_pytorch_ddp_8card_gcu.py" || true
sleep 2

# è®¾ç½®Pythonè·¯å¾„
export PYTHONPATH="$PROJECT_ROOT:$PYTHONPATH"

echo "ğŸ“‹ è®­ç»ƒé…ç½®:"
echo "  é…ç½®æ–‡ä»¶: $CONFIG_FILE"
echo "  å·¥ä½œç›®å½•: $WORK_DIR"
echo "  è®­ç»ƒè„šæœ¬: $TRAIN_SCRIPT"
echo "  è®¾å¤‡æ•°é‡: $WORLD_SIZE"
echo "  ä¸»èŠ‚ç‚¹: $MASTER_ADDR:$MASTER_PORT"

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
echo "ğŸš€ å¯åŠ¨8å¡GCUåˆ†å¸ƒå¼è®­ç»ƒ..."

# ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
torchrun \\
    --standalone \\
    --nnodes=1 \\
    --nproc_per_node=8 \\
    --rdzv_backend=c10d \\
    --rdzv_endpoint=127.0.0.1:29500 \\
    "$TRAIN_SCRIPT" \\
    --config "$CONFIG_FILE" \\
    --work-dir "$WORK_DIR" \\
    --launcher pytorch

echo "âœ… è®­ç»ƒå¯åŠ¨å®Œæˆ"
echo "ğŸ“Š ç›‘æ§GCUè®¾å¤‡ä½¿ç”¨æƒ…å†µ:"
echo "   ä½¿ç”¨å‘½ä»¤: watch -n 1 'python3 -c \"import torch_gcu; print(f\\\"GCUè®¾å¤‡æ•°é‡: {torch_gcu.device_count()}\\\")\"'"
echo "ğŸ“ æ—¥å¿—ä¿å­˜åœ¨: $WORK_DIR"
'''
    
    script_path.write_text(content, encoding='utf-8')
    script_path.chmod(0o755)
    print(f"âœ… åˆ›å»ºGCUè®­ç»ƒè„šæœ¬: {script_path}")

def create_single_process_fallback_script():
    """åˆ›å»ºå•è¿›ç¨‹å›é€€è„šæœ¬"""
    print("\nğŸ“ åˆ›å»ºå•è¿›ç¨‹å›é€€è„šæœ¬...")
    
    project_root = Path.cwd()
    script_path = project_root / 'start_single_gcu_training.sh'
    
    content = '''#!/bin/bash
# å•è¿›ç¨‹GCUè®­ç»ƒè„šæœ¬ï¼ˆåˆ†å¸ƒå¼å¤±è´¥æ—¶çš„å›é€€æ–¹æ¡ˆï¼‰

set -e

echo "ğŸš€ å¯åŠ¨å•è¿›ç¨‹GCUè®­ç»ƒ"

# è®¾ç½®ç¯å¢ƒå˜é‡
export PROJECT_ROOT="$(pwd)"
export PYTHONPATH="$PROJECT_ROOT:$PYTHONPATH"

# å•è¿›ç¨‹æ¨¡å¼
export WORLD_SIZE=1
export RANK=0
export LOCAL_RANK=0

# GCUç¯å¢ƒå˜é‡
export ECCL_BACKEND=gloo
export ECCL_DEVICE_TYPE=GCU
export ECCL_DEBUG=0
export CUDA_VISIBLE_DEVICES=""
export TOPS_VISIBLE_DEVICES=0

# è®­ç»ƒå‚æ•°
CONFIG_FILE="configs/dinov3/dinov3_vit-l16_mmrs1m_t20_gcu.py"
WORK_DIR="./work_dirs/dinov3_mmrs1m_t20_gcu_single"
TRAIN_SCRIPT="scripts/train_distributed_pytorch_ddp_8card_gcu.py"

# åˆ›å»ºå·¥ä½œç›®å½•
mkdir -p "$WORK_DIR"

echo "ğŸ“‹ å•è¿›ç¨‹è®­ç»ƒé…ç½®:"
echo "  é…ç½®æ–‡ä»¶: $CONFIG_FILE"
echo "  å·¥ä½œç›®å½•: $WORK_DIR"
echo "  è®­ç»ƒè„šæœ¬: $TRAIN_SCRIPT"
echo "  æ¨¡å¼: å•è¿›ç¨‹"

# å¯åŠ¨å•è¿›ç¨‹è®­ç»ƒ
echo "ğŸš€ å¯åŠ¨å•è¿›ç¨‹GCUè®­ç»ƒ..."

python3 "$TRAIN_SCRIPT" \\
    --config "$CONFIG_FILE" \\
    --work-dir "$WORK_DIR" \\
    --launcher none

echo "âœ… å•è¿›ç¨‹è®­ç»ƒå¯åŠ¨å®Œæˆ"
'''
    
    script_path.write_text(content, encoding='utf-8')
    script_path.chmod(0o755)
    print(f"âœ… åˆ›å»ºå•è¿›ç¨‹è®­ç»ƒè„šæœ¬: {script_path}")

def test_distributed_initialization():
    """æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–"""
    print("\nğŸ§ª æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–...")
    
    # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
    os.environ['WORLD_SIZE'] = '1'
    os.environ['RANK'] = '0'
    os.environ['LOCAL_RANK'] = '0'
    os.environ['MASTER_ADDR'] = '127.0.0.1'
    os.environ['MASTER_PORT'] = '29501'  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª
    
    try:
        import torch
        import torch.distributed as dist
        
        # æµ‹è¯•glooåç«¯
        print("æµ‹è¯•glooåç«¯...")
        try:
            dist.init_process_group(
                backend='gloo',
                init_method='env://',
                world_size=1,
                rank=0
            )
            print("âœ… glooåç«¯åˆå§‹åŒ–æˆåŠŸ")
            dist.destroy_process_group()
        except Exception as e:
            print(f"âŒ glooåç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼åˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å¼€å§‹é…ç½®GCUåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ...")
    
    # 1. æ£€æŸ¥GCUç¯å¢ƒ
    gcu_available, device_count = check_gcu_environment()
    
    # 2. æ£€æŸ¥åˆ†å¸ƒå¼åç«¯
    available_backends = check_distributed_backends()
    
    # 3. é…ç½®GCUç¯å¢ƒå˜é‡
    env_vars = configure_gcu_environment()
    
    # 4. åˆ›å»ºè®­ç»ƒè„šæœ¬
    create_gcu_training_script()
    create_single_process_fallback_script()
    
    # 5. æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–
    dist_test_ok = test_distributed_initialization()
    
    # æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ‰ GCUåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒé…ç½®å®Œæˆï¼")
    print("="*50)
    
    print(f"GCUè®¾å¤‡: {'âœ… å¯ç”¨' if gcu_available else 'âŒ ä¸å¯ç”¨'}")
    if gcu_available:
        print(f"è®¾å¤‡æ•°é‡: {device_count}")
    
    print(f"å¯ç”¨åç«¯: {', '.join(available_backends) if available_backends else 'æ— '}")
    print(f"åˆ†å¸ƒå¼æµ‹è¯•: {'âœ… é€šè¿‡' if dist_test_ok else 'âŒ å¤±è´¥'}")
    
    print("\nğŸ“‹ ä½¿ç”¨è¯´æ˜:")
    if gcu_available and device_count >= 8:
        print("1. 8å¡åˆ†å¸ƒå¼è®­ç»ƒ: ./start_gcu_training_fixed.sh")
    print("2. å•è¿›ç¨‹è®­ç»ƒ: ./start_single_gcu_training.sh")
    print("3. å¦‚æœåˆ†å¸ƒå¼å¤±è´¥ï¼Œè®­ç»ƒè„šæœ¬ä¼šè‡ªåŠ¨å›é€€åˆ°å•è¿›ç¨‹æ¨¡å¼")
    
    if not gcu_available:
        print("\nâš ï¸  è­¦å‘Š: GCUè®¾å¤‡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥:")
        print("1. torch_gcuæ˜¯å¦æ­£ç¡®å®‰è£…")
        print("2. GCUé©±åŠ¨æ˜¯å¦æ­£ç¡®å®‰è£…")
        print("3. è®¾å¤‡æ˜¯å¦æ­£ç¡®è¿æ¥")
    
    return 0 if gcu_available else 1

if __name__ == '__main__':
    sys.exit(main())