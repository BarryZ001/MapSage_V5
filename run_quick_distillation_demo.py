#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ MapSage V5 - å¿«é€ŸçŸ¥è¯†è’¸é¦æ¼”ç¤º

ç®€åŒ–ç‰ˆå®éªŒï¼Œç”¨äºå¿«é€ŸéªŒè¯æ”¹è¿›ç­–ç•¥çš„æœ‰æ•ˆæ€§
æ ¸å¿ƒæ”¹è¿›ç‚¹çš„å¿«é€Ÿæ¼”ç¤ºå’ŒéªŒè¯

Author: MapSage Team
Date: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
from datetime import datetime

# ğŸ¯ ç®€åŒ–ç‰ˆçŸ¥è¯†è’¸é¦æ¨¡å‹
class QuickDistillationModel(nn.Module):
    """å¿«é€ŸçŸ¥è¯†è’¸é¦æ¼”ç¤ºæ¨¡å‹ - éªŒè¯æ ¸å¿ƒæ”¹è¿›ç­–ç•¥"""
    
    def __init__(self, distill_cfg=None):
        super().__init__()
        
        # è’¸é¦é…ç½®
        self.distill_cfg = distill_cfg or {}
        self.task_weight = self.distill_cfg.get('task_weight', 0.6)
        self.distill_weight = self.distill_cfg.get('distill_weight', 0.3)
        self.feature_weight = self.distill_cfg.get('feature_weight', 0.1)
        
        # æ¸©åº¦è°ƒåº¦
        self.initial_temperature = self.distill_cfg.get('initial_temperature', 6.0)
        self.final_temperature = self.distill_cfg.get('final_temperature', 3.0)
        self.current_temperature = self.initial_temperature
        
        # æ¸è¿›å¼è®­ç»ƒ
        self.warmup_epochs = self.distill_cfg.get('warmup_epochs', 5)
        self.current_epoch = 0
        
        # ç®€åŒ–çš„æ•™å¸ˆæ¨¡å‹
        self.teacher_model = self._create_simple_teacher()
        self.teacher_model.eval()
        
        # ç®€åŒ–çš„å­¦ç”Ÿæ¨¡å‹
        self.student_model = self._create_simple_student()
        
        # ç‰¹å¾å¯¹é½
        self.feature_adapter = nn.Sequential(
            nn.Conv2d(64, 128, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True)
        )
        
        # æŸå¤±å‡½æ•°
        self.task_loss = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')
        self.feature_loss = nn.MSELoss(reduction='mean')
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
        
        print("âœ… å¿«é€Ÿè’¸é¦æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š æŸå¤±æƒé‡: ä»»åŠ¡={self.task_weight}, è’¸é¦={self.distill_weight}, ç‰¹å¾={self.feature_weight}")
    
    def _create_simple_teacher(self):
        """åˆ›å»ºç®€åŒ–æ•™å¸ˆæ¨¡å‹"""
        return nn.Sequential(
            nn.Conv2d(3, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 128, 3, 1, 1),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.Conv2d(128, 7, 1)  # 7ç±»åˆ†å‰²
        )
    
    def _create_simple_student(self):
        """åˆ›å»ºç®€åŒ–å­¦ç”Ÿæ¨¡å‹"""
        return nn.Sequential(
            nn.Conv2d(3, 32, 3, 1, 1),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 7, 1)  # 7ç±»åˆ†å‰²
        )
    
    def update_temperature(self, epoch, total_epochs):
        """æ›´æ–°æ¸©åº¦å‚æ•°"""
        self.current_epoch = epoch
        progress = epoch / total_epochs
        self.current_temperature = self.initial_temperature - \
                                 (self.initial_temperature - self.final_temperature) * progress
    
    def forward(self, x, targets=None, epoch=0):
        """å‰å‘ä¼ æ’­ - æ¸è¿›å¼è®­ç»ƒç­–ç•¥"""
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_features = []
        student_x = x
        
        # æå–ä¸­é—´ç‰¹å¾
        for i, layer in enumerate(self.student_model):
            student_x = layer(student_x)
            if i == 4:  # ç¬¬äºŒä¸ªå·ç§¯å±‚åçš„ç‰¹å¾
                student_features.append(student_x)
        
        student_logits = student_x
        
        # è®¡ç®—ä»»åŠ¡æŸå¤±
        task_loss = 0
        if targets is not None:
            task_loss = self.task_loss(student_logits, targets)
        
        # æ¸è¿›å¼è®­ç»ƒ: å‰å‡ ä¸ªepochåªåšä»»åŠ¡è®­ç»ƒ
        if epoch < self.warmup_epochs:
            return {
                'total_loss': task_loss,
                'task_loss': task_loss,
                'distill_loss': torch.tensor(0.0),
                'feature_loss': torch.tensor(0.0),
                'logits': student_logits
            }
        
        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            teacher_features = []
            teacher_x = x
            
            for i, layer in enumerate(self.teacher_model):
                teacher_x = layer(teacher_x)
                if i == 4:  # å¯¹åº”çš„æ•™å¸ˆç‰¹å¾
                    teacher_features.append(teacher_x)
            
            teacher_logits = teacher_x
        
        # è¾“å‡ºè’¸é¦æŸå¤±
        teacher_probs = F.softmax(teacher_logits / self.current_temperature, dim=1)
        student_log_probs = F.log_softmax(student_logits / self.current_temperature, dim=1)
        distill_loss = self.kl_loss(student_log_probs, teacher_probs) * (self.current_temperature ** 2)
        
        # ç‰¹å¾è’¸é¦æŸå¤±
        feature_loss = 0
        if len(student_features) > 0 and len(teacher_features) > 0:
            aligned_student_feat = self.feature_adapter(student_features[0])
            feature_loss = self.feature_loss(aligned_student_feat, teacher_features[0])
        
        # æ€»æŸå¤± (æ”¹è¿›çš„æƒé‡å¹³è¡¡)
        total_loss = (self.task_weight * task_loss + 
                     self.distill_weight * distill_loss + 
                     self.feature_weight * feature_loss)
        
        return {
            'total_loss': total_loss,
            'task_loss': task_loss,
            'distill_loss': distill_loss,
            'feature_loss': feature_loss,
            'logits': student_logits
        }

# ğŸš€ å¿«é€Ÿè®­ç»ƒå‡½æ•°
def quick_distillation_demo():
    """å¿«é€ŸçŸ¥è¯†è’¸é¦æ¼”ç¤º"""
    print("ğŸ¯ MapSage V5 - å¿«é€ŸçŸ¥è¯†è’¸é¦æ¼”ç¤º")
    print("=" * 60)
    
    # å®éªŒé…ç½®
    config = {
        'epochs': 15,
        'batch_size': 2,
        'learning_rate': 0.001,
        'device': 'cpu',
        'image_size': (128, 128)  # å‡å°å›¾åƒå°ºå¯¸
    }
    
    # è’¸é¦é…ç½® (æ ¸å¿ƒæ”¹è¿›)
    distill_config = {
        'task_weight': 0.6,
        'distill_weight': 0.3,
        'feature_weight': 0.1,
        'initial_temperature': 6.0,
        'final_temperature': 3.0,
        'warmup_epochs': 5
    }
    
    print(f"ğŸ“Š å®éªŒé…ç½®: {config}")
    print(f"ğŸ¯ è’¸é¦é…ç½®: {distill_config}")
    
    # åˆ›å»ºæ¨¡å‹
    model = QuickDistillationModel(distill_cfg=distill_config)
    model = model.to(config['device'])
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(
        model.student_model.parameters(),
        lr=config['learning_rate']
    )
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=config['epochs'],
        eta_min=config['learning_rate'] * 0.1
    )
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®
    def create_batch():
        images = torch.randn(config['batch_size'], 3, *config['image_size'])
        targets = torch.randint(0, 7, (config['batch_size'], *config['image_size']))
        return images.to(config['device']), targets.to(config['device'])
    
    print("\nğŸš€ å¼€å§‹å¿«é€Ÿè®­ç»ƒæ¼”ç¤º...")
    
    # è®°å½•ç»“æœ
    results = []
    best_loss = float('inf')
    
    start_time = time.time()
    
    for epoch in range(config['epochs']):
        model.train()
        model.update_temperature(epoch, config['epochs'])
        
        # æ¨¡æ‹Ÿè®­ç»ƒä¸€ä¸ªepoch (10ä¸ªbatch)
        epoch_losses = {'total': 0, 'task': 0, 'distill': 0, 'feature': 0}
        num_batches = 10
        
        for batch_idx in range(num_batches):
            images, targets = create_batch()
            
            # å‰å‘ä¼ æ’­
            outputs = model(images, targets, epoch)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            outputs['total_loss'].backward()
            optimizer.step()
            
            # ç´¯ç§¯æŸå¤±
            epoch_losses['total'] += outputs['total_loss'].item()
            epoch_losses['task'] += outputs['task_loss'].item()
            epoch_losses['distill'] += outputs['distill_loss'].item()
            epoch_losses['feature'] += outputs['feature_loss'].item()
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_losses:
            epoch_losses[key] /= num_batches
        
        # è®°å½•ç»“æœ
        current_lr = scheduler.get_last_lr()[0]
        current_temp = model.current_temperature
        
        result = {
            'epoch': epoch + 1,
            'losses': epoch_losses,
            'lr': current_lr,
            'temperature': current_temp
        }
        results.append(result)
        
        # æ‰“å°è¿›åº¦
        phase = "é¢„è®­ç»ƒé˜¶æ®µ" if epoch < distill_config['warmup_epochs'] else "è’¸é¦é˜¶æ®µ"
        print(f"ğŸ“ˆ Epoch {epoch+1:2d}/{config['epochs']} [{phase}]:")
        print(f"    æ€»æŸå¤±: {epoch_losses['total']:.4f}")
        print(f"    ä»»åŠ¡æŸå¤±: {epoch_losses['task']:.4f}")
        print(f"    è’¸é¦æŸå¤±: {epoch_losses['distill']:.4f}")
        print(f"    ç‰¹å¾æŸå¤±: {epoch_losses['feature']:.4f}")
        print(f"    å­¦ä¹ ç‡: {current_lr:.6f}")
        print(f"    æ¸©åº¦: {current_temp:.2f}")
        
        # æ›´æ–°æœ€ä½³æŸå¤±
        if epoch_losses['total'] < best_loss:
            best_loss = epoch_losses['total']
            print(f"    ğŸ’¾ æœ€ä½³æ¨¡å‹æ›´æ–° (æŸå¤±: {best_loss:.4f})")
        
        print()
    
    total_time = time.time() - start_time
    print(f"ğŸ‰ å¿«é€Ÿæ¼”ç¤ºå®Œæˆ! ç”¨æ—¶: {total_time:.2f}ç§’")
    print(f"ğŸ“Š æœ€ä½³æŸå¤±: {best_loss:.4f}")
    
    return results, model

# ğŸ“Š ç»“æœåˆ†æå‡½æ•°
def analyze_quick_results(results):
    """åˆ†æå¿«é€Ÿå®éªŒç»“æœ"""
    print("\nğŸ“ˆ å¿«é€Ÿå®éªŒç»“æœåˆ†æ:")
    print("=" * 50)
    
    # æå–æŸå¤±æ•°æ®
    epochs = [r['epoch'] for r in results]
    total_losses = [r['losses']['total'] for r in results]
    task_losses = [r['losses']['task'] for r in results]
    distill_losses = [r['losses']['distill'] for r in results]
    
    # é˜¶æ®µåˆ†æ
    warmup_epochs = 5
    warmup_task_losses = task_losses[:warmup_epochs]
    distill_task_losses = task_losses[warmup_epochs:]
    
    print(f"ğŸ“Š é¢„è®­ç»ƒé˜¶æ®µ (Epochs 1-{warmup_epochs}):")
    if len(warmup_task_losses) > 1:
        warmup_improvement = (warmup_task_losses[0] - warmup_task_losses[-1]) / warmup_task_losses[0] * 100
        print(f"    ä»»åŠ¡æŸå¤±: {warmup_task_losses[0]:.4f} â†’ {warmup_task_losses[-1]:.4f}")
        print(f"    æ”¹å–„å¹…åº¦: {warmup_improvement:.2f}%")
        
        if warmup_improvement > 10:
            print("    âœ… é¢„è®­ç»ƒé˜¶æ®µæ•ˆæœæ˜¾è‘—!")
        else:
            print("    âš ï¸  é¢„è®­ç»ƒé˜¶æ®µæ”¹å–„æœ‰é™")
    
    print(f"\nğŸ“Š è’¸é¦é˜¶æ®µ (Epochs {warmup_epochs+1}-{len(results)}):")
    if len(distill_task_losses) > 1:
        distill_improvement = (distill_task_losses[0] - distill_task_losses[-1]) / distill_task_losses[0] * 100
        print(f"    ä»»åŠ¡æŸå¤±: {distill_task_losses[0]:.4f} â†’ {distill_task_losses[-1]:.4f}")
        print(f"    æ”¹å–„å¹…åº¦: {distill_improvement:.2f}%")
        print(f"    è’¸é¦æŸå¤±: {distill_losses[warmup_epochs]:.4f} â†’ {distill_losses[-1]:.4f}")
        
        if distill_improvement > 5:
            print("    âœ… è’¸é¦é˜¶æ®µçŸ¥è¯†ä¼ é€’æœ‰æ•ˆ!")
        else:
            print("    âš ï¸  è’¸é¦é˜¶æ®µæ”¹å–„æœ‰é™")
    
    # æ•´ä½“åˆ†æ
    print(f"\nğŸ“Š æ•´ä½“è®­ç»ƒæ•ˆæœ:")
    overall_improvement = (task_losses[0] - task_losses[-1]) / task_losses[0] * 100
    print(f"    ä»»åŠ¡æŸå¤±æ€»æ”¹å–„: {overall_improvement:.2f}%")
    print(f"    æœ€ç»ˆä»»åŠ¡æŸå¤±: {task_losses[-1]:.4f}")
    print(f"    æœ€ç»ˆæ€»æŸå¤±: {total_losses[-1]:.4f}")
    
    # ä¸å‰æœŸå®éªŒå¯¹æ¯”
    print(f"\nğŸ” ä¸å‰æœŸå®éªŒå¯¹æ¯”:")
    baseline_task_loss = 1.9589  # å‰æœŸå®éªŒæœ€ç»ˆä»»åŠ¡æŸå¤±
    
    if task_losses[-1] < baseline_task_loss:
        improvement_vs_baseline = (baseline_task_loss - task_losses[-1]) / baseline_task_loss * 100
        print(f"    å‰æœŸå®éªŒä»»åŠ¡æŸå¤±: {baseline_task_loss:.4f}")
        print(f"    å½“å‰å®éªŒä»»åŠ¡æŸå¤±: {task_losses[-1]:.4f}")
        print(f"    ç›¸å¯¹æ”¹å–„: {improvement_vs_baseline:.2f}%")
        print("    ğŸ‰ æ”¹è¿›ç­–ç•¥éªŒè¯æˆåŠŸ!")
    else:
        print(f"    å½“å‰ä»»åŠ¡æŸå¤± ({task_losses[-1]:.4f}) ä»é«˜äºåŸºçº¿ ({baseline_task_loss:.4f})")
        print("    âš ï¸  éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ç­–ç•¥")
    
    # æ”¹è¿›å»ºè®®
    print(f"\nğŸ’¡ åŸºäºå¿«é€Ÿå®éªŒçš„æ”¹è¿›å»ºè®®:")
    
    if overall_improvement > 20:
        print("    âœ… å½“å‰ç­–ç•¥æ•ˆæœè‰¯å¥½ï¼Œå»ºè®®:")
        print("        - åœ¨å®Œæ•´æ•°æ®é›†ä¸ŠéªŒè¯")
        print("        - é€‚å½“å¢åŠ æ¨¡å‹å¤æ‚åº¦")
        print("        - å»¶é•¿è®­ç»ƒè½®æ•°")
    elif overall_improvement > 10:
        print("    âš ï¸  ç­–ç•¥æœ‰æ•ˆä½†æœ‰æ”¹è¿›ç©ºé—´ï¼Œå»ºè®®:")
        print("        - è°ƒæ•´æŸå¤±æƒé‡æ¯”ä¾‹")
        print("        - ä¼˜åŒ–æ¸©åº¦è°ƒåº¦ç­–ç•¥")
        print("        - å¢åŠ é¢„è®­ç»ƒè½®æ•°")
    else:
        print("    âŒ ç­–ç•¥æ•ˆæœæœ‰é™ï¼Œå»ºè®®:")
        print("        - é‡æ–°è¯„ä¼°æ¶æ„åŒ¹é…åº¦")
        print("        - è°ƒæ•´å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨")
        print("        - è€ƒè™‘å…¶ä»–è’¸é¦ç­–ç•¥")

# ğŸ¯ ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•° - è¿è¡Œå¿«é€ŸçŸ¥è¯†è’¸é¦æ¼”ç¤º"""
    print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # è¿è¡Œå¿«é€Ÿæ¼”ç¤º
    results, model = quick_distillation_demo()
    
    # åˆ†æç»“æœ
    analyze_quick_results(results)
    
    print(f"\nğŸ• ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nğŸ¯ å¿«é€ŸçŸ¥è¯†è’¸é¦æ¼”ç¤ºå®Œæˆ!")
    
    return results, model

if __name__ == "__main__":
    results, model = main()