# Kaggle Notebook - ç»Ÿä¸€Cellç‰ˆæœ¬

è¿™ä¸ªæ–‡ä»¶å°†æ‰€æœ‰ä»£ç åˆå¹¶åˆ°ä¸€ä¸ªCellä¸­ï¼Œä»¥é¿å…Kaggleç¯å¢ƒä¸­å¤šä¸ªCellä¹‹é—´çš„çŠ¶æ€å†²çªé—®é¢˜ã€‚

## ç»Ÿä¸€Cell - å®Œæ•´è®­ç»ƒä»£ç 

```python
# ğŸ”„ Kaggleå†…æ ¸é‡å¯åçš„å¿«é€Ÿç¯å¢ƒæ£€æŸ¥å’Œæ¢å¤
print("ğŸ”„ æ£€æŸ¥Kaggleå†…æ ¸é‡å¯åçš„ç¯å¢ƒçŠ¶æ€...")

import sys
import subprocess
import importlib.util

# æ£€æŸ¥å…³é”®åŒ…æ˜¯å¦å·²å®‰è£…
required_packages = {
    'mmcv': '2.1.0',
    'mmengine': '0.10.1', 
    'mmsegmentation': None  # ä»»æ„å…¼å®¹ç‰ˆæœ¬
}

missing_packages = []
for package, expected_version in required_packages.items():
    spec = importlib.util.find_spec(package)
    if spec is None:
        missing_packages.append(package)
        print(f"âŒ {package} æœªå®‰è£…")
    else:
        try:
            module = importlib.import_module(package)
            version = getattr(module, '__version__', 'unknown')
            if expected_version and version != expected_version:
                print(f"âš ï¸ {package} ç‰ˆæœ¬ä¸åŒ¹é…: {version} (æœŸæœ›: {expected_version})")
                missing_packages.append(package)
            else:
                print(f"âœ… {package} å·²å®‰è£…: {version}")
        except ImportError:
            missing_packages.append(package)
            print(f"âŒ {package} å¯¼å…¥å¤±è´¥")

if missing_packages:
    print(f"\nğŸš¨ æ£€æµ‹åˆ° {len(missing_packages)} ä¸ªåŒ…éœ€è¦é‡æ–°å®‰è£…")
    print("ğŸ“‹ è¯·è¿è¡Œä¸‹ä¸€ä¸ªCellè¿›è¡Œå®Œæ•´çš„ç¯å¢ƒè®¾ç½®")
else:
    print("\nâœ… æ‰€æœ‰å…³é”®åŒ…å·²æ­£ç¡®å®‰è£…ï¼Œå¯ä»¥ç›´æ¥è·³è½¬åˆ°è®­ç»ƒCell")
    print("ğŸ’¡ æç¤º: å¦‚æœé‡åˆ°å¯¼å…¥é”™è¯¯ï¼Œè¯·è¿è¡Œä¸‹ä¸€ä¸ªCellé‡æ–°å®‰è£…ä¾èµ–")

# æ£€æŸ¥GPUçŠ¶æ€
import torch
if torch.cuda.is_available():
    print(f"ğŸ® GPUå¯ç”¨: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
else:
    print("âš ï¸ GPUä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")

print("\n" + "="*50)

# ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…

# Install required packages with proper mmcv installation
!pip install -q mmengine==0.10.1 ftfy regex
!pip install -q -U openmim
# Force remove any existing mmcv installations to avoid conflicts
!pip uninstall -y mmcv mmcv-full mmcv-lite
# Clear pip cache to ensure clean installation
!pip cache purge
# Use mmcv==2.1.0 for stable compatibility with updated mmsegmentation
!mim install "mmcv==2.1.0" --force-reinstall -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html
# Use compatible mmsegmentation version for mmcv 2.0+
!pip install -q "mmsegmentation>=1.2.0" --force-reinstall
!pip install -q opencv-python-headless pillow numpy torch torchvision

# Important: Restart kernel after installing new mmcv version
print("âœ… æ‰€æœ‰ä¾èµ–åŒ…å®‰è£…å®Œæˆ")
print("âš ï¸ é‡è¦æç¤ºï¼šå®‰è£…å®Œæˆåè¯·é‡å¯å†…æ ¸(Restart Kernel)ä»¥ç¡®ä¿æ–°ç‰ˆæœ¬MMCVç”Ÿæ•ˆ")
print("ğŸ“‹ æ­¥éª¤ï¼šKernel -> Restart Kernelï¼Œç„¶åé‡æ–°è¿è¡Œæ‰€æœ‰Cell")

# é…ç½®æ–‡ä»¶åˆ›å»º

# Create the training configuration
config_content = '''
# model settings
norm_cfg = dict(type='SyncBN', requires_grad=True)
data_preprocessor = dict(
    type='SegDataPreProcessor',
    mean=[123.675, 116.28, 103.53],
    std=[58.395, 57.12, 57.375],
    bgr_to_rgb=True,
    pad_val=0,
    seg_pad_val=255,
    size=(512, 512))
model = dict(
    type='EncoderDecoder',
    data_preprocessor=data_preprocessor,
    backbone=dict(
        type='MixVisionTransformer',
        in_channels=3,
        embed_dims=[64, 128, 256, 512],
        num_stages=4,
        num_layers=[3, 4, 6, 3],
        num_heads=[1, 2, 4, 8],
        patch_sizes=[7, 3, 3, 3],
        sr_ratios=[8, 4, 2, 1],
        out_indices=(0, 1, 2, 3),
        mlp_ratio=4,
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.1),
    decode_head=dict(
        type='SegformerHead',
        in_channels=[64, 128, 256, 512],
        in_index=[0, 1, 2, 3],
        channels=256,
        dropout_ratio=0.1,
        num_classes=7,
        norm_cfg=norm_cfg,
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0)),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))

# dataset settings
dataset_type = 'LoveDADataset'
data_root = '/kaggle/input/loveda'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
crop_size = (512, 512)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', scale=(2048, 512), keep_ratio=True),
    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='PackSegInputs')
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', scale=(2048, 512), keep_ratio=True),
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs')
]
train_dataloader = dict(
    batch_size=4,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='InfiniteSampler', shuffle=True),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        data_prefix=dict(
            img_path='Train',
            seg_map_path='Train'),
        pipeline=train_pipeline))
val_dataloader = dict(
    batch_size=1,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        data_prefix=dict(
            img_path='Val',
            seg_map_path='Val'),
        pipeline=test_pipeline))
test_dataloader = val_dataloader

val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])
test_evaluator = val_evaluator

# training schedule
train_cfg = dict(type='IterBasedTrainLoop', max_iters=40000, val_interval=4000)
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')

# optimizer
optimizer = dict(
    type='AdamW', lr=0.00006, betas=(0.9, 0.999), weight_decay=0.01)
optim_wrapper = dict(type='OptimWrapper', optimizer=optimizer, clip_grad=None)

# learning policy
param_scheduler = [
    dict(
        type='LinearLR', start_factor=1e-6, by_epoch=False, begin=0, end=1500),
    dict(
        type='PolyLR',
        power=1.0,
        begin=1500,
        end=40000,
        eta_min=0.0,
        by_epoch=False,
    )
]

# runtime settings
default_scope = 'mmseg'
env_cfg = dict(
    cudnn_benchmark=True,
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='nccl'),
)
vis_backends = [dict(type='LocalVisBackend')]
visualizer = dict(
    type='SegLocalVisualizer', vis_backends=vis_backends, name='visualizer')
log_processor = dict(by_epoch=False)
log_level = 'INFO'
load_from = '/kaggle/input/mapsage-stage02-checkpoint-6000/best_mIoU_iter_6000.pth'
resume = False

# hooks
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggingHook', interval=50, log_metric_by_epoch=False),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(type='CheckpointHook', by_epoch=False, interval=2000, save_best='mIoU'),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='SegVisualizationHook'))

# custom hooks
custom_hooks = []

work_dir = './work_dirs/segformer_mit-b2_8xb1-160k_loveda-512x512'
auto_scale_lr = dict(enable=False, base_batch_size=16)
'''

# Write config to file
with open('/kaggle/working/train_config.py', 'w') as f:
    f.write(config_content)

print("âœ… è®­ç»ƒé…ç½®æ–‡ä»¶å·²åˆ›å»º: /kaggle/working/train_config.py")

# æ•°æ®é›†éªŒè¯

import os

# Check if LoveDA dataset exists
loveda_path = '/kaggle/input/loveda'
if os.path.exists(loveda_path):
    print(f"âœ… LoveDAæ•°æ®é›†è·¯å¾„å­˜åœ¨: {loveda_path}")
    
    # List contents
    contents = os.listdir(loveda_path)
    print(f"ğŸ“ æ•°æ®é›†å†…å®¹: {contents}")
    
    # Check for Train and Val directories
    for split in ['Train', 'Val']:
        split_path = os.path.join(loveda_path, split)
        if os.path.exists(split_path):
            print(f"âœ… {split} ç›®å½•å­˜åœ¨")
            split_contents = os.listdir(split_path)
            print(f"ğŸ“ {split} å†…å®¹: {split_contents}")
            
            # Check for Rural and Urban subdirectories
            for area in ['Rural', 'Urban']:
                area_path = os.path.join(split_path, area)
                if os.path.exists(area_path):
                    area_contents = os.listdir(area_path)
                    print(f"ğŸ“ {split}/{area} å†…å®¹: {area_contents}")
                    
                    # Check for images_png and masks_png
                    for folder in ['images_png', 'masks_png']:
                        folder_path = os.path.join(area_path, folder)
                        if os.path.exists(folder_path):
                            file_count = len(os.listdir(folder_path))
                            print(f"ğŸ“Š {split}/{area}/{folder}: {file_count} ä¸ªæ–‡ä»¶")
                        else:
                            print(f"âŒ {split}/{area}/{folder} ä¸å­˜åœ¨")
                else:
                    print(f"âŒ {split}/{area} ä¸å­˜åœ¨")
        else:
            print(f"âŒ {split} ç›®å½•ä¸å­˜åœ¨")
else:
    print(f"âŒ LoveDAæ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {loveda_path}")
    print("å°†ä½¿ç”¨è™šæ‹Ÿæ•°æ®è¿›è¡Œè®­ç»ƒ")

# Check checkpoint
checkpoint_path = '/kaggle/input/mapsage-stage02-checkpoint-6000/best_mIoU_iter_6000.pth'
if os.path.exists(checkpoint_path):
    print(f"âœ… Checkpointæ–‡ä»¶å­˜åœ¨: {checkpoint_path}")
else:
    print(f"âŒ Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")

print("âœ… æ•°æ®é›†å’ŒcheckpointéªŒè¯å®Œæˆ")

# çŸ¥è¯†è’¸é¦è®­ç»ƒæ‰§è¡Œ

# Import necessary functions for knowledge distillation training
import os
import sys
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, List, Optional, Union, Any
import numpy as np
from PIL import Image
import time

# Critical: Complete registry cleanup BEFORE any MMCV imports
print("ğŸ” å¼€å§‹MMCVç¯å¢ƒéªŒè¯...")

# Step 1: Clear ALL cached imports and registries
mmcv_modules = [k for k in sys.modules.keys() if k.startswith('mmcv')]
mmengine_modules = [k for k in sys.modules.keys() if k.startswith('mmengine')]
for module in mmcv_modules + mmengine_modules:
    if module in sys.modules:
        del sys.modules[module]
print(f"âœ… å·²æ¸…ç† {len(mmcv_modules + mmengine_modules)} ä¸ªç¼“å­˜æ¨¡å—")

# Step 2: Clear all global registries that might conflict
try:
    import gc
    gc.collect()
    if hasattr(__builtins__, '__main__'):
        main_attrs = [attr for attr in dir(__builtins__['__main__']) if 'registry' in attr.lower() or 'transform' in attr.lower()]
        for attr in main_attrs:
            try:
                delattr(__builtins__['__main__'], attr)
            except: pass
    print("âœ… å·²æ¸…ç†å…¨å±€æ³¨å†Œè¡¨")
except: pass

# Step 3: Check MMCV version with isolated import
try:
    version_check_code = '''
import mmcv
mmcv_version = mmcv.__version__
'''
    local_vars = {}
    exec(version_check_code, {}, local_vars)
    mmcv_version = local_vars['mmcv_version']
    
    print(f"ğŸ” æ£€æµ‹åˆ°MMCVç‰ˆæœ¬: {mmcv_version}")
    
    if mmcv_version != "2.1.0":
        print(f"âŒ é”™è¯¯ï¼šæ£€æµ‹åˆ°MMCV {mmcv_version}ï¼Œä½†éœ€è¦mmcv==2.1.0")
        raise RuntimeError(f"MMCVç‰ˆæœ¬ä¸åŒ¹é…ï¼šæœŸæœ›2.1.0ï¼Œå®é™…{mmcv_version}")
    else:
        print(f"âœ… MMCVç‰ˆæœ¬å®Œå…¨åŒ¹é…ï¼š{mmcv_version} == 2.1.0")
        
except ImportError as e:
    print(f"âŒ MMCVå¯¼å…¥å¤±è´¥ï¼š{e}")
    raise RuntimeError("MMCVæœªæ­£ç¡®å®‰è£…")
except Exception as e:
    print(f"âŒ MMCVç‰ˆæœ¬æ£€æŸ¥å¤±è´¥ï¼š{e}")
    raise RuntimeError(f"MMCVç¯å¢ƒéªŒè¯å¤±è´¥ï¼š{e}")

print("âœ… MMCVç¯å¢ƒéªŒè¯é€šè¿‡ï¼Œå¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")

# ğŸ“ Knowledge Distillation Implementation
print("ğŸ“ åˆå§‹åŒ–çŸ¥è¯†è’¸é¦æ¶æ„...")

# Teacher-Student Distillation Model
class KnowledgeDistillationModel(nn.Module):
    """å®Œæ•´çš„å¸ˆç”ŸçŸ¥è¯†è’¸é¦æ¨¡å‹"""
    
    def __init__(self, teacher_cfg, student_cfg, distill_cfg=None):
        super().__init__()
        
        # è’¸é¦é…ç½®
        self.distill_cfg = distill_cfg or {}
        self.alpha = self.distill_cfg.get('alpha', 0.7)  # è’¸é¦æŸå¤±æƒé‡
        self.temperature = self.distill_cfg.get('temperature', 4.0)  # æ¸©åº¦å‚æ•°
        self.feature_loss_weight = self.distill_cfg.get('feature_loss_weight', 0.5)
        
        # æ•™å¸ˆæ¨¡å‹ (DINOv3-based)
        self.teacher_model = self._create_teacher_model()
        self.teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹å§‹ç»ˆå¤„äºè¯„ä¼°æ¨¡å¼
        
        # å­¦ç”Ÿæ¨¡å‹ (SegFormer-B0)
        self.student_model = self._create_student_model()
        
        # ç‰¹å¾å¯¹é½å±‚
        self.feature_adapters = nn.ModuleList([
            nn.Conv2d(32, 768, 1),   # B0 stage0 -> DINOv3 dim
            nn.Conv2d(64, 768, 1),   # B0 stage1 -> DINOv3 dim
            nn.Conv2d(160, 768, 1),  # B0 stage2 -> DINOv3 dim
            nn.Conv2d(256, 768, 1)   # B0 stage3 -> DINOv3 dim
        ])
        
        # æŸå¤±å‡½æ•° - ä¿®å¤åˆ†å‰²ä»»åŠ¡çš„æŸå¤±è®¡ç®—
        self.mse_loss = nn.MSELoss()
        # ä½¿ç”¨ignore_index=255å¤„ç†æ— æ•ˆæ ‡ç­¾ï¼Œreduction='mean'ç¡®ä¿ç¨³å®šè®­ç»ƒ
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=255, reduction='mean')
        self.kl_loss = nn.KLDivLoss(reduction='batchmean')
        
        print("âœ… çŸ¥è¯†è’¸é¦æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
    
    def _create_teacher_model(self):
        """åˆ›å»ºæ•™å¸ˆæ¨¡å‹ (ç®€åŒ–çš„DINOv3)"""
        class TeacherModel(nn.Module):
            def __init__(self):
                super().__init__()
                # ç®€åŒ–çš„ViT backbone
                self.patch_embed = nn.Conv2d(3, 768, kernel_size=16, stride=16)
                self.pos_embed = nn.Parameter(torch.randn(1, 1024, 768) * 0.02)
                self.blocks = nn.ModuleList([
                    nn.TransformerEncoderLayer(768, 12, 3072, dropout=0.0, batch_first=True)
                    for _ in range(12)
                ])
                self.norm = nn.LayerNorm(768)
                
                # åˆ†å‰²å¤´
                self.decode_head = nn.Sequential(
                    nn.ConvTranspose2d(768, 512, 4, 2, 1),
                    nn.BatchNorm2d(512),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(512, 256, 4, 2, 1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(256, 128, 4, 2, 1),
                    nn.BatchNorm2d(128),
                    nn.ReLU(inplace=True),
                    nn.ConvTranspose2d(128, 7, 4, 2, 1)  # 7 classes for LoveDA
                )
            
            def forward(self, x):
                B, C, H, W = x.shape
                
                # Patch embedding
                x = self.patch_embed(x)  # [B, 768, H/16, W/16]
                x = x.flatten(2).transpose(1, 2)  # [B, N, 768]
                
                # Add position embedding
                if x.size(1) <= self.pos_embed.size(1):
                    x = x + self.pos_embed[:, :x.size(1)]
                
                # Transformer blocks
                features = []
                for i, block in enumerate(self.blocks):
                    x = block(x)
                    if i in [2, 5, 8, 11]:  # Multi-scale features
                        feat = x.transpose(1, 2).view(B, 768, int(H/16), int(W/16))
                        features.append(feat)
                
                # Final normalization
                x = self.norm(x)
                x = x.transpose(1, 2).view(B, 768, int(H/16), int(W/16))
                
                # Decode head
                logits = self.decode_head(x)
                
                return logits, features
        
        return TeacherModel()
    
    def _create_student_model(self):
        """åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ (SegFormer-B0)"""
        class StudentModel(nn.Module):
            def __init__(self):
                super().__init__()
                # ç®€åŒ–çš„MixViT backbone
                self.patch_embeds = nn.ModuleList([
                    nn.Conv2d(3, 32, 7, 4, 3),      # Stage 0
                    nn.Conv2d(32, 64, 3, 2, 1),     # Stage 1
                    nn.Conv2d(64, 160, 3, 2, 1),    # Stage 2
                    nn.Conv2d(160, 256, 3, 2, 1)    # Stage 3
                ])
                
                self.norms = nn.ModuleList([
                    nn.LayerNorm(32),
                    nn.LayerNorm(64),
                    nn.LayerNorm(160),
                    nn.LayerNorm(256)
                ])
                
                # ç®€åŒ–çš„æ³¨æ„åŠ›å±‚
                self.attentions = nn.ModuleList([
                    nn.MultiheadAttention(32, 1, batch_first=True),
                    nn.MultiheadAttention(64, 2, batch_first=True),
                    nn.MultiheadAttention(160, 5, batch_first=True),
                    nn.MultiheadAttention(256, 8, batch_first=True)
                ])
                
                # SegFormer decode head
                self.decode_head = nn.Sequential(
                    nn.Conv2d(32+64+160+256, 256, 1),
                    nn.BatchNorm2d(256),
                    nn.ReLU(inplace=True),
                    nn.Dropout2d(0.1),
                    nn.Conv2d(256, 7, 1)  # 7 classes for LoveDA
                )
            
            def forward(self, x):
                B, C, H, W = x.shape
                features = []
                
                # Multi-stage feature extraction
                for i, (patch_embed, norm, attn) in enumerate(zip(self.patch_embeds, self.norms, self.attentions)):
                    x = patch_embed(x)
                    
                    # Reshape for attention
                    B, C, H_new, W_new = x.shape
                    x_flat = x.flatten(2).transpose(1, 2)  # [B, N, C]
                    x_flat = norm(x_flat)
                    
                    # Self-attention
                    x_attn, _ = attn(x_flat, x_flat, x_flat)
                    x = x_attn.transpose(1, 2).view(B, C, H_new, W_new)
                    
                    features.append(x)
                
                # Multi-scale feature fusion
                target_size = features[0].shape[2:]
                upsampled_features = []
                for feat in features:
                    if feat.shape[2:] != target_size:
                        feat = F.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)
                    upsampled_features.append(feat)
                
                # Concatenate and decode
                fused_features = torch.cat(upsampled_features, dim=1)
                logits = self.decode_head(fused_features)
                
                # Upsample to input size
                logits = F.interpolate(logits, size=(H, W), mode='bilinear', align_corners=False)
                
                return logits, features
        
        return StudentModel()
    
    def forward(self, inputs, targets=None, mode='train'):
        """å‰å‘ä¼ æ’­"""
        if mode == 'train' and targets is not None:
            return self.forward_train(inputs, targets)
        else:
            return self.predict(inputs)
    
    def forward_train(self, inputs, targets):
        """è®­ç»ƒæ¨¡å¼å‰å‘ä¼ æ’­"""
        # æ•™å¸ˆæ¨¡å‹æ¨ç† (æ— æ¢¯åº¦)
        with torch.no_grad():
            teacher_logits, teacher_features = self.teacher_model(inputs)
        
        # å­¦ç”Ÿæ¨¡å‹æ¨ç†
        student_logits, student_features = self.student_model(inputs)
        
        # è®¡ç®—æŸå¤±
        losses = {}
        
        # 1. ä»»åŠ¡æŸå¤± (åˆ†å‰²æŸå¤±)
        task_loss = self.ce_loss(student_logits, targets)
        losses['loss_task'] = task_loss
        
        # 2. çŸ¥è¯†è’¸é¦æŸå¤±
        kd_loss = self._compute_kd_loss(teacher_logits, student_logits)
        losses['loss_kd'] = kd_loss
        
        # 3. ç‰¹å¾è’¸é¦æŸå¤±
        feature_loss = self._compute_feature_loss(teacher_features, student_features)
        losses['loss_feature'] = feature_loss
        
        # æ€»æŸå¤±
        total_loss = (1 - self.alpha) * task_loss + self.alpha * kd_loss + self.feature_loss_weight * feature_loss
        losses['loss'] = total_loss
        
        return losses
    
    def predict(self, inputs):
        """é¢„æµ‹æ¨¡å¼"""
        student_logits, _ = self.student_model(inputs)
        return F.softmax(student_logits, dim=1)
    
    def _compute_kd_loss(self, teacher_logits, student_logits):
        """è®¡ç®—çŸ¥è¯†è’¸é¦æŸå¤±"""
        try:
            # ç¡®ä¿logitså½¢çŠ¶åŒ¹é…
            if teacher_logits.shape != student_logits.shape:
                teacher_logits = F.interpolate(teacher_logits, size=student_logits.shape[2:], mode='bilinear', align_corners=False)
            
            # æ¸©åº¦ç¼©æ”¾
            teacher_soft = F.softmax(teacher_logits / self.temperature, dim=1)
            student_log_soft = F.log_softmax(student_logits / self.temperature, dim=1)
            
            # KLæ•£åº¦æŸå¤±
            kd_loss = self.kl_loss(student_log_soft, teacher_soft) * (self.temperature ** 2)
            
            # æ£€æŸ¥æŸå¤±å€¼æœ‰æ•ˆæ€§
            if torch.isnan(kd_loss) or torch.isinf(kd_loss):
                print("âš ï¸ è­¦å‘Šï¼šKDæŸå¤±æ— æ•ˆï¼Œä½¿ç”¨é›¶æŸå¤±")
                return torch.tensor(0.0, device=kd_loss.device, requires_grad=True)
            
            return kd_loss
        except Exception as e:
            print(f"âš ï¸ KDæŸå¤±è®¡ç®—é”™è¯¯: {e}ï¼Œä½¿ç”¨é›¶æŸå¤±")
            return torch.tensor(0.0, device=student_logits.device, requires_grad=True)
    
    def _compute_feature_loss(self, teacher_features, student_features):
        """è®¡ç®—ç‰¹å¾è’¸é¦æŸå¤±"""
        try:
            total_loss = 0.0
            valid_features = 0
            
            for i, (t_feat, s_feat) in enumerate(zip(teacher_features, student_features)):
                if i < len(self.feature_adapters):
                    # ç‰¹å¾å¯¹é½
                    adapted_s_feat = self.feature_adapters[i](s_feat)
                    
                    # ç©ºé—´å°ºå¯¸å¯¹é½
                    if adapted_s_feat.shape[2:] != t_feat.shape[2:]:
                        adapted_s_feat = F.interpolate(
                            adapted_s_feat, 
                            size=t_feat.shape[2:], 
                            mode='bilinear', 
                            align_corners=False
                        )
                    
                    # MSEæŸå¤±
                    loss = self.mse_loss(adapted_s_feat, t_feat.detach())
                    
                    # æ£€æŸ¥ç‰¹å¾æŸå¤±æœ‰æ•ˆæ€§
                    if not (torch.isnan(loss) or torch.isinf(loss)):
                        total_loss += loss
                        valid_features += 1
            
            if valid_features == 0:
                print("âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æœ‰æ•ˆç‰¹å¾æŸå¤±ï¼Œä½¿ç”¨é›¶æŸå¤±")
                device = teacher_features[0].device if teacher_features else student_features[0].device
                return torch.tensor(0.0, device=device, requires_grad=True)
            
            avg_loss = total_loss / valid_features
            
            # æœ€ç»ˆæ£€æŸ¥
            if torch.isnan(avg_loss) or torch.isinf(avg_loss):
                print("âš ï¸ è­¦å‘Šï¼šç‰¹å¾æŸå¤±æ— æ•ˆï¼Œä½¿ç”¨é›¶æŸå¤±")
                device = teacher_features[0].device if teacher_features else student_features[0].device
                return torch.tensor(0.0, device=device, requires_grad=True)
            
            return avg_loss
            
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾æŸå¤±è®¡ç®—é”™è¯¯: {e}ï¼Œä½¿ç”¨é›¶æŸå¤±")
            device = teacher_features[0].device if teacher_features else student_features[0].device
            return torch.tensor(0.0, device=device, requires_grad=True)

# åˆ›å»ºçŸ¥è¯†è’¸é¦æ¨¡å‹
print("ğŸ—ï¸ åˆ›å»ºçŸ¥è¯†è’¸é¦æ¨¡å‹...")
distill_model = KnowledgeDistillationModel(
    teacher_cfg={},
    student_cfg={},
    distill_cfg={
        'alpha': 0.7,
        'temperature': 4.0,
        'feature_loss_weight': 0.5
    }
)

# GPUè®¾ç½®
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
distill_model = distill_model.to(device)
print(f"âœ… æ¨¡å‹å·²ç§»è‡³è®¾å¤‡: {device}")

# ä¼˜åŒ–å™¨è®¾ç½® (åªä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹)
student_params = list(distill_model.student_model.parameters()) + list(distill_model.feature_adapters.parameters())
optimizer = torch.optim.AdamW(student_params, lr=0.00004, weight_decay=0.01)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)

print("âœ… ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨è®¾ç½®å®Œæˆ")

# çœŸå®LoveDAæ•°æ®é›†è®­ç»ƒ
print("ğŸ“Š å¼€å§‹çœŸå®æ•°æ®é›†çŸ¥è¯†è’¸é¦è®­ç»ƒ...")

# åˆ›å»ºçœŸå®æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
try:
    # å¯¼å…¥å¿…è¦çš„æ•°æ®å¤„ç†æ¨¡å—
    import mmcv
    from mmengine.dataset import DefaultSampler
    from torch.utils.data import DataLoader
    
    # ç®€åŒ–çš„æ•°æ®é›†ç±» (å…¼å®¹çœŸå®æ•°æ®)
    class SimpleLoveDADataset:
        def __init__(self, data_root, split='Train'):
            self.data_root = data_root
            self.split = split
            self.img_dir = os.path.join(data_root, split)
            self.samples = self._load_samples()
            
        def _load_samples(self):
            samples = []
            for area in ['Rural', 'Urban']:
                img_path = os.path.join(self.img_dir, area, 'images_png')
                mask_path = os.path.join(self.img_dir, area, 'masks_png')
                if os.path.exists(img_path) and os.path.exists(mask_path):
                    img_files = sorted([f for f in os.listdir(img_path) if f.endswith('.png')])
                    for img_file in img_files:
                        mask_file = img_file  # å‡è®¾maskæ–‡ä»¶åç›¸åŒ
                        if os.path.exists(os.path.join(mask_path, mask_file)):
                            samples.append({
                                'img': os.path.join(img_path, img_file),
                                'mask': os.path.join(mask_path, mask_file)
                            })
            return samples
        
        def __len__(self):
            return len(self.samples)
        
        def __getitem__(self, idx):
             sample = self.samples[idx]
             
             # åŠ è½½å›¾åƒ
             img = Image.open(sample['img']).convert('RGB')
             img = np.array(img).transpose(2, 0, 1).astype(np.float32) / 255.0
             
             # åŠ è½½mask
             mask = Image.open(sample['mask'])
             mask = np.array(mask).astype(np.int64)
             
             # ğŸ”§ å…³é”®ä¿®å¤ï¼šå¤„ç†æ ‡ç­¾å€¼èŒƒå›´é—®é¢˜
             # LoveDAæ•°æ®é›†æ ‡ç­¾å€¼å¯èƒ½åŒ…å«255(å¿½ç•¥å€¼)æˆ–å…¶ä»–æ— æ•ˆå€¼
             # å°†æ‰€æœ‰æ ‡ç­¾å€¼é™åˆ¶åœ¨[0, 6]èŒƒå›´å†…
             mask = np.clip(mask, 0, 6)  # ç¡®ä¿æ ‡ç­¾åœ¨æœ‰æ•ˆèŒƒå›´å†…
             
             # å°†255ç­‰æ— æ•ˆå€¼æ˜ å°„ä¸º0(èƒŒæ™¯ç±»)
             mask[mask > 6] = 0
             
             # è°ƒæ•´å°ºå¯¸åˆ°512x512
             img = torch.from_numpy(img)
             mask = torch.from_numpy(mask)
             
             img = F.interpolate(img.unsqueeze(0), size=(512, 512), mode='bilinear', align_corners=False).squeeze(0)
             mask = F.interpolate(mask.unsqueeze(0).unsqueeze(0).float(), size=(512, 512), mode='nearest').squeeze(0).squeeze(0).long()
             
             # ğŸ”§ äºŒæ¬¡æ£€æŸ¥ï¼šç¡®ä¿resizeåçš„maskä»åœ¨æœ‰æ•ˆèŒƒå›´å†…
             mask = torch.clamp(mask, 0, 6)
             
             return img, mask
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = SimpleLoveDADataset('/kaggle/input/loveda', 'Train')
    train_loader = DataLoader(
        train_dataset,
        batch_size=2,  # é€‚åˆGPUå†…å­˜çš„batch size
        shuffle=True,
        num_workers=2,
        pin_memory=True
    )
    
    print(f"âœ… æˆåŠŸåŠ è½½çœŸå®æ•°æ®é›†ï¼Œè®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
    
except Exception as e:
    print(f"âš ï¸ çœŸå®æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
    print("ğŸ”„ å›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®é›†...")
    
    # å›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®
    class DummyDataset:
        def __init__(self, num_samples=100):
            self.num_samples = num_samples
        
        def __len__(self):
            return self.num_samples
        
        def __getitem__(self, idx):
            img = torch.randn(3, 512, 512)
            mask = torch.randint(0, 7, (512, 512))
            return img, mask
    
    train_dataset = DummyDataset(200)
    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)

# è®­ç»ƒå¾ªç¯
num_epochs = 20  # å¢åŠ è®­ç»ƒè½®æ•°
best_loss = float('inf')

for epoch in range(num_epochs):
    distill_model.train()
    distill_model.teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹å§‹ç»ˆä¸ºè¯„ä¼°æ¨¡å¼
    
    epoch_losses = {'task': 0.0, 'kd': 0.0, 'feature': 0.0, 'total': 0.0}
    num_batches = 0
    
    # çœŸå®æ•°æ®è®­ç»ƒ
     for batch_idx, (inputs, targets) in enumerate(train_loader):
         inputs = inputs.to(device)
         targets = targets.to(device)
         
         # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ ‡ç­¾é¢„å¤„ç†å’ŒéªŒè¯
         # å°†æ‰€æœ‰æ— æ•ˆæ ‡ç­¾(>6æˆ–<0)æ˜ å°„ä¸ºignore_index=255
         invalid_mask = (targets < 0) | (targets > 6)
         targets[invalid_mask] = 255  # ä½¿ç”¨ignore_index
         
         # æ£€æŸ¥å¤„ç†åçš„æ ‡ç­¾
         valid_labels = targets[targets != 255]
         if len(valid_labels) == 0:
             print("âš ï¸ è­¦å‘Šï¼šbatchä¸­æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œè·³è¿‡")
             continue
             
         # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°æ ‡ç­¾ç»Ÿè®¡
         if batch_idx == 0:
             unique_labels = torch.unique(valid_labels)
             print(f"ğŸ“Š æ‰¹æ¬¡æœ‰æ•ˆæ ‡ç­¾èŒƒå›´: {unique_labels.tolist()}")
             total_invalid = invalid_mask.sum().item()
             if total_invalid > 0:
                 print(f"âš ï¸ å¤„ç†äº† {total_invalid} ä¸ªæ— æ•ˆæ ‡ç­¾å€¼")
        
        # å‰å‘ä¼ æ’­ - æ·»åŠ å¼‚å¸¸å¤„ç†
        try:
            losses = distill_model.forward_train(inputs, targets)
            
            # æ£€æŸ¥æŸå¤±å€¼æ˜¯å¦æœ‰æ•ˆ
            if torch.isnan(losses['loss']) or torch.isinf(losses['loss']):
                print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°æ— æ•ˆæŸå¤±å€¼ {losses['loss'].item()}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            losses['loss'].backward()
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰æ•ˆ
            total_norm = 0
            for p in distill_model.parameters():
                if p.grad is not None:
                    param_norm = p.grad.data.norm(2)
                    total_norm += param_norm.item() ** 2
            total_norm = total_norm ** (1. / 2)
            
            if torch.isnan(torch.tensor(total_norm)) or torch.isinf(torch.tensor(total_norm)):
                print(f"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°æ— æ•ˆæ¢¯åº¦ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                continue
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(distill_model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
        except RuntimeError as e:
            if "CUDA error" in str(e) or "assert" in str(e) or "out of range" in str(e):
                print(f"âŒ è¿è¡Œæ—¶é”™è¯¯ï¼š{e}")
                print(f"ğŸ“Š è¾“å…¥å½¢çŠ¶: {inputs.shape}, æ ‡ç­¾å½¢çŠ¶: {targets.shape}")
                valid_targets = targets[targets != 255]
                if len(valid_targets) > 0:
                    print(f"ğŸ“Š æœ‰æ•ˆæ ‡ç­¾èŒƒå›´: [{valid_targets.min().item()}, {valid_targets.max().item()}]")
                    print(f"ğŸ“Š æœ‰æ•ˆæ ‡ç­¾å”¯ä¸€å€¼: {torch.unique(valid_targets).tolist()}")
                else:
                    print(f"ğŸ“Š æ— æœ‰æ•ˆæ ‡ç­¾ï¼Œå…¨éƒ¨æ ‡ç­¾å€¼: {torch.unique(targets).tolist()}")
                # æ¸…ç†GPUå†…å­˜å¹¶è·³è¿‡æ­¤æ‰¹æ¬¡
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                continue
            else:
                raise e
        
        # è®°å½•æŸå¤±
        epoch_losses['task'] += losses['loss_task'].item()
        epoch_losses['kd'] += losses['loss_kd'].item()
        epoch_losses['feature'] += losses['loss_feature'].item()
        epoch_losses['total'] += losses['loss'].item()
        num_batches += 1
        
        if batch_idx % 20 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}: "
                  f"Total={losses['loss'].item():.4f}, "
                  f"Task={losses['loss_task'].item():.4f}, "
                  f"KD={losses['loss_kd'].item():.4f}, "
                  f"Feature={losses['loss_feature'].item():.4f}")
            
            # GPUå†…å­˜ç›‘æ§
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**3
                print(f"    GPUå†…å­˜ä½¿ç”¨: {memory_used:.1f}GB")
    
    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step()
    
    # æ‰“å°epochç»Ÿè®¡
    if num_batches > 0:
        avg_losses = {k: v/num_batches for k, v in epoch_losses.items()}
        print(f"\nğŸ“ˆ Epoch {epoch+1} å¹³å‡æŸå¤±:")
        print(f"   æ€»æŸå¤±: {avg_losses['total']:.4f}")
        print(f"   ä»»åŠ¡æŸå¤±: {avg_losses['task']:.4f}")
        print(f"   è’¸é¦æŸå¤±: {avg_losses['kd']:.4f}")
        print(f"   ç‰¹å¾æŸå¤±: {avg_losses['feature']:.4f}")
        print(f"   å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_losses['total'] < best_loss:
            best_loss = avg_losses['total']
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': distill_model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': best_loss
            }, '/kaggle/working/best_distill_model.pth')
            print(f"   ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {best_loss:.4f})")
    
    # éªŒè¯
    if (epoch + 1) % 5 == 0:
        distill_model.eval()
        with torch.no_grad():
            # ä½¿ç”¨çœŸå®æ•°æ®è¿›è¡ŒéªŒè¯
            val_inputs, val_targets = next(iter(train_loader))
            val_inputs = val_inputs.to(device)
            val_pred = distill_model.predict(val_inputs)
            print(f"ğŸ” éªŒè¯ - é¢„æµ‹å½¢çŠ¶: {val_pred.shape}, é¢„æµ‹èŒƒå›´: [{val_pred.min():.3f}, {val_pred.max():.3f}]")

print("\nğŸ¯ çœŸå®æ•°æ®é›†çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆï¼")
print("\nğŸ“‹ è®­ç»ƒæ€»ç»“:")
print("   âœ… æ•™å¸ˆæ¨¡å‹: ç®€åŒ–DINOv3æ¶æ„ (å†»ç»“å‚æ•°)")
print("   âœ… å­¦ç”Ÿæ¨¡å‹: SegFormer-B0æ¶æ„ (å¯è®­ç»ƒ)")
print("   âœ… è’¸é¦ç­–ç•¥: ç‰¹å¾è’¸é¦ + çŸ¥è¯†è’¸é¦ + ä»»åŠ¡æŸå¤±")
print("   âœ… ç‰¹å¾å¯¹é½: 4å±‚å·ç§¯é€‚é…å™¨")
print("   âœ… æ¸©åº¦å‚æ•°: 4.0")
print("   âœ… è’¸é¦æƒé‡: Î±=0.7")
print("   âœ… ä¼˜åŒ–å™¨: AdamW (ä»…å­¦ç”Ÿæ¨¡å‹å‚æ•°)")
print("   âœ… å­¦ä¹ ç‡è°ƒåº¦: CosineAnnealing")
print("\nğŸš€ è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¸ˆç”ŸçŸ¥è¯†è’¸é¦è®­ç»ƒå®ç°ï¼")
```

## ä½¿ç”¨è¯´æ˜

1. å°†ä¸Šè¿°ä»£ç å¤åˆ¶åˆ°Kaggle notebookçš„ä¸€ä¸ªCellä¸­
2. ç¡®ä¿æ•°æ®é›†è·¯å¾„æ­£ç¡®ï¼š`/kaggle/input/loveda`
3. ç¡®ä¿checkpointè·¯å¾„æ­£ç¡®ï¼š`/kaggle/input/mapsage-stage02-checkpoint-6000/best_mIoU_iter_6000.pth`
4. è¿è¡ŒCellå³å¯å¼€å§‹è®­ç»ƒ

è¿™ä¸ªç»Ÿä¸€ç‰ˆæœ¬é¿å…äº†å¤šä¸ªCellä¹‹é—´çš„çŠ¶æ€å†²çªé—®é¢˜ï¼Œç‰¹åˆ«æ˜¯torch.loadçš„è¡¥ä¸å†²çªã€‚