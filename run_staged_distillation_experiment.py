#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ MapSage V5 - åˆ†é˜¶æ®µçŸ¥è¯†è’¸é¦å®éªŒ

åŸºäºæ·±åº¦åˆ†æçš„æ”¹è¿›ç­–ç•¥ï¼š
1. ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥
2. é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦
3. åŠ¨æ€æ¶æ„åŒ¹é…
4. æ™ºèƒ½æŸå¤±å¹³è¡¡

Author: MapSage Team
Date: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
from datetime import datetime
import json
import os
from collections import defaultdict

# ğŸ¯ åˆ†é˜¶æ®µè’¸é¦é…ç½®
STAGED_DISTILL_CONFIG = {
    'stage1_task_focus': {
        'epochs': 8,
        'task_weight': 1.0,
        'distill_weight': 0.0,
        'feature_weight': 0.0,
        'learning_rate': 0.001,
        'description': 'çº¯ä»»åŠ¡å­¦ä¹ é˜¶æ®µ - å»ºç«‹åŸºç¡€ä»»åŠ¡èƒ½åŠ›'
    },
    'stage2_gentle_distill': {
        'epochs': 12,
        'task_weight': 0.8,
        'distill_weight': 0.15,
        'feature_weight': 0.05,
        'learning_rate': 0.0008,
        'description': 'æ¸©å’Œè’¸é¦é˜¶æ®µ - é€æ­¥å¼•å…¥æ•™å¸ˆçŸ¥è¯†'
    },
    'stage3_balanced_learning': {
        'epochs': 8,
        'task_weight': 0.65,
        'distill_weight': 0.25,
        'feature_weight': 0.1,
        'learning_rate': 0.0005,
        'description': 'å¹³è¡¡å­¦ä¹ é˜¶æ®µ - ä¼˜åŒ–æœ€ç»ˆæ€§èƒ½'
    }
}

# ğŸ§  é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦æ¨¡å—
class SelectiveDistillationModule(nn.Module):
    """åŸºäºæ³¨æ„åŠ›çš„é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦"""
    
    def __init__(self, teacher_dim, student_dim):
        super().__init__()
        
        # ç‰¹å¾ç»´åº¦é€‚é…
        self.feature_adapter = nn.Sequential(
            nn.Linear(teacher_dim, student_dim),
            nn.LayerNorm(student_dim),
            nn.ReLU()
        )
        
        # ä»»åŠ¡ç›¸å…³æ€§è¯„ä¼°ç½‘ç»œ
        self.relevance_gate = nn.Sequential(
            nn.Linear(teacher_dim, teacher_dim // 4),
            nn.ReLU(),
            nn.Linear(teacher_dim // 4, 1),
            nn.Sigmoid()
        )
        
        # çŸ¥è¯†è´¨é‡è¯„ä¼°
        self.quality_assessor = nn.Sequential(
            nn.Linear(student_dim, student_dim // 2),
            nn.ReLU(),
            nn.Linear(student_dim // 2, 1),
            nn.Sigmoid()
        )
    
    def forward(self, teacher_features, student_features, task_loss=None):
        """é€‰æ‹©æ€§çŸ¥è¯†è’¸é¦å‰å‘ä¼ æ’­"""
        
        # è¯„ä¼°æ•™å¸ˆç‰¹å¾çš„ä»»åŠ¡ç›¸å…³æ€§
        relevance_scores = self.relevance_gate(teacher_features)
        
        # é€‰æ‹©æ€§ç‰¹å¾ä¼ é€’
        selected_teacher = teacher_features * relevance_scores
        adapted_teacher = self.feature_adapter(selected_teacher)
        
        # è¯„ä¼°å­¦ç”Ÿç‰¹å¾è´¨é‡
        student_quality = self.quality_assessor(student_features)
        
        # è®¡ç®—é€‰æ‹©æ€§è’¸é¦æŸå¤±
        distill_loss = F.mse_loss(student_features, adapted_teacher)
        
        # è´¨é‡åŠ æƒ
        if task_loss is not None:
            # ä»»åŠ¡æŸå¤±é«˜æ—¶ï¼Œé™ä½è’¸é¦æƒé‡
            quality_weight = torch.exp(-task_loss)
            distill_loss = distill_loss * quality_weight
        
        return {
            'distill_loss': distill_loss,
            'relevance_scores': relevance_scores.mean(),
            'student_quality': student_quality.mean(),
            'adapted_features': adapted_teacher
        }

# ğŸ—ï¸ æ¸è¿›å¼å­¦ç”Ÿæ¨¡å‹
class ProgressiveStudentModel(nn.Module):
    """åŠ¨æ€å¤æ‚åº¦è°ƒæ•´çš„å­¦ç”Ÿæ¨¡å‹"""
    
    def __init__(self, input_dim=128, hidden_dims=[64, 32], output_dim=10):
        super().__init__()
        
        self.input_dim = input_dim
        self.hidden_dims = hidden_dims
        self.output_dim = output_dim
        self.current_depth = 1  # å½“å‰æ¿€æ´»çš„å±‚æ•°
        
        # æ„å»ºæ¸è¿›å¼å±‚ç»“æ„
        self.layers = nn.ModuleList()
        prev_dim = input_dim
        
        for i, hidden_dim in enumerate(hidden_dims):
            layer = nn.Sequential(
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.1)
            )
            self.layers.append(layer)
            prev_dim = hidden_dim
        
        # è¾“å‡ºå±‚ - éœ€è¦æ ¹æ®å½“å‰æ¿€æ´»å±‚è°ƒæ•´è¾“å…¥ç»´åº¦
        self.output_layers = nn.ModuleList([
            nn.Linear(hidden_dims[i], output_dim) for i in range(len(hidden_dims))
        ])
        
        # ç‰¹å¾æå–å™¨ï¼ˆç”¨äºè’¸é¦ï¼‰- åŠ¨æ€ç»´åº¦
        self.feature_extractors = nn.ModuleList([
            nn.Linear(hidden_dims[i], 64) for i in range(len(hidden_dims))
        ])
    
    def expand_model(self, stage):
        """æ ¹æ®è®­ç»ƒé˜¶æ®µæ‰©å±•æ¨¡å‹å¤æ‚åº¦"""
        if stage == 'stage1_task_focus':
            self.current_depth = 1
        elif stage == 'stage2_gentle_distill':
            self.current_depth = min(2, len(self.layers))
        elif stage == 'stage3_balanced_learning':
            self.current_depth = len(self.layers)
        
        print(f"ğŸ“ˆ æ¨¡å‹å¤æ‚åº¦è°ƒæ•´: æ¿€æ´» {self.current_depth}/{len(self.layers)} å±‚")
    
    def forward(self, x, return_features=False):
        """å‰å‘ä¼ æ’­"""
        
        # æ¸è¿›å¼å‰å‘ä¼ æ’­
        layer_outputs = []
        for i in range(self.current_depth):
            x = self.layers[i](x)
            layer_outputs.append(x)
        
        # æå–ç‰¹å¾ï¼ˆç”¨äºè’¸é¦ï¼‰- ä½¿ç”¨å½“å‰æ¿€æ´»å±‚çš„æœ€åä¸€å±‚
        if return_features and layer_outputs:
            current_layer_idx = min(self.current_depth - 1, len(self.feature_extractors) - 1)
            features = self.feature_extractors[current_layer_idx](layer_outputs[-1])
        else:
            features = None
        
        # è¾“å‡ºé¢„æµ‹ - ä½¿ç”¨å¯¹åº”çš„è¾“å‡ºå±‚
        if layer_outputs:
            current_layer_idx = min(self.current_depth - 1, len(self.output_layers) - 1)
            output = self.output_layers[current_layer_idx](layer_outputs[-1])
        else:
            # å¦‚æœæ²¡æœ‰æ¿€æ´»å±‚ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè¾“å‡ºå±‚
            output = self.output_layers[0](x)
        
        if return_features:
            return output, features
        return output

# ğŸ“ æ™ºèƒ½æ•™å¸ˆæ¨¡å‹
class IntelligentTeacherModel(nn.Module):
    """å¢å¼ºç‰ˆæ•™å¸ˆæ¨¡å‹"""
    
    def __init__(self, input_dim=128, hidden_dims=[256, 128, 64], output_dim=10):
        super().__init__()
        
        # æ„å»ºæ•™å¸ˆç½‘ç»œ
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.2)
            ])
            prev_dim = hidden_dim
        
        self.backbone = nn.Sequential(*layers)
        self.output_layer = nn.Linear(prev_dim, output_dim)
        
        # å¤šå±‚ç‰¹å¾æå–
        self.feature_extractors = nn.ModuleDict({
            'shallow': nn.Linear(hidden_dims[0], 64),
            'middle': nn.Linear(hidden_dims[1], 64),
            'deep': nn.Linear(hidden_dims[2], 64)
        })
    
    def forward(self, x, return_features=False):
        """å‰å‘ä¼ æ’­"""
        
        features = {}
        
        # é€å±‚å‰å‘ä¼ æ’­å¹¶æå–ç‰¹å¾
        for i, layer in enumerate(self.backbone):
            x = layer(x)
            
            # åœ¨å…³é”®å±‚æå–ç‰¹å¾
            if i == 3:  # ç¬¬ä¸€ä¸ªéšè—å±‚å
                features['shallow'] = self.feature_extractors['shallow'](x)
            elif i == 7:  # ç¬¬äºŒä¸ªéšè—å±‚å
                features['middle'] = self.feature_extractors['middle'](x)
            elif i == 11:  # ç¬¬ä¸‰ä¸ªéšè—å±‚å
                features['deep'] = self.feature_extractors['deep'](x)
        
        output = self.output_layer(x)
        
        if return_features:
            return output, features
        return output

# ğŸš€ åˆ†é˜¶æ®µè’¸é¦è®­ç»ƒå™¨
class StagedDistillationTrainer:
    """åˆ†é˜¶æ®µçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, config=None):
        self.config = config or STAGED_DISTILL_CONFIG
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.teacher_model = IntelligentTeacherModel().to(self.device)
        self.student_model = ProgressiveStudentModel().to(self.device)
        
        # é€‰æ‹©æ€§è’¸é¦æ¨¡å—
        self.selective_distill = SelectiveDistillationModule(
            teacher_dim=64, student_dim=64
        ).to(self.device)
        
        # è®­ç»ƒå†å²
        self.training_history = defaultdict(list)
        self.stage_results = {}
        
        # é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹
        self._pretrain_teacher()
    
    def _pretrain_teacher(self):
        """é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹"""
        print("ğŸ“ é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
        
        # ç®€å•çš„é¢„è®­ç»ƒè¿‡ç¨‹
        optimizer = torch.optim.Adam(self.teacher_model.parameters(), lr=0.001)
        
        for epoch in range(10):
            # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®
            x = torch.randn(64, 128).to(self.device)
            y = torch.randint(0, 10, (64,)).to(self.device)
            
            optimizer.zero_grad()
            output = self.teacher_model(x)
            loss = F.cross_entropy(output, y)
            loss.backward()
            optimizer.step()
            
            if epoch % 3 == 0:
                print(f"    æ•™å¸ˆé¢„è®­ç»ƒ Epoch {epoch+1}/10, Loss: {loss.item():.4f}")
        
        self.teacher_model.eval()
        print("âœ… æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒå®Œæˆ")
    
    def _create_synthetic_data(self, batch_size=32):
        """åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®"""
        x = torch.randn(batch_size, 128).to(self.device)
        y = torch.randint(0, 10, (batch_size,)).to(self.device)
        return x, y
    
    def train_stage(self, stage_name, stage_config):
        """è®­ç»ƒå•ä¸ªé˜¶æ®µ"""
        print(f"\nğŸš€ å¼€å§‹ {stage_name}: {stage_config['description']}")
        print(f"    è®­ç»ƒè½®æ•°: {stage_config['epochs']}")
        print(f"    æŸå¤±æƒé‡: ä»»åŠ¡={stage_config['task_weight']:.2f}, "
              f"è’¸é¦={stage_config['distill_weight']:.2f}, "
              f"ç‰¹å¾={stage_config['feature_weight']:.2f}")
        
        # è°ƒæ•´å­¦ç”Ÿæ¨¡å‹å¤æ‚åº¦
        self.student_model.expand_model(stage_name)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            list(self.student_model.parameters()) + 
            list(self.selective_distill.parameters()),
            lr=stage_config['learning_rate']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=stage_config['epochs']
        )
        
        stage_losses = []
        
        for epoch in range(stage_config['epochs']):
            epoch_losses = {'total': 0.0, 'task': 0.0, 'distill': 0.0, 'feature': 0.0}
            num_batches = 20  # æ¯ä¸ªepochçš„æ‰¹æ¬¡æ•°
            
            for batch in range(num_batches):
                # ç”Ÿæˆè®­ç»ƒæ•°æ®
                x, y = self._create_synthetic_data()
                
                optimizer.zero_grad()
                
                # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                student_output, student_features = self.student_model(x, return_features=True)
                
                # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
                with torch.no_grad():
                    teacher_output, teacher_features = self.teacher_model(x, return_features=True)
                
                # è®¡ç®—ä»»åŠ¡æŸå¤±
                task_loss = F.cross_entropy(student_output, y)
                
                # è®¡ç®—è’¸é¦æŸå¤±
                distill_loss = torch.tensor(0.0).to(self.device)
                feature_loss = torch.tensor(0.0).to(self.device)
                
                if stage_config['distill_weight'] > 0:
                    # è¾“å‡ºè’¸é¦
                    temperature = 4.0
                    soft_teacher = F.softmax(teacher_output / temperature, dim=1)
                    soft_student = F.log_softmax(student_output / temperature, dim=1)
                    distill_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
                
                if stage_config['feature_weight'] > 0:
                    # é€‰æ‹©æ€§ç‰¹å¾è’¸é¦
                    distill_result = self.selective_distill(
                        teacher_features['deep'], student_features, task_loss
                    )
                    feature_loss = distill_result['distill_loss']
                
                # æ€»æŸå¤±
                total_loss = (
                    stage_config['task_weight'] * task_loss +
                    stage_config['distill_weight'] * distill_loss +
                    stage_config['feature_weight'] * feature_loss
                )
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    list(self.student_model.parameters()) + 
                    list(self.selective_distill.parameters()),
                    max_norm=1.0
                )
                optimizer.step()
                
                # è®°å½•æŸå¤±
                epoch_losses['total'] += float(total_loss.item())
                epoch_losses['task'] += float(task_loss.item())
                epoch_losses['distill'] += float(distill_loss.item())
                epoch_losses['feature'] += float(feature_loss.item())
            
            # å¹³å‡æŸå¤±
            for key in epoch_losses:
                epoch_losses[key] = float(epoch_losses[key]) / num_batches
            
            stage_losses.append(epoch_losses)
            scheduler.step()
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 3 == 0 or epoch == stage_config['epochs'] - 1:
                print(f"    Epoch {epoch+1:2d}/{stage_config['epochs']:2d}: "
                      f"æ€»æŸå¤±={epoch_losses['total']:.4f}, "
                      f"ä»»åŠ¡={epoch_losses['task']:.4f}, "
                      f"è’¸é¦={epoch_losses['distill']:.4f}, "
                      f"ç‰¹å¾={epoch_losses['feature']:.4f}, "
                      f"LR={scheduler.get_last_lr()[0]:.6f}")
        
        # ä¿å­˜é˜¶æ®µç»“æœ
        initial_task_loss = stage_losses[0]['task'] if stage_losses else 0.0
        final_task_loss = stage_losses[-1]['task'] if stage_losses else 0.0
        improvement = initial_task_loss - final_task_loss
        
        self.stage_results[stage_name] = {
            'config': stage_config,
            'losses': stage_losses,
            'final_task_loss': final_task_loss,
            'improvement': improvement
        }
        
        print(f"âœ… {stage_name} å®Œæˆ")
        print(f"    ä»»åŠ¡æŸå¤±æ”¹å–„: {self.stage_results[stage_name]['improvement']:.4f}")
        
        return stage_losses
    
    def run_full_experiment(self):
        """è¿è¡Œå®Œæ•´çš„åˆ†é˜¶æ®µå®éªŒ"""
        print("ğŸ¯ å¼€å§‹åˆ†é˜¶æ®µçŸ¥è¯†è’¸é¦å®éªŒ")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        start_time = time.time()
        
        # ä¾æ¬¡æ‰§è¡Œå„ä¸ªé˜¶æ®µ
        for stage_name, stage_config in self.config.items():
            self.train_stage(stage_name, stage_config)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\nğŸ‰ åˆ†é˜¶æ®µå®éªŒå®Œæˆ! æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        
        # åˆ†æç»“æœ
        self.analyze_results()
        
        return self.stage_results
    
    def analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        print("\nğŸ“Š åˆ†é˜¶æ®µå®éªŒç»“æœåˆ†æ:")
        print("=" * 60)
        
        total_improvement = 0
        initial_loss = None
        final_loss = None
        
        for stage_name, results in self.stage_results.items():
            config = results['config']
            improvement = results['improvement']
            final_task_loss = results['final_task_loss']
            
            if initial_loss is None:
                initial_loss = results['losses'][0]['task']
            final_loss = final_task_loss
            
            print(f"\nğŸ“ˆ {stage_name}:")
            print(f"    æè¿°: {config['description']}")
            print(f"    è®­ç»ƒè½®æ•°: {config['epochs']}")
            print(f"    ä»»åŠ¡æŸå¤±æ”¹å–„: {improvement:.4f} ({improvement/results['losses'][0]['task']*100:.1f}%)")
            print(f"    æœ€ç»ˆä»»åŠ¡æŸå¤±: {final_task_loss:.4f}")
            
            total_improvement += improvement
        
        # æ•´ä½“åˆ†æ
        overall_improvement = (initial_loss - final_loss) / initial_loss * 100
        
        print(f"\nğŸ¯ æ•´ä½“å®éªŒæ•ˆæœ:")
        print(f"    åˆå§‹ä»»åŠ¡æŸå¤±: {initial_loss:.4f}")
        print(f"    æœ€ç»ˆä»»åŠ¡æŸå¤±: {final_loss:.4f}")
        print(f"    æ€»ä½“æ”¹å–„: {total_improvement:.4f} ({overall_improvement:.1f}%)")
        
        # æˆåŠŸè¯„ä¼°
        if overall_improvement > 15:
            print("\nâœ… å®éªŒæˆåŠŸ! åˆ†é˜¶æ®µç­–ç•¥æ˜¾è‘—æ”¹å–„äº†æ€§èƒ½")
            print("    å»ºè®®: åœ¨å®Œæ•´æ•°æ®é›†ä¸ŠéªŒè¯å¹¶è¿›ä¸€æ­¥ä¼˜åŒ–")
        elif overall_improvement > 5:
            print("\nâš ï¸  å®éªŒéƒ¨åˆ†æˆåŠŸï¼Œæœ‰æ”¹å–„ä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´")
            print("    å»ºè®®: è°ƒæ•´å„é˜¶æ®µçš„æŸå¤±æƒé‡å’Œè®­ç»ƒè½®æ•°")
        else:
            print("\nâŒ å®éªŒæ•ˆæœæœ‰é™ï¼Œéœ€è¦é‡æ–°è®¾è®¡ç­–ç•¥")
            print("    å»ºè®®: æ£€æŸ¥æ¨¡å‹æ¶æ„åŒ¹é…åº¦å’Œæ•°æ®è´¨é‡")
        
        # ä¿å­˜ç»“æœ
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_dir = "experiments/staged_distillation"
        os.makedirs(results_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f"{results_dir}/staged_distill_results_{timestamp}.json"
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'timestamp': timestamp,
            'config': self.config,
            'results': {}
        }
        
        for stage_name, results in self.stage_results.items():
            save_data['results'][stage_name] = {
                'config': results['config'],
                'final_task_loss': results['final_task_loss'],
                'improvement': results['improvement'],
                'loss_history': [loss['task'] for loss in results['losses']]
            }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜è‡³: {results_file}")

# ğŸ¯ ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•° - è¿è¡Œåˆ†é˜¶æ®µçŸ¥è¯†è’¸é¦å®éªŒ"""
    print("ğŸ¯ MapSage V5 - åˆ†é˜¶æ®µçŸ¥è¯†è’¸é¦å®éªŒ")
    print("=" * 50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = StagedDistillationTrainer()
    
    # è¿è¡Œå®éªŒ
    results = trainer.run_full_experiment()
    
    print("\nğŸ¯ åˆ†é˜¶æ®µçŸ¥è¯†è’¸é¦å®éªŒå®Œæˆ!")
    return results

if __name__ == "__main__":
    results = main()