#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ MapSage V5 - æ”¹è¿›ç‰ˆçŸ¥è¯†è’¸é¦å®éªŒæ‰§è¡Œè„šæœ¬

åŸºäºå‰æœŸå®éªŒåˆ†æï¼Œè®¾è®¡æ›´ç²¾ç»†ã€æˆåŠŸæ¦‚ç‡æ›´é«˜çš„çŸ¥è¯†è’¸é¦å®éªŒ

æ ¸å¿ƒæ”¹è¿›ç­–ç•¥:
1. æ¸è¿›å¼è®­ç»ƒ (ä»»åŠ¡é¢„è®­ç»ƒ â†’ çŸ¥è¯†è’¸é¦)
2. è‡ªé€‚åº”æŸå¤±æƒé‡å¹³è¡¡
3. å¤šå±‚æ¬¡æ··åˆè’¸é¦ç­–ç•¥
4. è½»é‡åŒ–å¸ˆç”Ÿæ¶æ„åŒ¹é…
5. åŠ¨æ€æ¸©åº¦è°ƒåº¦

Author: MapSage Team
Date: 2024
"""

import os
import sys
import time
import json
import logging
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/Users/barryzhang/myDev3/MapSage_V5')

# å¯¼å…¥æ”¹è¿›çš„è’¸é¦é…ç½®
from configs.train_distill_dinov3_v2_improved import (
    ImprovedKnowledgeDistillationModel,
    improved_distillation_training,
    compare_with_previous_experiment
)

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, Dataset
from torch.utils.tensorboard import SummaryWriter

# ğŸ”§ å®éªŒé…ç½®ç±»
class ExperimentConfig:
    """æ”¹è¿›ç‰ˆå®éªŒé…ç½®ç®¡ç†"""
    
    def __init__(self):
        # åŸºç¡€é…ç½®
        self.project_name = "MapSage_V5_Improved_Distillation"
        self.experiment_name = f"improved_distill_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # è®­ç»ƒé…ç½®
        self.epochs = 25
        self.batch_size = 4
        self.num_classes = 7
        self.input_size = (512, 512)
        
        # ä¼˜åŒ–å™¨é…ç½®
        self.learning_rate = 0.0001
        self.weight_decay = 0.01
        self.momentum = 0.9
        self.betas = (0.9, 0.999)
        
        # è’¸é¦é…ç½® (å…³é”®æ”¹è¿›)
        self.distill_config = {
            'task_weight': 0.6,        # æé«˜ä»»åŠ¡æƒé‡
            'distill_weight': 0.3,     # é™ä½è’¸é¦æƒé‡
            'feature_weight': 0.1,     # é™ä½ç‰¹å¾æƒé‡
            'attention_weight': 0.05,  # æ³¨æ„åŠ›æƒé‡
            'initial_temperature': 6.0,
            'final_temperature': 3.0,
            'warmup_epochs': 5         # æ¸è¿›å¼è®­ç»ƒ
        }
        
        # è°ƒåº¦å™¨é…ç½®
        self.scheduler_config = {
            'type': 'cosine',
            'T_max': self.epochs,
            'eta_min': self.learning_rate * 0.01
        }
        
        # ä¿å­˜é…ç½®
        self.save_interval = 5
        self.log_interval = 10
        self.eval_interval = 5
        
        # è·¯å¾„é…ç½®
        self.output_dir = Path(f'/Users/barryzhang/myDev3/MapSage_V5/experiments/{self.experiment_name}')
        self.checkpoint_dir = self.output_dir / 'checkpoints'
        self.log_dir = self.output_dir / 'logs'
        self.result_dir = self.output_dir / 'results'
        
        # åˆ›å»ºç›®å½•
        for dir_path in [self.output_dir, self.checkpoint_dir, self.log_dir, self.result_dir]:
            dir_path.mkdir(parents=True, exist_ok=True)
    
    def save_config(self):
        """ä¿å­˜å®éªŒé…ç½®"""
        config_dict = {
            'project_name': self.project_name,
            'experiment_name': self.experiment_name,
            'device': self.device,
            'epochs': self.epochs,
            'batch_size': self.batch_size,
            'learning_rate': self.learning_rate,
            'distill_config': self.distill_config,
            'scheduler_config': self.scheduler_config
        }
        
        config_path = self.output_dir / 'experiment_config.json'
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config_dict, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ å®éªŒé…ç½®å·²ä¿å­˜: {config_path}")

# ğŸ¯ è™šæ‹Ÿæ•°æ®é›†ç±» (ç”¨äºå®éªŒæ¼”ç¤º)
class DummySegmentationDataset(Dataset):
    """è™šæ‹Ÿåˆ†å‰²æ•°æ®é›† - ç”¨äºå®éªŒæ¼”ç¤º"""
    
    def __init__(self, num_samples=1000, image_size=(512, 512), num_classes=7):
        self.num_samples = num_samples
        self.image_size = image_size
        self.num_classes = num_classes
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        # ç”Ÿæˆè™šæ‹Ÿå›¾åƒ (æ¨¡æ‹Ÿé¥æ„Ÿå›¾åƒç‰¹å¾)
        image = torch.randn(3, *self.image_size)
        
        # ç”Ÿæˆè™šæ‹Ÿåˆ†å‰²æ ‡ç­¾ (æ¨¡æ‹Ÿåœ°ç‰©åˆ†ç±»)
        mask = torch.randint(0, self.num_classes, self.image_size)
        
        return image, mask

# ğŸ“Š å®éªŒç›‘æ§ç±»
class ExperimentMonitor:
    """å®éªŒç›‘æ§å’Œæ—¥å¿—è®°å½•"""
    
    def __init__(self, config):
        self.config = config
        self.writer = SummaryWriter(log_dir=str(config.log_dir))
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(config.log_dir / 'experiment.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # å®éªŒè®°å½•
        self.epoch_results = []
        self.best_metrics = {
            'best_loss': float('inf'),
            'best_epoch': 0,
            'best_task_loss': float('inf')
        }
    
    def log_epoch(self, epoch, metrics, lr, temperature):
        """è®°å½•æ¯ä¸ªepochçš„ç»“æœ"""
        # æ§åˆ¶å°è¾“å‡º
        self.logger.info(f"\nğŸ“ˆ Epoch {epoch+1}/{self.config.epochs}:")
        self.logger.info(f"    æ€»æŸå¤±: {metrics['total_loss']:.4f}")
        self.logger.info(f"    ä»»åŠ¡æŸå¤±: {metrics['task_loss']:.4f}")
        self.logger.info(f"    è’¸é¦æŸå¤±: {metrics['distill_loss']:.4f}")
        self.logger.info(f"    ç‰¹å¾æŸå¤±: {metrics['feature_loss']:.4f}")
        self.logger.info(f"    æ³¨æ„åŠ›æŸå¤±: {metrics['attention_loss']:.4f}")
        self.logger.info(f"    å­¦ä¹ ç‡: {lr:.6f}")
        self.logger.info(f"    æ¸©åº¦: {temperature:.2f}")
        
        # TensorBoardè®°å½•
        self.writer.add_scalar('Loss/Total', metrics['total_loss'], epoch)
        self.writer.add_scalar('Loss/Task', metrics['task_loss'], epoch)
        self.writer.add_scalar('Loss/Distillation', metrics['distill_loss'], epoch)
        self.writer.add_scalar('Loss/Feature', metrics['feature_loss'], epoch)
        self.writer.add_scalar('Loss/Attention', metrics['attention_loss'], epoch)
        self.writer.add_scalar('Training/LearningRate', lr, epoch)
        self.writer.add_scalar('Training/Temperature', temperature, epoch)
        
        # æ›´æ–°æœ€ä½³æŒ‡æ ‡
        if metrics['total_loss'] < self.best_metrics['best_loss']:
            self.best_metrics['best_loss'] = metrics['total_loss']
            self.best_metrics['best_epoch'] = epoch
            self.best_metrics['best_task_loss'] = metrics['task_loss']
            self.logger.info(f"    ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (æŸå¤±: {metrics['total_loss']:.4f})")
        
        # ä¿å­˜epochç»“æœ
        epoch_result = {
            'epoch': epoch,
            'metrics': metrics,
            'lr': lr,
            'temperature': temperature
        }
        self.epoch_results.append(epoch_result)
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results = {
            'best_metrics': self.best_metrics,
            'epoch_results': self.epoch_results,
            'config': self.config.__dict__
        }
        
        result_path = self.config.result_dir / 'experiment_results.json'
        with open(result_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜: {result_path}")
    
    def close(self):
        """å…³é—­ç›‘æ§"""
        self.writer.close()

# ğŸš€ æ”¹è¿›ç‰ˆè®­ç»ƒå™¨
class ImprovedDistillationTrainer:
    """æ”¹è¿›ç‰ˆçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, config):
        self.config = config
        self.device = config.device
        
        # åˆ›å»ºæ¨¡å‹
        self.model = ImprovedKnowledgeDistillationModel(
            distill_cfg=config.distill_config
        ).to(self.device)
        
        # åˆ›å»ºæ•°æ®é›†
        self.train_dataset = DummySegmentationDataset(
            num_samples=1000,
            image_size=config.input_size,
            num_classes=config.num_classes
        )
        
        self.val_dataset = DummySegmentationDataset(
            num_samples=200,
            image_size=config.input_size,
            num_classes=config.num_classes
        )
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=config.batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            num_workers=2,
            pin_memory=True
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨ (åªä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹)
        student_params = (
            list(self.model.student_model.parameters()) +
            list(self.model.multi_scale_adapters.parameters()) +
            list(self.model.attention_transfer.parameters())
        )
        
        self.optimizer = torch.optim.AdamW(
            student_params,
            lr=config.learning_rate,
            weight_decay=config.weight_decay,
            betas=config.betas
        )
        
        # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            self.optimizer,
            T_max=config.scheduler_config['T_max'],
            eta_min=config.scheduler_config['eta_min']
        )
        
        # åˆ›å»ºç›‘æ§å™¨
        self.monitor = ExperimentMonitor(config)
        
        print(f"âœ… æ”¹è¿›ç‰ˆè®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(self.train_dataset)}")
        print(f"ğŸ“Š éªŒè¯æ ·æœ¬: {len(self.val_dataset)}")
        print(f"ğŸ¯ è®¾å¤‡: {self.device}")
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        self.model.update_temperature(epoch, self.config.epochs)
        
        epoch_metrics = {
            'total_loss': 0,
            'task_loss': 0,
            'distill_loss': 0,
            'feature_loss': 0,
            'attention_loss': 0
        }
        
        num_batches = len(self.train_loader)
        
        for batch_idx, (images, targets) in enumerate(self.train_loader):
            images = images.to(self.device)
            targets = targets.to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(images, targets, epoch)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            outputs['total_loss'].backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                self.model.student_model.parameters(), 
                max_norm=1.0
            )
            
            self.optimizer.step()
            
            # ç´¯ç§¯æŸå¤±
            for key in epoch_metrics:
                if key in outputs:
                    epoch_metrics[key] += outputs[key].item()
            
            # å®šæœŸæ‰“å°è¿›åº¦
            if batch_idx % self.config.log_interval == 0:
                progress = 100. * batch_idx / num_batches
                print(f"\rè®­ç»ƒè¿›åº¦: {progress:.1f}% [{batch_idx}/{num_batches}]", end='')
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in epoch_metrics:
            epoch_metrics[key] /= num_batches
        
        return epoch_metrics
    
    def validate_epoch(self, epoch):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        val_metrics = {
            'total_loss': 0,
            'task_loss': 0,
            'distill_loss': 0,
            'feature_loss': 0,
            'attention_loss': 0
        }
        
        num_batches = len(self.val_loader)
        
        with torch.no_grad():
            for images, targets in self.val_loader:
                images = images.to(self.device)
                targets = targets.to(self.device)
                
                outputs = self.model(images, targets, epoch)
                
                # ç´¯ç§¯æŸå¤±
                for key in val_metrics:
                    if key in outputs:
                        val_metrics[key] += outputs[key].item()
        
        # è®¡ç®—å¹³å‡æŸå¤±
        for key in val_metrics:
            val_metrics[key] /= num_batches
        
        return val_metrics
    
    def save_checkpoint(self, epoch, metrics):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'metrics': metrics,
            'config': self.config.__dict__
        }
        
        checkpoint_path = self.config.checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pth'
        torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if metrics['total_loss'] == self.monitor.best_metrics['best_loss']:
            best_path = self.config.checkpoint_dir / 'best_model.pth'
            torch.save(checkpoint, best_path)
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"\nğŸš€ å¼€å§‹æ”¹è¿›ç‰ˆçŸ¥è¯†è’¸é¦è®­ç»ƒ...")
        print(f"ğŸ“Š å®éªŒé…ç½®: {self.config.experiment_name}")
        
        start_time = time.time()
        
        for epoch in range(self.config.epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            current_lr = self.scheduler.get_last_lr()[0]
            current_temp = self.model.current_temperature
            
            # éªŒè¯
            if (epoch + 1) % self.config.eval_interval == 0:
                val_metrics = self.validate_epoch(epoch)
                print(f"\nğŸ” éªŒè¯ç»“æœ - æ€»æŸå¤±: {val_metrics['total_loss']:.4f}")
            
            # è®°å½•ç»“æœ
            self.monitor.log_epoch(epoch, train_metrics, current_lr, current_temp)
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.save_interval == 0:
                self.save_checkpoint(epoch, train_metrics)
            
            epoch_time = time.time() - epoch_start
            print(f"\nâ±ï¸  Epochç”¨æ—¶: {epoch_time:.2f}s")
        
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æ€»ç”¨æ—¶: {total_time/3600:.2f}å°æ—¶")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        self.monitor.save_results()
        self.monitor.close()
        
        return self.monitor.best_metrics

# ğŸ“ˆ å®éªŒåˆ†æå™¨
class ExperimentAnalyzer:
    """å®éªŒç»“æœåˆ†æå™¨"""
    
    def __init__(self, config):
        self.config = config
    
    def analyze_training_progress(self, results):
        """åˆ†æè®­ç»ƒè¿›åº¦"""
        print("\nğŸ“ˆ è®­ç»ƒè¿›åº¦åˆ†æ:")
        
        epoch_results = results['epoch_results']
        
        # æŸå¤±è¶‹åŠ¿åˆ†æ
        total_losses = [r['metrics']['total_loss'] for r in epoch_results]
        task_losses = [r['metrics']['task_loss'] for r in epoch_results]
        distill_losses = [r['metrics']['distill_loss'] for r in epoch_results]
        
        print(f"    ğŸ“Š æ€»æŸå¤±å˜åŒ–: {total_losses[0]:.4f} â†’ {total_losses[-1]:.4f}")
        print(f"    ğŸ“Š ä»»åŠ¡æŸå¤±å˜åŒ–: {task_losses[0]:.4f} â†’ {task_losses[-1]:.4f}")
        print(f"    ğŸ“Š è’¸é¦æŸå¤±å˜åŒ–: {distill_losses[0]:.4f} â†’ {distill_losses[-1]:.4f}")
        
        # æ”¶æ•›æ€§åˆ†æ
        if len(total_losses) >= 10:
            recent_losses = total_losses[-10:]
            loss_std = np.std(recent_losses)
            print(f"    ğŸ“Š æœ€è¿‘10è½®æŸå¤±æ ‡å‡†å·®: {loss_std:.4f} (è¶Šå°è¶Šç¨³å®š)")
        
        # ä»»åŠ¡æ€§èƒ½åˆ†æ
        task_improvement = (task_losses[0] - task_losses[-1]) / task_losses[0] * 100
        print(f"    ğŸ“Š ä»»åŠ¡æŸå¤±æ”¹å–„: {task_improvement:.2f}%")
        
        if task_improvement > 5:
            print("    âœ… ä»»åŠ¡æ€§èƒ½æ˜¾è‘—æå‡!")
        elif task_improvement > 1:
            print("    âš ï¸  ä»»åŠ¡æ€§èƒ½è½»å¾®æå‡")
        else:
            print("    âŒ ä»»åŠ¡æ€§èƒ½æœªæ˜æ˜¾æ”¹å–„")
    
    def compare_with_baseline(self, results):
        """ä¸åŸºçº¿å®éªŒå¯¹æ¯”"""
        print("\nğŸ” ä¸å‰æœŸå®éªŒå¯¹æ¯”:")
        
        # å‰æœŸå®éªŒç»“æœ (å‚è€ƒå€¼)
        baseline_results = {
            'final_total_loss': 53.1705,
            'final_task_loss': 1.9589,
            'final_distill_loss': 75.0790,
            'task_improvement': 0.05  # å‡ ä¹æ— æ”¹å–„
        }
        
        # å½“å‰å®éªŒç»“æœ
        current_results = results['epoch_results'][-1]['metrics']
        
        print(f"    ğŸ“Š æ€»æŸå¤±å¯¹æ¯”:")
        print(f"        å‰æœŸ: {baseline_results['final_total_loss']:.4f}")
        print(f"        å½“å‰: {current_results['total_loss']:.4f}")
        
        print(f"    ğŸ“Š ä»»åŠ¡æŸå¤±å¯¹æ¯”:")
        print(f"        å‰æœŸ: {baseline_results['final_task_loss']:.4f}")
        print(f"        å½“å‰: {current_results['task_loss']:.4f}")
        
        # æ”¹å–„è¯„ä¼°
        total_improvement = (baseline_results['final_total_loss'] - current_results['total_loss']) / baseline_results['final_total_loss'] * 100
        task_improvement = (baseline_results['final_task_loss'] - current_results['task_loss']) / baseline_results['final_task_loss'] * 100
        
        print(f"    ğŸ“ˆ æ€»æŸå¤±æ”¹å–„: {total_improvement:.2f}%")
        print(f"    ğŸ“ˆ ä»»åŠ¡æŸå¤±æ”¹å–„: {task_improvement:.2f}%")
        
        if task_improvement > 10:
            print("    ğŸ‰ å®éªŒæ”¹è¿›éå¸¸æˆåŠŸ!")
        elif task_improvement > 5:
            print("    âœ… å®éªŒæ”¹è¿›æˆåŠŸ!")
        elif task_improvement > 0:
            print("    âš ï¸  å®éªŒæœ‰è½»å¾®æ”¹è¿›")
        else:
            print("    âŒ å®éªŒæ”¹è¿›æ•ˆæœä¸æ˜æ˜¾")
    
    def generate_recommendations(self, results):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        print("\nğŸ’¡ åç»­æ”¹è¿›å»ºè®®:")
        
        epoch_results = results['epoch_results']
        final_metrics = epoch_results[-1]['metrics']
        
        # åŸºäºç»“æœçš„å»ºè®®
        if final_metrics['task_loss'] > 1.5:
            print("    ğŸ¯ ä»»åŠ¡æŸå¤±ä»è¾ƒé«˜ï¼Œå»ºè®®:")
            print("        - è¿›ä¸€æ­¥æé«˜ä»»åŠ¡æƒé‡ (0.6 â†’ 0.7)")
            print("        - å»¶é•¿é¢„è®­ç»ƒé˜¶æ®µ (5 â†’ 8 epochs)")
            print("        - è°ƒæ•´å­¦ä¹ ç‡ç­–ç•¥")
        
        if final_metrics['distill_loss'] > 50:
            print("    ğŸ¯ è’¸é¦æŸå¤±è¾ƒé«˜ï¼Œå»ºè®®:")
            print("        - è°ƒæ•´æ¸©åº¦å‚æ•°èŒƒå›´")
            print("        - ä¼˜åŒ–ç‰¹å¾å¯¹é½ç­–ç•¥")
            print("        - è€ƒè™‘æ›´åŒ¹é…çš„å¸ˆç”Ÿæ¶æ„")
        
        print("    ğŸš€ é€šç”¨æ”¹è¿›æ–¹å‘:")
        print("        - å¼•å…¥çœŸå®æ•°æ®é›†éªŒè¯")
        print("        - æ·»åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡ (mIoU, Accuracy)")
        print("        - å®æ–½æ—©åœç­–ç•¥")
        print("        - å°è¯•ä¸åŒçš„è’¸é¦ç­–ç•¥ç»„åˆ")

# ğŸ¯ ä¸»å®éªŒå‡½æ•°
def run_improved_distillation_experiment():
    """è¿è¡Œæ”¹è¿›ç‰ˆçŸ¥è¯†è’¸é¦å®éªŒ"""
    print("ğŸ¯ MapSage V5 - æ”¹è¿›ç‰ˆçŸ¥è¯†è’¸é¦å®éªŒ")
    print("=" * 80)
    
    # åˆ›å»ºå®éªŒé…ç½®
    config = ExperimentConfig()
    config.save_config()
    
    # å®éªŒå‰åˆ†æ
    print("\nğŸ“‹ å®éªŒå‰åˆ†æ:")
    compare_with_previous_experiment()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ImprovedDistillationTrainer(config)
    
    # å¼€å§‹è®­ç»ƒ
    best_metrics = trainer.train()
    
    # åŠ è½½ç»“æœè¿›è¡Œåˆ†æ
    result_path = config.result_dir / 'experiment_results.json'
    with open(result_path, 'r', encoding='utf-8') as f:
        results = json.load(f)
    
    # ç»“æœåˆ†æ
    analyzer = ExperimentAnalyzer(config)
    analyzer.analyze_training_progress(results)
    analyzer.compare_with_baseline(results)
    analyzer.generate_recommendations(results)
    
    print(f"\nğŸ‰ æ”¹è¿›ç‰ˆçŸ¥è¯†è’¸é¦å®éªŒå®Œæˆ!")
    print(f"ğŸ“Š æœ€ä½³æŸå¤±: {best_metrics['best_loss']:.4f}")
    print(f"ğŸ“ å®éªŒç»“æœä¿å­˜åœ¨: {config.output_dir}")
    
    return results, config

if __name__ == "__main__":
    # è¿è¡Œå®éªŒ
    results, config = run_improved_distillation_experiment()