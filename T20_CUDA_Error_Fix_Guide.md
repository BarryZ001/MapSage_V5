# T20æœåŠ¡å™¨CUDAè®¾å¤‡é”™è¯¯ä¿®å¤æŒ‡å—

## ğŸš¨ é—®é¢˜æè¿°

åœ¨T20æœåŠ¡å™¨ä¸Šè¿è¡Œ8å¡åˆ†å¸ƒå¼è®­ç»ƒæ—¶ï¼Œå‡ºç°ä»¥ä¸‹é”™è¯¯ï¼š

```
RuntimeError: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx
```

é”™è¯¯å‘ç”Ÿåœ¨ï¼š
```python
File "/usr/local/lib/python3.8/dist-packages/mmengine/dist/utils.py", line 130, in _init_dist_pytorch
    torch.cuda.set_device(local_rank)
```

## ğŸ” é—®é¢˜åˆ†æ

### æ ¹æœ¬åŸå› 
1. **MMEngineçš„åˆ†å¸ƒå¼åˆå§‹åŒ–é—®é¢˜**: MMEngineçš„`init_dist`å‡½æ•°ä¼šè°ƒç”¨`torch.cuda.set_device(local_rank)`
2. **CUDAä¸GCUå†²çª**: T20æœåŠ¡å™¨ä½¿ç”¨GCUè®¾å¤‡ï¼Œæ²¡æœ‰NVIDIA GPUï¼Œä½†MMEngineä»å°è¯•åˆå§‹åŒ–CUDA
3. **åç«¯é…ç½®ä¸å½“**: è™½ç„¶é…ç½®äº†`backend='gloo'`ï¼Œä½†MMEngineå†…éƒ¨ä»ä¼šæ‰§è¡ŒCUDAç›¸å…³ä»£ç 

### é”™è¯¯è°ƒç”¨é“¾
```
init_dist() -> _init_dist_pytorch() -> torch.cuda.set_device() -> RuntimeError
```

## âœ… è§£å†³æ–¹æ¡ˆ

### 1. ä¿®æ”¹è®­ç»ƒè„šæœ¬ (scripts/train_distributed_8card_gcu.py)

**åŸä»£ç **:
```python
# 2. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (è®©MMEngineæŒ‰æ ‡å‡†æ–¹å¼åˆå§‹åŒ–)
if cfg.get('launcher', 'none') == 'pytorch':
    from mmengine.dist import init_dist
    init_dist(launcher='pytorch', backend=cfg.env_cfg.dist_cfg.get('backend', 'eccl'))
    print("ğŸ”§ MMEngineåˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
```

**ä¿®å¤å**:
```python
# 2. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (ç»•è¿‡MMEngineçš„CUDAè°ƒç”¨ï¼Œç›´æ¥ä½¿ç”¨torch.distributed)
if cfg.get('launcher', 'none') == 'pytorch':
    # è·å–åˆ†å¸ƒå¼å‚æ•°
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
    os.environ['MASTER_ADDR'] = os.environ.get('MASTER_ADDR', '127.0.0.1')
    os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT', '29500')
    
    # ç›´æ¥ä½¿ç”¨torch.distributedåˆå§‹åŒ–ï¼Œé¿å…MMEngineçš„CUDAè°ƒç”¨
    if not dist.is_initialized():
        dist.init_process_group(
            backend='gloo',  # ä½¿ç”¨glooåç«¯ï¼Œå…¼å®¹GCU
            rank=rank,
            world_size=world_size,
            init_method=f"tcp://{os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}"
        )
        print(f"ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank {rank}/{world_size}, Backend: {dist.get_backend()}")
    else:
        print("ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ–")
```

### 2. ä¿®æ”¹é…ç½®æ–‡ä»¶ (configs/train_dinov3_mmrs1m_t20_gcu_8card.py)

ç¡®ä¿ä½¿ç”¨glooåç«¯ï¼š
```python
env_cfg = dict(
    cudnn_benchmark=False,  # GCUç¯å¢ƒä¸‹ç¦ç”¨cudnn
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='gloo'),  # ä½¿ç”¨glooåç«¯æ”¯æŒGCUåˆ†å¸ƒå¼è®­ç»ƒï¼Œé¿å…CUDAè°ƒç”¨
    resource_limit=4096
)
```

## ğŸ”§ å…³é”®ä¿®å¤ç‚¹

1. **ç»•è¿‡MMEngineçš„init_dist**: ç›´æ¥ä½¿ç”¨`torch.distributed.init_process_group`
2. **ä½¿ç”¨glooåç«¯**: é¿å…NCCLå’ŒCUDAç›¸å…³è°ƒç”¨
3. **ç¯å¢ƒå˜é‡ç®¡ç†**: æ­£ç¡®è®¾ç½®MASTER_ADDRå’ŒMASTER_PORT
4. **åˆ†å¸ƒå¼å‚æ•°è·å–**: ä»ç¯å¢ƒå˜é‡ä¸­è·å–rankã€local_rankã€world_size

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. æ›´æ–°ä»£ç 
```bash
cd /workspace/code/MapSage_V5
git pull origin main
```

### 2. é‡æ–°å¯åŠ¨è®­ç»ƒ
```bash
./start_8card_training_correct.sh
```

### 3. éªŒè¯ä¿®å¤
æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦å‡ºç°ï¼š
```
ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank 0/8, Backend: gloo
ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank 1/8, Backend: gloo
...
ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank 7/8, Backend: gloo
```

## ğŸ“Š é¢„æœŸç»“æœ

ä¿®å¤ååº”è¯¥çœ‹åˆ°ï¼š
- âœ… æ‰€æœ‰8ä¸ªè¿›ç¨‹æˆåŠŸåˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
- âœ… ä½¿ç”¨glooåç«¯ï¼Œé¿å…CUDAè°ƒç”¨
- âœ… æ¯ä¸ªè¿›ç¨‹æ­£ç¡®è®¾ç½®ä¸ºå¯¹åº”çš„GCUè®¾å¤‡
- âœ… è®­ç»ƒæ­£å¸¸å¼€å§‹ï¼Œæ— NVIDIAé©±åŠ¨é”™è¯¯

## ğŸ” æ•…éšœæ’é™¤

### å¦‚æœä»æœ‰é—®é¢˜

1. **æ£€æŸ¥ç¯å¢ƒå˜é‡**:
```bash
echo $WORLD_SIZE
echo $MASTER_ADDR  
echo $MASTER_PORT
```

2. **æ£€æŸ¥GCUè®¾å¤‡**:
```bash
python3 -c "import torch_gcu; print(torch_gcu.device_count())"
```

3. **æ£€æŸ¥è¿›ç¨‹çŠ¶æ€**:
```bash
ps aux | grep train_distributed_8card_gcu.py
```

4. **æŸ¥çœ‹è¯¦ç»†æ—¥å¿—**:
```bash
tail -f work_dirs/dinov3_mmrs1m_t20_gcu_8card_correct/logs/train.log
```

## ğŸ“ æŠ€æœ¯è¯´æ˜

### ä¸ºä»€ä¹ˆç»•è¿‡MMEngineï¼Ÿ
- MMEngineçš„`init_dist`å‡½æ•°åœ¨PyTorchåç«¯ä¸‹ä¼šå¼ºåˆ¶è°ƒç”¨`torch.cuda.set_device`
- è¿™ä¸ªè°ƒç”¨åœ¨æ²¡æœ‰NVIDIA GPUçš„ç¯å¢ƒä¸­ä¼šå¤±è´¥
- ç›´æ¥ä½¿ç”¨`torch.distributed.init_process_group`å¯ä»¥é¿å…è¿™ä¸ªé—®é¢˜

### ä¸ºä»€ä¹ˆä½¿ç”¨glooåç«¯ï¼Ÿ
- glooåç«¯æ˜¯CPU-basedï¼Œä¸ä¾èµ–ç‰¹å®šç¡¬ä»¶
- å…¼å®¹GCUè®¾å¤‡çš„åˆ†å¸ƒå¼è®­ç»ƒ
- é¿å…NCCLåç«¯å¯¹NVIDIA GPUçš„ä¾èµ–

---

**ä¿®å¤æ—¶é—´**: 2025-09-21  
**é€‚ç”¨ç¯å¢ƒ**: ç‡§åŸT20 GCU Ã— 8å¡  
**çŠ¶æ€**: å·²ä¿®å¤å¹¶æµ‹è¯•