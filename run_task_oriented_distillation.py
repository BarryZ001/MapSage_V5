#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ MapSage V5 - ä»»åŠ¡å¯¼å‘çŸ¥è¯†è’¸é¦å®éªŒ

åŸºäºç»¼åˆåˆ†æçš„æœ€ç»ˆæ”¹è¿›æ–¹æ¡ˆï¼š
1. ä»»åŠ¡å¯¼å‘çš„çŸ¥è¯†é€‰æ‹©æ€§ä¼ é€’
2. å¯¹æŠ—å¼çŸ¥è¯†è´¨é‡éªŒè¯
3. åŠ¨æ€è’¸é¦ç­–ç•¥è°ƒæ•´
4. å¤šç»´åº¦æ€§èƒ½è¯„ä¼°

Author: MapSage Team
Date: 2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import time
from datetime import datetime
import json
import os
from collections import defaultdict

# ğŸ¯ ä»»åŠ¡å¯¼å‘è’¸é¦é…ç½®
TASK_ORIENTED_CONFIG = {
    'model': {
        'teacher_dim': 256,
        'student_dim': 128,
        'hidden_dim': 64,
        'num_classes': 10
    },
    'training': {
        'total_epochs': 30,
        'warmup_epochs': 8,
        'learning_rate': 0.001,
        'batch_size': 32,
        'num_batches': 25
    },
    'distillation': {
        'initial_task_weight': 0.8,
        'final_task_weight': 0.6,
        'initial_distill_weight': 0.1,
        'final_distill_weight': 0.3,
        'adversarial_weight': 0.1,
        'relevance_threshold': 0.6,
        'quality_threshold': 0.7
    }
}

# ğŸ§  ä»»åŠ¡å¯¼å‘çŸ¥è¯†è¿‡æ»¤å™¨
class TaskOrientedKnowledgeFilter(nn.Module):
    """ä»»åŠ¡å¯¼å‘çš„çŸ¥è¯†é€‰æ‹©æ€§ä¼ é€’æ¨¡å—"""
    
    def __init__(self, teacher_dim, student_dim, hidden_dim=64):
        super().__init__()
        
        self.teacher_dim = teacher_dim
        self.student_dim = student_dim
        
        # ä»»åŠ¡ç›¸å…³æ€§è¯„ä¼°ç½‘ç»œ
        self.relevance_assessor = nn.Sequential(
            nn.Linear(teacher_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # çŸ¥è¯†è´¨é‡è¯„ä¼°ç½‘ç»œ
        self.quality_assessor = nn.Sequential(
            nn.Linear(teacher_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()
        )
        
        # ç‰¹å¾é€‚é…å™¨
        self.feature_adapter = nn.Sequential(
            nn.Linear(teacher_dim, student_dim),
            nn.LayerNorm(student_dim),
            nn.ReLU()
        )
        
        # çŸ¥è¯†èåˆç½‘ç»œ
        self.knowledge_fusion = nn.Sequential(
            nn.Linear(student_dim * 2, student_dim),
            nn.ReLU(),
            nn.Linear(student_dim, student_dim)
        )
    
    def forward(self, teacher_features, student_features, task_loss=None):
        """ä»»åŠ¡å¯¼å‘çš„çŸ¥è¯†è¿‡æ»¤å’Œä¼ é€’"""
        
        batch_size = teacher_features.size(0)
        
        # è¯„ä¼°æ•™å¸ˆçŸ¥è¯†çš„ä»»åŠ¡ç›¸å…³æ€§
        relevance_scores = self.relevance_assessor(teacher_features)
        quality_scores = self.quality_assessor(teacher_features)
        
        # è®¡ç®—ç»¼åˆçŸ¥è¯†ä»·å€¼åˆ†æ•°
        knowledge_value = relevance_scores * quality_scores
        
        # å¦‚æœæœ‰ä»»åŠ¡æŸå¤±ä¿¡æ¯ï¼Œè°ƒæ•´çŸ¥è¯†ä»·å€¼
        if task_loss is not None:
            # ä»»åŠ¡æŸå¤±é«˜æ—¶ï¼Œæé«˜çŸ¥è¯†é€‰æ‹©çš„ä¸¥æ ¼ç¨‹åº¦
            task_difficulty = torch.sigmoid(task_loss)
            knowledge_threshold = 0.5 + 0.3 * task_difficulty
            knowledge_mask = (knowledge_value > knowledge_threshold).float()
        else:
            knowledge_mask = (knowledge_value > 0.6).float()
        
        # é€‰æ‹©æ€§çŸ¥è¯†ä¼ é€’
        filtered_teacher = teacher_features * knowledge_mask
        adapted_teacher = self.feature_adapter(filtered_teacher)
        
        # çŸ¥è¯†èåˆ
        fused_features = torch.cat([student_features, adapted_teacher], dim=1)
        enhanced_student = self.knowledge_fusion(fused_features)
        
        # è®¡ç®—è’¸é¦æŸå¤±
        distill_loss = F.mse_loss(enhanced_student, adapted_teacher)
        
        return {
            'distill_loss': distill_loss,
            'enhanced_features': enhanced_student,
            'relevance_scores': relevance_scores.mean(),
            'quality_scores': quality_scores.mean(),
            'knowledge_utilization': knowledge_mask.mean(),
            'knowledge_value': knowledge_value.mean()
        }

# ğŸ›¡ï¸ å¯¹æŠ—å¼çŸ¥è¯†éªŒè¯å™¨
class AdversarialKnowledgeValidator(nn.Module):
    """å¯¹æŠ—å¼çŸ¥è¯†è´¨é‡éªŒè¯æ¨¡å—"""
    
    def __init__(self, feature_dim, hidden_dim=64):
        super().__init__()
        
        # çŸ¥è¯†è´¨é‡åˆ¤åˆ«å™¨
        self.quality_discriminator = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1),
            nn.Sigmoid()
        )
        
        # ä»»åŠ¡æ€§èƒ½é¢„æµ‹å™¨
        self.performance_predictor = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, student_features, task_loss):
        """å¯¹æŠ—å¼çŸ¥è¯†è´¨é‡éªŒè¯"""
        
        batch_size = student_features.size(0)
        
        # è¯„ä¼°å­¦ç”Ÿç‰¹å¾è´¨é‡
        feature_quality = self.quality_discriminator(student_features)
        
        # é¢„æµ‹ä»»åŠ¡æ€§èƒ½ï¼ˆæŸå¤±è¶Šä½ï¼Œæ€§èƒ½è¶Šå¥½ï¼‰
        predicted_performance = self.performance_predictor(student_features)
        
        # å°†æ ‡é‡ä»»åŠ¡æŸå¤±æ‰©å±•ä¸ºæ‰¹æ¬¡ç»´åº¦
        if task_loss.dim() == 0:  # æ ‡é‡
            actual_performance = torch.full((batch_size,), 1.0 / (1.0 + task_loss.item()), device=student_features.device)
        else:
            actual_performance = 1.0 / (1.0 + task_loss)
        
        # æ€§èƒ½é¢„æµ‹æŸå¤±
        performance_loss = F.mse_loss(predicted_performance.squeeze(), actual_performance)
        
        # å¯¹æŠ—æŸå¤±ï¼šç‰¹å¾è´¨é‡åº”è¯¥ä¸å®é™…ä»»åŠ¡æ€§èƒ½ä¸€è‡´
        target_quality = (actual_performance > 0.5).float()
        adversarial_loss = F.binary_cross_entropy(feature_quality.squeeze(), target_quality)
        
        return {
            'performance_loss': performance_loss,
            'adversarial_loss': adversarial_loss,
            'feature_quality': feature_quality.mean(),
            'predicted_performance': predicted_performance.mean()
        }

# ğŸ“ æ™ºèƒ½æ•™å¸ˆæ¨¡å‹
class IntelligentTeacher(nn.Module):
    """å¢å¼ºç‰ˆæ•™å¸ˆæ¨¡å‹"""
    
    def __init__(self, input_dim=128, teacher_dim=256, num_classes=10):
        super().__init__()
        
        self.backbone = nn.Sequential(
            nn.Linear(input_dim, teacher_dim),
            nn.BatchNorm1d(teacher_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(teacher_dim, teacher_dim),
            nn.BatchNorm1d(teacher_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            nn.Linear(teacher_dim, teacher_dim // 2),
            nn.BatchNorm1d(teacher_dim // 2),
            nn.ReLU()
        )
        
        self.feature_extractor = nn.Linear(teacher_dim // 2, teacher_dim)
        self.classifier = nn.Linear(teacher_dim // 2, num_classes)
    
    def forward(self, x, return_features=False):
        features = self.backbone(x)
        extracted_features = self.feature_extractor(features)
        output = self.classifier(features)
        
        if return_features:
            return output, extracted_features
        return output

# ğŸ¯ ä»»åŠ¡å¯¼å‘å­¦ç”Ÿæ¨¡å‹
class TaskOrientedStudent(nn.Module):
    """ä»»åŠ¡å¯¼å‘çš„å­¦ç”Ÿæ¨¡å‹"""
    
    def __init__(self, input_dim=128, student_dim=128, num_classes=10):
        super().__init__()
        
        self.backbone = nn.Sequential(
            nn.Linear(input_dim, student_dim),
            nn.BatchNorm1d(student_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            nn.Linear(student_dim, student_dim // 2),
            nn.BatchNorm1d(student_dim // 2),
            nn.ReLU()
        )
        
        self.feature_extractor = nn.Linear(student_dim // 2, student_dim)
        self.classifier = nn.Linear(student_dim // 2, num_classes)
        
        # ä»»åŠ¡é€‚åº”å±‚
        self.task_adapter = nn.Sequential(
            nn.Linear(student_dim, student_dim),
            nn.ReLU(),
            nn.Linear(student_dim, student_dim)
        )
    
    def forward(self, x, return_features=False):
        features = self.backbone(x)
        extracted_features = self.feature_extractor(features)
        
        # ä»»åŠ¡é€‚åº”
        adapted_features = self.task_adapter(extracted_features)
        
        output = self.classifier(features)
        
        if return_features:
            return output, adapted_features
        return output

# ğŸš€ ä»»åŠ¡å¯¼å‘è’¸é¦è®­ç»ƒå™¨
class TaskOrientedDistillationTrainer:
    """ä»»åŠ¡å¯¼å‘çŸ¥è¯†è’¸é¦è®­ç»ƒå™¨"""
    
    def __init__(self, config=None):
        self.config = config or TASK_ORIENTED_CONFIG
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.teacher = IntelligentTeacher(
            teacher_dim=self.config['model']['teacher_dim'],
            num_classes=self.config['model']['num_classes']
        ).to(self.device)
        
        self.student = TaskOrientedStudent(
            student_dim=self.config['model']['student_dim'],
            num_classes=self.config['model']['num_classes']
        ).to(self.device)
        
        # çŸ¥è¯†è¿‡æ»¤å™¨
        self.knowledge_filter = TaskOrientedKnowledgeFilter(
            teacher_dim=self.config['model']['teacher_dim'],
            student_dim=self.config['model']['student_dim'],
            hidden_dim=self.config['model']['hidden_dim']
        ).to(self.device)
        
        # å¯¹æŠ—å¼éªŒè¯å™¨
        self.adversarial_validator = AdversarialKnowledgeValidator(
            feature_dim=self.config['model']['student_dim'],
            hidden_dim=self.config['model']['hidden_dim']
        ).to(self.device)
        
        # è®­ç»ƒå†å²
        self.training_history = defaultdict(list)
        
        # é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹
        self._pretrain_teacher()
    
    def _pretrain_teacher(self):
        """é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹"""
        print("ğŸ“ é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹...")
        
        optimizer = torch.optim.Adam(self.teacher.parameters(), lr=0.001)
        
        for epoch in range(15):
            total_loss = 0
            for batch in range(20):
                x = torch.randn(32, 128).to(self.device)
                y = torch.randint(0, 10, (32,)).to(self.device)
                
                optimizer.zero_grad()
                output = self.teacher(x)
                loss = F.cross_entropy(output, y)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            if epoch % 5 == 0:
                print(f"    æ•™å¸ˆé¢„è®­ç»ƒ Epoch {epoch+1}/15, Loss: {total_loss/20:.4f}")
        
        self.teacher.eval()
        print("âœ… æ•™å¸ˆæ¨¡å‹é¢„è®­ç»ƒå®Œæˆ")
    
    def _create_synthetic_data(self, batch_size=32):
        """åˆ›å»ºåˆæˆè®­ç»ƒæ•°æ®"""
        x = torch.randn(batch_size, 128).to(self.device)
        y = torch.randint(0, 10, (batch_size,)).to(self.device)
        return x, y
    
    def _get_dynamic_weights(self, epoch, total_epochs):
        """åŠ¨æ€è°ƒæ•´æŸå¤±æƒé‡"""
        progress = epoch / total_epochs
        
        # ä»»åŠ¡æƒé‡ï¼šä»é«˜åˆ°ä½
        task_weight = (
            self.config['distillation']['initial_task_weight'] * (1 - progress) +
            self.config['distillation']['final_task_weight'] * progress
        )
        
        # è’¸é¦æƒé‡ï¼šä»ä½åˆ°é«˜
        distill_weight = (
            self.config['distillation']['initial_distill_weight'] * (1 - progress) +
            self.config['distillation']['final_distill_weight'] * progress
        )
        
        adversarial_weight = self.config['distillation']['adversarial_weight']
        
        return task_weight, distill_weight, adversarial_weight
    
    def train_epoch(self, epoch, total_epochs):
        """è®­ç»ƒå•ä¸ªepoch"""
        
        # è·å–åŠ¨æ€æƒé‡
        task_weight, distill_weight, adversarial_weight = self._get_dynamic_weights(epoch, total_epochs)
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        all_params = (
            list(self.student.parameters()) +
            list(self.knowledge_filter.parameters()) +
            list(self.adversarial_validator.parameters())
        )
        optimizer = torch.optim.Adam(all_params, lr=self.config['training']['learning_rate'])
        
        epoch_metrics = {
            'total_loss': 0, 'task_loss': 0, 'distill_loss': 0, 'adversarial_loss': 0,
            'relevance_scores': 0, 'quality_scores': 0, 'knowledge_utilization': 0,
            'feature_quality': 0, 'accuracy': 0
        }
        
        num_batches = self.config['training']['num_batches']
        
        for batch in range(num_batches):
            # ç”Ÿæˆè®­ç»ƒæ•°æ®
            x, y = self._create_synthetic_data(self.config['training']['batch_size'])
            
            optimizer.zero_grad()
            
            # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
            student_output, student_features = self.student(x, return_features=True)
            
            # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆæ— æ¢¯åº¦ï¼‰
            with torch.no_grad():
                teacher_output, teacher_features = self.teacher(x, return_features=True)
            
            # è®¡ç®—ä»»åŠ¡æŸå¤±
            task_loss = F.cross_entropy(student_output, y)
            
            # ä»»åŠ¡å¯¼å‘çŸ¥è¯†è¿‡æ»¤
            filter_result = self.knowledge_filter(
                teacher_features, student_features, task_loss
            )
            distill_loss = filter_result['distill_loss']
            
            # å¯¹æŠ—å¼çŸ¥è¯†éªŒè¯
            validator_result = self.adversarial_validator(
                filter_result['enhanced_features'], task_loss
            )
            adversarial_loss = validator_result['adversarial_loss']
            
            # æ€»æŸå¤±
            total_loss = (
                task_weight * task_loss +
                distill_weight * distill_loss +
                adversarial_weight * adversarial_loss
            )
            
            # åå‘ä¼ æ’­
            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(all_params, max_norm=1.0)
            optimizer.step()
            
            # è®¡ç®—å‡†ç¡®ç‡
            with torch.no_grad():
                pred = student_output.argmax(dim=1)
                accuracy = (pred == y).float().mean()
            
            # ç´¯ç§¯æŒ‡æ ‡
            epoch_metrics['total_loss'] += float(total_loss.item())
            epoch_metrics['task_loss'] += float(task_loss.item())
            epoch_metrics['distill_loss'] += float(distill_loss.item())
            epoch_metrics['adversarial_loss'] += float(adversarial_loss.item())
            epoch_metrics['relevance_scores'] += float(filter_result['relevance_scores'].item())
            epoch_metrics['quality_scores'] += float(filter_result['quality_scores'].item())
            epoch_metrics['knowledge_utilization'] += float(filter_result['knowledge_utilization'].item())
            epoch_metrics['feature_quality'] += float(validator_result['feature_quality'].item())
            epoch_metrics['accuracy'] += float(accuracy.item())
        
        # å¹³å‡åŒ–æŒ‡æ ‡
        for key in epoch_metrics:
            epoch_metrics[key] /= num_batches
        
        # æ·»åŠ æƒé‡ä¿¡æ¯
        epoch_metrics['task_weight'] = task_weight
        epoch_metrics['distill_weight'] = distill_weight
        epoch_metrics['adversarial_weight'] = adversarial_weight
        
        return epoch_metrics
    
    def run_experiment(self):
        """è¿è¡Œå®Œæ•´å®éªŒ"""
        print("ğŸ¯ å¼€å§‹ä»»åŠ¡å¯¼å‘çŸ¥è¯†è’¸é¦å®éªŒ")
        print(f"ğŸ“… å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ”§ é…ç½®: {json.dumps(self.config, indent=2, ensure_ascii=False)}")
        
        start_time = time.time()
        total_epochs = self.config['training']['total_epochs']
        warmup_epochs = self.config['training']['warmup_epochs']
        
        best_accuracy = 0
        best_task_loss = float('inf')
        
        for epoch in range(total_epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            metrics = self.train_epoch(epoch, total_epochs)
            
            # è®°å½•å†å²
            for key, value in metrics.items():
                self.training_history[key].append(value)
            
            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            if metrics['accuracy'] > best_accuracy:
                best_accuracy = metrics['accuracy']
            if metrics['task_loss'] < best_task_loss:
                best_task_loss = metrics['task_loss']
            
            # æ‰“å°è¿›åº¦
            if (epoch + 1) % 5 == 0 or epoch < warmup_epochs:
                stage = "é¢„çƒ­é˜¶æ®µ" if epoch < warmup_epochs else "è’¸é¦é˜¶æ®µ"
                print(f"\nğŸ“ˆ Epoch {epoch+1:2d}/{total_epochs} [{stage}]:")
                print(f"    æ€»æŸå¤±: {metrics['total_loss']:.4f}")
                print(f"    ä»»åŠ¡æŸå¤±: {metrics['task_loss']:.4f} | å‡†ç¡®ç‡: {metrics['accuracy']:.3f}")
                print(f"    è’¸é¦æŸå¤±: {metrics['distill_loss']:.4f} | å¯¹æŠ—æŸå¤±: {metrics['adversarial_loss']:.4f}")
                print(f"    çŸ¥è¯†ç›¸å…³æ€§: {metrics['relevance_scores']:.3f} | çŸ¥è¯†è´¨é‡: {metrics['quality_scores']:.3f}")
                print(f"    çŸ¥è¯†åˆ©ç”¨ç‡: {metrics['knowledge_utilization']:.3f} | ç‰¹å¾è´¨é‡: {metrics['feature_quality']:.3f}")
                print(f"    æƒé‡ - ä»»åŠ¡: {metrics['task_weight']:.2f}, è’¸é¦: {metrics['distill_weight']:.2f}, å¯¹æŠ—: {metrics['adversarial_weight']:.2f}")
        
        end_time = time.time()
        total_time = end_time - start_time
        
        print(f"\nğŸ‰ ä»»åŠ¡å¯¼å‘è’¸é¦å®éªŒå®Œæˆ! æ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.3f}")
        print(f"ğŸ“Š æœ€ä½³ä»»åŠ¡æŸå¤±: {best_task_loss:.4f}")
        
        # åˆ†æç»“æœ
        self.analyze_results()
        
        return self.training_history
    
    def analyze_results(self):
        """åˆ†æå®éªŒç»“æœ"""
        print("\nğŸ“Š ä»»åŠ¡å¯¼å‘è’¸é¦å®éªŒç»“æœåˆ†æ:")
        print("=" * 60)
        
        # è·å–å…³é”®æŒ‡æ ‡
        initial_task_loss = self.training_history['task_loss'][0]
        final_task_loss = self.training_history['task_loss'][-1]
        initial_accuracy = self.training_history['accuracy'][0]
        final_accuracy = self.training_history['accuracy'][-1]
        
        task_improvement = (initial_task_loss - final_task_loss) / initial_task_loss * 100
        accuracy_improvement = (final_accuracy - initial_accuracy) / initial_accuracy * 100
        
        print(f"\nğŸ¯ ä»»åŠ¡æ€§èƒ½åˆ†æ:")
        print(f"    åˆå§‹ä»»åŠ¡æŸå¤±: {initial_task_loss:.4f} â†’ æœ€ç»ˆ: {final_task_loss:.4f}")
        print(f"    ä»»åŠ¡æŸå¤±æ”¹å–„: {task_improvement:.1f}%")
        print(f"    åˆå§‹å‡†ç¡®ç‡: {initial_accuracy:.3f} â†’ æœ€ç»ˆ: {final_accuracy:.3f}")
        print(f"    å‡†ç¡®ç‡æå‡: {accuracy_improvement:.1f}%")
        
        # çŸ¥è¯†ä¼ é€’æ•ˆæœåˆ†æ
        avg_relevance = np.mean(self.training_history['relevance_scores'][-10:])
        avg_quality = np.mean(self.training_history['quality_scores'][-10:])
        avg_utilization = np.mean(self.training_history['knowledge_utilization'][-10:])
        
        print(f"\nğŸ§  çŸ¥è¯†ä¼ é€’åˆ†æ:")
        print(f"    å¹³å‡çŸ¥è¯†ç›¸å…³æ€§: {avg_relevance:.3f}")
        print(f"    å¹³å‡çŸ¥è¯†è´¨é‡: {avg_quality:.3f}")
        print(f"    å¹³å‡çŸ¥è¯†åˆ©ç”¨ç‡: {avg_utilization:.3f}")
        
        # æˆåŠŸè¯„ä¼°
        if task_improvement > 15 and accuracy_improvement > 10:
            print("\nâœ… å®éªŒéå¸¸æˆåŠŸ! ä»»åŠ¡å¯¼å‘ç­–ç•¥æ˜¾è‘—æ”¹å–„äº†æ€§èƒ½")
            print("    å»ºè®®: åœ¨çœŸå®æ•°æ®é›†ä¸ŠéªŒè¯å¹¶éƒ¨ç½²")
        elif task_improvement > 8 and accuracy_improvement > 5:
            print("\nâš ï¸  å®éªŒéƒ¨åˆ†æˆåŠŸï¼Œæœ‰æ˜æ˜¾æ”¹å–„")
            print("    å»ºè®®: è¿›ä¸€æ­¥ä¼˜åŒ–çŸ¥è¯†è¿‡æ»¤ç­–ç•¥")
        elif task_improvement > 3:
            print("\nğŸ”„ å®éªŒæœ‰æ”¹å–„ä½†ä»æœ‰ä¼˜åŒ–ç©ºé—´")
            print("    å»ºè®®: è°ƒæ•´å¯¹æŠ—è®­ç»ƒæƒé‡å’ŒçŸ¥è¯†è¿‡æ»¤é˜ˆå€¼")
        else:
            print("\nâŒ å®éªŒæ•ˆæœæœ‰é™ï¼Œéœ€è¦é‡æ–°è¯„ä¼°ç­–ç•¥")
            print("    å»ºè®®: æ£€æŸ¥æ•°æ®è´¨é‡å’Œæ¨¡å‹æ¶æ„åŒ¹é…åº¦")
        
        # ä¿å­˜ç»“æœ
        self.save_results()
    
    def save_results(self):
        """ä¿å­˜å®éªŒç»“æœ"""
        results_dir = "experiments/task_oriented_distillation"
        os.makedirs(results_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f"{results_dir}/task_oriented_results_{timestamp}.json"
        
        # å‡†å¤‡ä¿å­˜æ•°æ®
        save_data = {
            'timestamp': timestamp,
            'config': self.config,
            'training_history': dict(self.training_history),
            'final_metrics': {
                'task_loss': self.training_history['task_loss'][-1],
                'accuracy': self.training_history['accuracy'][-1],
                'distill_loss': self.training_history['distill_loss'][-1],
                'knowledge_utilization': self.training_history['knowledge_utilization'][-1]
            }
        }
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(save_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜è‡³: {results_file}")

# ğŸ¯ ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•° - è¿è¡Œä»»åŠ¡å¯¼å‘çŸ¥è¯†è’¸é¦å®éªŒ"""
    print("ğŸ¯ MapSage V5 - ä»»åŠ¡å¯¼å‘çŸ¥è¯†è’¸é¦å®éªŒ")
    print("=" * 50)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = TaskOrientedDistillationTrainer()
    
    # è¿è¡Œå®éªŒ
    results = trainer.run_experiment()
    
    print("\nğŸ¯ ä»»åŠ¡å¯¼å‘çŸ¥è¯†è’¸é¦å®éªŒå®Œæˆ!")
    return results

if __name__ == "__main__":
    results = main()