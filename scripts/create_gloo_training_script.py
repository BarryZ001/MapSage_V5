#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›å»ºä½¿ç”¨glooåç«¯çš„åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
ç”±äºECCLåç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨glooåç«¯ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆ
"""

import os

def create_gloo_distributed_script():
    """åˆ›å»ºä½¿ç”¨glooåç«¯çš„åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬"""
    
    script_content = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨glooåç«¯çš„GCUåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
é€‚ç”¨äºtorch_gcuç¯å¢ƒä¸­ECCLåç«¯ä¸å¯ç”¨çš„æƒ…å†µ
"""

import os
import sys
import torch
import torch.distributed as dist
import torch_gcu  # type: ignore

def setup_distributed_gloo():
    """è®¾ç½®glooåç«¯çš„åˆ†å¸ƒå¼ç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®glooåç«¯åˆ†å¸ƒå¼ç¯å¢ƒ...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if 'RANK' not in os.environ:
        os.environ['RANK'] = '0'
    if 'WORLD_SIZE' not in os.environ:
        os.environ['WORLD_SIZE'] = '1'
    if 'MASTER_ADDR' not in os.environ:
        os.environ['MASTER_ADDR'] = '127.0.0.1'
    if 'MASTER_PORT' not in os.environ:
        os.environ['MASTER_PORT'] = '29500'
    
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    
    print(f"ğŸ“Š åˆ†å¸ƒå¼å‚æ•°:")
    print(f"  - RANK: {rank}")
    print(f"  - WORLD_SIZE: {world_size}")
    print(f"  - MASTER_ADDR: {os.environ['MASTER_ADDR']}")
    print(f"  - MASTER_PORT: {os.environ['MASTER_PORT']}")
    
    try:
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼ˆä½¿ç”¨glooåç«¯ï¼‰
        print("ğŸš€ åˆå§‹åŒ–glooåç«¯...")
        dist.init_process_group(
            backend='gloo',
            init_method=f"tcp://{os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}",
            world_size=world_size,
            rank=rank
        )
        
        print("âœ… glooåç«¯åˆå§‹åŒ–æˆåŠŸï¼")
        
        # è®¾ç½®GCUè®¾å¤‡
        if torch_gcu.is_available():  # type: ignore
            device_count = torch_gcu.device_count()  # type: ignore
            local_rank = rank % device_count
            torch_gcu.set_device(local_rank)  # type: ignore
            print(f"âœ… è®¾ç½®GCUè®¾å¤‡: {local_rank}")
        
        return True
        
    except Exception as e:
        print(f"âŒ glooåç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def test_distributed_operations():
    """æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œ"""
    print("\\nğŸ§ª æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œ...")
    
    try:
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        if torch_gcu.is_available():  # type: ignore
            device = torch_gcu.current_device()  # type: ignore
            tensor = torch.tensor([float(dist.get_rank())], device=f'gcu:{device}')
        else:
            tensor = torch.tensor([float(dist.get_rank())])
        
        print(f"ğŸ“Š è¿›ç¨‹ {dist.get_rank()} çš„åŸå§‹å¼ é‡: {tensor.item()}")
        
        # æ‰§è¡Œall_reduceæ“ä½œ
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        print(f"âœ… è¿›ç¨‹ {dist.get_rank()} çš„all_reduceç»“æœ: {tensor.item()}")
        
        # æ‰§è¡Œbroadcastæ“ä½œ
        if dist.get_rank() == 0:
            broadcast_tensor = torch.tensor([42.0])
        else:
            broadcast_tensor = torch.tensor([0.0])
            
        if torch_gcu.is_available():  # type: ignore
            device = torch_gcu.current_device()  # type: ignore
            broadcast_tensor = broadcast_tensor.to(f'gcu:{device}')
        
        dist.broadcast(broadcast_tensor, src=0)
        print(f"âœ… è¿›ç¨‹ {dist.get_rank()} çš„broadcastç»“æœ: {broadcast_tensor.item()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    try:
        if dist.is_initialized():
            dist.destroy_process_group()
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²æ¸…ç†")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒæ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ glooåç«¯åˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•")
    print("=" * 50)
    
    try:
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        if not setup_distributed_gloo():
            return False
        
        # æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œ
        success = test_distributed_operations()
        
        # æ¸…ç†ç¯å¢ƒ
        cleanup_distributed()
        
        if success:
            print("\\nğŸ‰ glooåç«¯åˆ†å¸ƒå¼æµ‹è¯•æˆåŠŸï¼")
            print("ğŸ’¡ å¯ä»¥ä½¿ç”¨glooåç«¯è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        else:
            print("\\nâŒ glooåç«¯åˆ†å¸ƒå¼æµ‹è¯•å¤±è´¥")
        
        return success
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        cleanup_distributed()
        return False

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
'''
    
    # å†™å…¥è„šæœ¬æ–‡ä»¶
    script_path = "scripts/test_gloo_distributed.py"
    try:
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        # è®¾ç½®æ‰§è¡Œæƒé™
        os.chmod(script_path, 0o755)
        print(f"âœ… glooåˆ†å¸ƒå¼æµ‹è¯•è„šæœ¬å·²ç”Ÿæˆ: {script_path}")
        return script_path
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆglooåˆ†å¸ƒå¼æµ‹è¯•è„šæœ¬å¤±è´¥: {e}")
        return None

def create_training_launcher():
    """åˆ›å»ºåˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬"""
    
    launcher_content = '''#!/bin/bash
# glooåç«¯åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬

echo "ğŸš€ å¯åŠ¨glooåç«¯åˆ†å¸ƒå¼è®­ç»ƒ..."

# è®¾ç½®ç¯å¢ƒå˜é‡
source scripts/setup_eccl_env.sh

# æ£€æŸ¥å‚æ•°
SCRIPT_PATH=${1:-"scripts/train_distributed_gcu_fixed.py"}
NUM_GPUS=${2:-8}

echo "ğŸ“‹ è®­ç»ƒå‚æ•°:"
echo "  - è®­ç»ƒè„šæœ¬: $SCRIPT_PATH"
echo "  - GPUæ•°é‡: $NUM_GPUS"

# ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¼ºåˆ¶ä½¿ç”¨glooåç«¯ï¼‰
export TORCH_DISTRIBUTED_BACKEND=gloo

torchrun \\
    --nproc_per_node=$NUM_GPUS \\
    --master_addr=127.0.0.1 \\
    --master_port=29500 \\
    $SCRIPT_PATH \\
    --backend=gloo \\
    --launcher=pytorch

echo "ğŸ¯ åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼"
'''
    
    launcher_path = "scripts/start_gloo_training.sh"
    try:
        with open(launcher_path, 'w', encoding='utf-8') as f:
            f.write(launcher_content)
        
        os.chmod(launcher_path, 0o755)
        print(f"âœ… glooè®­ç»ƒå¯åŠ¨è„šæœ¬å·²ç”Ÿæˆ: {launcher_path}")
        return launcher_path
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆglooè®­ç»ƒå¯åŠ¨è„šæœ¬å¤±è´¥: {e}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ åˆ›å»ºglooåç«¯åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•è„šæœ¬
    test_script = create_gloo_distributed_script()
    
    # åˆ›å»ºå¯åŠ¨è„šæœ¬
    launcher_script = create_training_launcher()
    
    print("\\nğŸ“‹ ä½¿ç”¨è¯´æ˜:")
    print("1. è¿è¡Œè¯Šæ–­è„šæœ¬: python3 scripts/diagnose_torch_gcu_backends.py")
    print("2. æµ‹è¯•glooåç«¯: python3 scripts/test_gloo_distributed.py")
    print("3. å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ: bash scripts/start_gloo_training.sh")
    print("\\nğŸ’¡ ç”±äºECCLåç«¯ä¸å¯ç”¨ï¼Œå»ºè®®ä½¿ç”¨glooåç«¯è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")

if __name__ == '__main__':
    main()