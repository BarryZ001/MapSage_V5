#!/bin/bash
# glooåç«¯åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨è„šæœ¬

echo "ğŸš€ å¯åŠ¨glooåç«¯åˆ†å¸ƒå¼è®­ç»ƒ..."

# è®¾ç½®ç¯å¢ƒå˜é‡
source scripts/setup_eccl_env.sh

# æ£€æŸ¥å‚æ•°
SCRIPT_PATH=${1:-"scripts/train_distributed_gcu_fixed.py"}
NUM_GPUS=${2:-8}
CONFIG_FILE=${3:-"configs/train_dinov3_mmrs1m_t20_gcu.py"}

echo "ğŸ“‹ è®­ç»ƒå‚æ•°:"
echo "  - è®­ç»ƒè„šæœ¬: $SCRIPT_PATH"
echo "  - GPUæ•°é‡: $NUM_GPUS"
echo "  - é…ç½®æ–‡ä»¶: $CONFIG_FILE"

# æ£€æŸ¥é…ç½®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
if [ ! -f "$CONFIG_FILE" ]; then
    echo "âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $CONFIG_FILE"
    echo "ğŸ’¡ è¯·ä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š"
    echo "   bash scripts/start_gloo_training.sh [è®­ç»ƒè„šæœ¬] [GPUæ•°é‡] [é…ç½®æ–‡ä»¶]"
    echo "   ä¾‹å¦‚: bash scripts/start_gloo_training.sh scripts/train_distributed_gcu_fixed.py 8 configs/train_dinov3_mmrs1m_t20_gcu.py"
    exit 1
fi

# ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¼ºåˆ¶ä½¿ç”¨glooåç«¯ï¼‰
export TORCH_DISTRIBUTED_BACKEND=gloo

torchrun \
    --nproc_per_node=$NUM_GPUS \
    --master_addr=127.0.0.1 \
    --master_port=29500 \
    $SCRIPT_PATH \
    $CONFIG_FILE \
    --backend=gloo \
    --launcher=pytorch

echo "ğŸ¯ åˆ†å¸ƒå¼è®­ç»ƒå®Œæˆï¼"
