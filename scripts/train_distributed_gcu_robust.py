#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import os
import sys
import time
import socket
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# ä¿®å¤distutils.versionå…¼å®¹æ€§é—®é¢˜
try:
    from packaging import version
    import distutils
    if not hasattr(distutils, 'version'):
        distutils.version = version
        print("âœ… ä¿®å¤distutils.versionå…¼å®¹æ€§é—®é¢˜")
except ImportError:
    try:
        import distutils.version
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥ç‰ˆæœ¬å¤„ç†æ¨¡å—ï¼Œå¯èƒ½å½±å“TensorBoardåŠŸèƒ½")

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from mmengine.config import Config
from mmengine.runner import Runner

# å°è¯•å¯¼å…¥torch_gcuå’Œptex
try:
    import torch_gcu  # type: ignore
    import torch_gcu.distributed as gcu_dist  # type: ignore
    print(f"âœ… torch_gcuå¯¼å…¥æˆåŠŸï¼Œå¯ç”¨è®¾å¤‡æ•°: {torch_gcu.device_count()}")
    print("âœ… torch_gcu.distributedæ¨¡å—å¯¼å…¥æˆåŠŸ")
    USE_GCU_DISTRIBUTED = True
except ImportError as e:
    print(f"âš ï¸ torch_gcuå¯¼å…¥å¤±è´¥: {e}")
    torch_gcu = None
    gcu_dist = None
    USE_GCU_DISTRIBUTED = False

try:
    import ptex  # type: ignore
    print("âœ… ptexå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ ptexå¯¼å…¥å¤±è´¥: {e}")
    ptex = None

# å¯¼å…¥MMSegç›¸å…³æ¨¡å—
try:
    import mmseg  # type: ignore
    from mmseg.models import *  # type: ignore
    from mmseg.datasets import *  # type: ignore
except ImportError as e:
    print(f"âš ï¸ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    import mmseg_custom.models
    import mmseg_custom.datasets
    print("âœ… è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")


def check_network_connectivity(master_addr, master_port, timeout=10):
    """æ£€æŸ¥ç½‘ç»œè¿æ¥æ€§"""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(timeout)
        result = sock.connect_ex((master_addr, int(master_port)))
        sock.close()
        return result == 0
    except Exception as e:
        print(f"âš ï¸ ç½‘ç»œè¿æ¥æ£€æŸ¥å¤±è´¥: {e}")
        return False


def setup_distributed_robust(backend='gloo', max_retries=3, retry_delay=5):
    """è®¾ç½®ç¨³å®šçš„åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # è·å–åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    
    print(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®: world_size={world_size}, rank={rank}, local_rank={local_rank}")
    
    # éªŒè¯ç¯å¢ƒå˜é‡è®¾ç½®
    master_addr = os.environ.get('MASTER_ADDR', '127.0.0.1')
    master_port = os.environ.get('MASTER_PORT', '29500')
    print(f"ğŸ”§ Masteråœ°å€: {master_addr}:{master_port}")
    
    # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
    timeout_seconds = 300  # 5åˆ†é’Ÿè¶…æ—¶
    os.environ['NCCL_BLOCKING_WAIT'] = '1'
    os.environ['GLOO_SOCKET_IFNAME'] = 'eth0'  # æŒ‡å®šç½‘ç»œæ¥å£
    
    # å¦‚æœæ˜¯å¤šè¿›ç¨‹åˆ†å¸ƒå¼è®­ç»ƒï¼Œåˆå§‹åŒ–è¿›ç¨‹ç»„
    if world_size > 1:
        # æ£€æŸ¥ç½‘ç»œè¿æ¥æ€§ï¼ˆä»…å¯¹émasterèŠ‚ç‚¹ï¼‰
        if rank != 0:
            print(f"ğŸ” æ£€æŸ¥ä¸MasterèŠ‚ç‚¹çš„ç½‘ç»œè¿æ¥...")
            if not check_network_connectivity(master_addr, master_port):
                print(f"âš ï¸ æ— æ³•è¿æ¥åˆ°MasterèŠ‚ç‚¹ {master_addr}:{master_port}")
                time.sleep(retry_delay)  # ç­‰å¾…ä¸€æ®µæ—¶é—´å†é‡è¯•
        
        # ä½¿ç”¨ä¼ å…¥çš„backendå‚æ•°ï¼Œé»˜è®¤ä¸ºgloo
        os.environ['MMENGINE_DDP_BACKEND'] = backend
        print(f"ğŸ”§ è®¾ç½®MMEngine DDPåç«¯ä¸º: {backend}")
        
        # å¤šæ¬¡é‡è¯•åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
        for attempt in range(max_retries):
            if not dist.is_initialized():
                try:
                    print(f"ğŸ”„ å°è¯•åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ (ç¬¬{attempt + 1}æ¬¡)")
                    
                    # è®¾ç½®æ›´é•¿çš„è¶…æ—¶æ—¶é—´
                    import datetime
                    timeout = datetime.timedelta(seconds=timeout_seconds)
                    
                    dist.init_process_group(
                        backend=backend,
                        init_method=f"tcp://{master_addr}:{master_port}",
                        world_size=world_size,
                        rank=rank,
                        timeout=timeout
                    )
                    print(f"âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–å®Œæˆ - Backend: {backend}")
                    break
                    
                except Exception as e:
                    print(f"âŒ ç¬¬{attempt + 1}æ¬¡{backend}åç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                    if attempt < max_retries - 1:
                        print(f"â³ ç­‰å¾…{retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                    else:
                        print("âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œæ— æ³•åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ")
                        raise
            else:
                print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²ç»åˆå§‹åŒ–")
                break
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        try:
            torch_gcu.set_device(local_rank)
            print(f"ğŸ”§ è®¾ç½®GCUè®¾å¤‡: {local_rank}")
        except Exception as e:
            print(f"âš ï¸ è®¾ç½®GCUè®¾å¤‡å¤±è´¥: {e}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['TOPS_VISIBLE_DEVICES'] = str(local_rank)
    print(f"ğŸ”§ è®¾ç½®TOPS_VISIBLE_DEVICES: {local_rank}")
    
    # ç¦ç”¨CUDA
    os.environ['CUDA_VISIBLE_DEVICES'] = ''
    print("ğŸ”§ ç¦ç”¨CUDA_VISIBLE_DEVICES")
    
    print(f"âœ… åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank {rank}/{world_size}")
    return rank, local_rank, world_size


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    try:
        if dist.is_initialized():
            print("ğŸ§¹ æ¸…ç†åˆ†å¸ƒå¼è¿›ç¨‹ç»„...")
            dist.destroy_process_group()
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„æ¸…ç†å®Œæˆ")
    except Exception as e:
        print(f"âš ï¸ åˆ†å¸ƒå¼æ¸…ç†å¤±è´¥: {e}")
    
    # æ¸…ç†torch_gcu - ä¿®å¤empty_cacheæ–¹æ³•ä¸å­˜åœ¨çš„é—®é¢˜
    if torch_gcu is not None:
        try:
            # æ£€æŸ¥torch_gcuæ˜¯å¦æœ‰empty_cacheæ–¹æ³•
            if hasattr(torch_gcu, 'empty_cache'):
                torch_gcu.empty_cache()
                print("âœ… torch_gcuç¼“å­˜æ¸…ç†å®Œæˆ")
            elif hasattr(torch_gcu, 'synchronize'):
                torch_gcu.synchronize()
                print("âœ… torch_gcuåŒæ­¥å®Œæˆ")
            else:
                print("â„¹ï¸ torch_gcuæ— éœ€æ¸…ç†ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸ torch_gcuæ¸…ç†å¤±è´¥: {e}")


def main():
    parser = argparse.ArgumentParser(description='Robust MMSegmentation distributed training script for GCU')
    parser.add_argument('config', help='train config file path')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], default='pytorch', help='job launcher')
    parser.add_argument('--backend', choices=['nccl', 'gloo', 'mpi'], default='gloo', help='distributed backend')
    parser.add_argument('--max-retries', type=int, default=3, help='maximum retries for distributed initialization')
    parser.add_argument('--retry-delay', type=int, default=5, help='delay between retries in seconds')
    args = parser.parse_args()

    print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–ç¨³å®šçš„åˆ†å¸ƒå¼MMSegmentationè®­ç»ƒ...")
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, local_rank, world_size = setup_distributed_robust(
        backend=args.backend,
        max_retries=args.max_retries,
        retry_delay=args.retry_delay
    )
    
    try:
        # ä»æ–‡ä»¶åŠ è½½é…ç½®
        if not os.path.exists(args.config):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        
        print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        cfg = Config.fromfile(args.config)
        
        # æ¸…ç†é…ç½®ä¸­å¯èƒ½å¯¼è‡´pickleé”™è¯¯çš„æ¨¡å—å¯¹è±¡
        def clean_config_for_pickle(config_dict):
            """é€’å½’æ¸…ç†é…ç½®ä¸­ä¸èƒ½è¢«pickleçš„å¯¹è±¡"""
            if isinstance(config_dict, dict):
                cleaned = {}
                for key, value in config_dict.items():
                    # è·³è¿‡æ¨¡å—å¯¹è±¡å’Œå‡½æ•°å¯¹è±¡
                    if hasattr(value, '__module__') and not isinstance(value, (str, int, float, bool, list, tuple, dict)):
                        continue
                    elif callable(value) and not isinstance(value, type):
                        continue
                    else:
                        cleaned[key] = clean_config_for_pickle(value)
                return cleaned
            elif isinstance(config_dict, (list, tuple)):
                return [clean_config_for_pickle(item) for item in config_dict]
            else:
                return config_dict
        
        # å¤‡ä»½åŸå§‹é…ç½®ä¸­çš„å…³é”®ä¿¡æ¯
        original_custom_imports = getattr(cfg, 'custom_imports', None)
        
        # ä¸´æ—¶ç§»é™¤å¯èƒ½å¯¼è‡´pickleé—®é¢˜çš„custom_imports
        if hasattr(cfg, 'custom_imports'):
            delattr(cfg, 'custom_imports')
            print("ğŸ”§ ä¸´æ—¶ç§»é™¤custom_importsä»¥é¿å…pickleé”™è¯¯")
        
        # è®¾ç½®å·¥ä½œç›®å½•
        if cfg.get('work_dir', None) is None:
            cfg.work_dir = './work_dirs'
        
        # è®¾ç½®GCUè®¾å¤‡
        device_str = None
        if ptex is not None:
            try:
                device_str = "xla"
                print(f"ğŸ”§ è®¾ç½®GCUè®¾å¤‡ä¸º: {device_str}")
            except Exception as e:
                print(f"âš ï¸ è®¾ç½®ptexè®¾å¤‡å¤±è´¥: {e}")
        elif torch_gcu is not None:
            try:
                device_str = f"gcu:{local_rank}"
                print(f"ğŸ”§ è®¾ç½®GCUè®¾å¤‡ä¸º: {device_str}")
            except Exception as e:
                print(f"âš ï¸ è®¾ç½®torch_gcuè®¾å¤‡å¤±è´¥: {e}")
        
        # æ›´æ–°é…ç½®ä»¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
        if world_size > 1:
            cfg.launcher = args.launcher
        else:
            cfg.launcher = 'none'
            print("ğŸ”§ å•è¿›ç¨‹æ¨¡å¼ï¼Œç¦ç”¨åˆ†å¸ƒå¼")
        
        # è°ƒæ•´batch sizeï¼ˆæ¯ä¸ªè¿›ç¨‹çš„batch sizeï¼‰
        if hasattr(cfg, 'train_dataloader') and 'batch_size' in cfg.train_dataloader:
            original_batch_size = cfg.train_dataloader.batch_size
            cfg.train_dataloader.batch_size = max(1, original_batch_size // world_size)
            print(f"ğŸ“Š è°ƒæ•´batch size: {original_batch_size} -> {cfg.train_dataloader.batch_size} (per process)")
        
        # é…ç½®MMEngineçš„åˆ†å¸ƒå¼è®¾ç½®ä»¥æ­£ç¡®å¤„ç†GCUè®¾å¤‡
        if world_size > 1 and device_str is not None:
            if not hasattr(cfg, 'model_wrapper_cfg'):
                cfg.model_wrapper_cfg = {}
            
            # é…ç½®åˆ†å¸ƒå¼åŒ…è£…å™¨ï¼Œå¢åŠ ç¨³å®šæ€§è®¾ç½®
            cfg.model_wrapper_cfg.update({
                'type': 'MMDistributedDataParallel',
                'device_ids': None,
                'broadcast_buffers': False,  # å‡å°‘é€šä¿¡å¼€é”€
                'find_unused_parameters': True,  # å¤„ç†æœªä½¿ç”¨çš„å‚æ•°
            })
            print("ğŸ”§ é…ç½®MMEngineåˆ†å¸ƒå¼åŒ…è£…å™¨")
        
        # æ·»åŠ åˆ†å¸ƒå¼è®­ç»ƒçš„ç¨³å®šæ€§è®¾ç½®
        if world_size > 1:
            # è®¾ç½®æ¢¯åº¦åŒæ­¥é¢‘ç‡
            if hasattr(cfg, 'optim_wrapper'):
                if not hasattr(cfg.optim_wrapper, 'accumulative_counts'):
                    cfg.optim_wrapper.accumulative_counts = 1
            
            # è®¾ç½®æ£€æŸ¥ç‚¹ä¿å­˜ç­–ç•¥
            if hasattr(cfg, 'default_hooks'):
                if 'checkpoint' in cfg.default_hooks:
                    cfg.default_hooks.checkpoint.save_best = 'auto'
                    cfg.default_hooks.checkpoint.max_keep_ckpts = 3
        
        print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        
        # åœ¨åˆ›å»ºRunnerä¹‹å‰ï¼Œç¡®ä¿é…ç½®å¯ä»¥è¢«æ·±æ‹·è´
        try:
            import copy
            # æµ‹è¯•é…ç½®æ˜¯å¦å¯ä»¥è¢«æ·±æ‹·è´
            copy.deepcopy(cfg)
            print("âœ… é…ç½®æ·±æ‹·è´æµ‹è¯•é€šè¿‡")
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ·±æ‹·è´æµ‹è¯•å¤±è´¥: {e}")
            # å¦‚æœæ·±æ‹·è´å¤±è´¥ï¼Œå°è¯•é‡æ–°æ„å»ºé…ç½®
            print("ğŸ”§ å°è¯•é‡æ–°æ„å»ºé…ç½®...")
            cfg_dict = cfg.to_dict()
            cfg = Config(cfg_dict)
        
        # åˆ›å»ºRunnerå¹¶å¼€å§‹è®­ç»ƒ
        runner = Runner.from_cfg(cfg)
        runner.train()
        
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()


if __name__ == '__main__':
    main()