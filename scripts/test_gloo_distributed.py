#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨glooåç«¯çš„GCUåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
é€‚ç”¨äºtorch_gcuç¯å¢ƒä¸­ECCLåç«¯ä¸å¯ç”¨çš„æƒ…å†µ
"""

import os
import sys
import torch
import torch.distributed as dist
import torch_gcu  # type: ignore

def setup_distributed_gloo():
    """è®¾ç½®glooåç«¯çš„åˆ†å¸ƒå¼ç¯å¢ƒ"""
    print("ğŸ”§ è®¾ç½®glooåç«¯åˆ†å¸ƒå¼ç¯å¢ƒ...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if 'RANK' not in os.environ:
        os.environ['RANK'] = '0'
    if 'WORLD_SIZE' not in os.environ:
        os.environ['WORLD_SIZE'] = '1'
    if 'MASTER_ADDR' not in os.environ:
        os.environ['MASTER_ADDR'] = '127.0.0.1'
    if 'MASTER_PORT' not in os.environ:
        os.environ['MASTER_PORT'] = '29500'
    
    rank = int(os.environ['RANK'])
    world_size = int(os.environ['WORLD_SIZE'])
    
    print(f"ğŸ“Š åˆ†å¸ƒå¼å‚æ•°:")
    print(f"  - RANK: {rank}")
    print(f"  - WORLD_SIZE: {world_size}")
    print(f"  - MASTER_ADDR: {os.environ['MASTER_ADDR']}")
    print(f"  - MASTER_PORT: {os.environ['MASTER_PORT']}")
    
    try:
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼ˆä½¿ç”¨glooåç«¯ï¼‰
        print("ğŸš€ åˆå§‹åŒ–glooåç«¯...")
        dist.init_process_group(
            backend='gloo',
            init_method=f"tcp://{os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}",
            world_size=world_size,
            rank=rank
        )
        
        print("âœ… glooåç«¯åˆå§‹åŒ–æˆåŠŸï¼")
        
        # è®¾ç½®GCUè®¾å¤‡
        if torch_gcu.is_available():  # type: ignore
            device_count = torch_gcu.device_count()  # type: ignore
            local_rank = rank % device_count
            torch_gcu.set_device(local_rank)  # type: ignore
            print(f"âœ… è®¾ç½®GCUè®¾å¤‡: {local_rank}")
        
        return True
        
    except Exception as e:
        print(f"âŒ glooåç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
        return False

def test_distributed_operations():
    """æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œ"""
    print("\nğŸ§ª æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œ...")
    
    try:
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        if torch_gcu.is_available():  # type: ignore
            device = torch_gcu.current_device()  # type: ignore
            tensor = torch.tensor([float(dist.get_rank())], device=f'gcu:{device}')
        else:
            tensor = torch.tensor([float(dist.get_rank())])
        
        print(f"ğŸ“Š è¿›ç¨‹ {dist.get_rank()} çš„åŸå§‹å¼ é‡: {tensor.item()}")
        
        # æ‰§è¡Œall_reduceæ“ä½œ
        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
        print(f"âœ… è¿›ç¨‹ {dist.get_rank()} çš„all_reduceç»“æœ: {tensor.item()}")
        
        # æ‰§è¡Œbroadcastæ“ä½œ
        if dist.get_rank() == 0:
            broadcast_tensor = torch.tensor([42.0])
        else:
            broadcast_tensor = torch.tensor([0.0])
            
        if torch_gcu.is_available():  # type: ignore
            device = torch_gcu.current_device()  # type: ignore
            broadcast_tensor = broadcast_tensor.to(f'gcu:{device}')
        
        dist.broadcast(broadcast_tensor, src=0)
        print(f"âœ… è¿›ç¨‹ {dist.get_rank()} çš„broadcastç»“æœ: {broadcast_tensor.item()}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    try:
        if dist.is_initialized():
            dist.destroy_process_group()
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²æ¸…ç†")
    except Exception as e:
        print(f"âš ï¸ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒæ—¶å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ glooåç«¯åˆ†å¸ƒå¼è®­ç»ƒæµ‹è¯•")
    print("=" * 50)
    
    try:
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        if not setup_distributed_gloo():
            return False
        
        # æµ‹è¯•åˆ†å¸ƒå¼æ“ä½œ
        success = test_distributed_operations()
        
        # æ¸…ç†ç¯å¢ƒ
        cleanup_distributed()
        
        if success:
            print("\nğŸ‰ glooåç«¯åˆ†å¸ƒå¼æµ‹è¯•æˆåŠŸï¼")
            print("ğŸ’¡ å¯ä»¥ä½¿ç”¨glooåç«¯è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        else:
            print("\nâŒ glooåç«¯åˆ†å¸ƒå¼æµ‹è¯•å¤±è´¥")
        
        return success
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        cleanup_distributed()
        return False

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)
