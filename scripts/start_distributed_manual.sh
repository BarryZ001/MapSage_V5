#!/bin/bash
# scripts/start_distributed_manual.sh - æ‰‹åŠ¨å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

set -e

# é…ç½®å‚æ•°
CONFIG_FILE="configs/train_dinov3_mmrs1m_t20_gcu.py"
WORLD_SIZE=8
MASTER_ADDR="127.0.0.1"
MASTER_PORT=29500

echo "ğŸš€ æ‰‹åŠ¨å¯åŠ¨DINOv3åˆ†å¸ƒå¼è®­ç»ƒ - ä½¿ç”¨${WORLD_SIZE}ä¸ªGCU"
echo "ğŸ“ é…ç½®æ–‡ä»¶: ${CONFIG_FILE}"
echo "ğŸŒ Masteråœ°å€: ${MASTER_ADDR}:${MASTER_PORT}"

# è®¾ç½®ç¯å¢ƒå˜é‡
export TORCH_DEVICE=xla
export XLA_USE_BF16=1
export MMENGINE_DEVICE=xla
export CUDA_VISIBLE_DEVICES=""
export WORLD_SIZE=${WORLD_SIZE}
export MASTER_ADDR=${MASTER_ADDR}
export MASTER_PORT=${MASTER_PORT}

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒè¿›ç¨‹
for i in $(seq 0 $((WORLD_SIZE-1))); do
    echo "ğŸ”„ å¯åŠ¨è¿›ç¨‹ ${i}/${WORLD_SIZE}..."
    
    # åœ¨åå°å¯åŠ¨æ¯ä¸ªè¿›ç¨‹
    RANK=${i} LOCAL_RANK=${i} python3 scripts/train_distributed.py ${CONFIG_FILE} --launcher pytorch > "logs/train_rank_${i}.log" 2>&1 &
    
    # è®°å½•è¿›ç¨‹ID
    echo $! > "logs/train_rank_${i}.pid"
    
    echo "âœ… è¿›ç¨‹ ${i} å·²å¯åŠ¨ (PID: $!)"
    
    # çŸ­æš‚å»¶è¿Ÿé¿å…åŒæ—¶å¯åŠ¨
    sleep 3
done

echo "ğŸ‰ æ‰€æœ‰${WORLD_SIZE}ä¸ªè®­ç»ƒè¿›ç¨‹å·²å¯åŠ¨"
echo "ğŸ“Š ç›‘æ§å‘½ä»¤: tail -f logs/train_rank_*.log"
echo "ğŸ›‘ åœæ­¢å‘½ä»¤: ./scripts/stop_distributed_training.sh"

# ç­‰å¾…æ‰€æœ‰è¿›ç¨‹
wait