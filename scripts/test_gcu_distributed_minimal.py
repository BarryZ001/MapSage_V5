#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€å°çš„torch_gcuåˆ†å¸ƒå¼æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ecclåç«¯æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import torch
import torch.distributed as dist  # type: ignore
import time

def test_gcu_import():
    """æµ‹è¯•torch_gcuå¯¼å…¥"""
    try:
        import torch_gcu  # type: ignore
        import torch_gcu.distributed as gcu_dist  # type: ignore
        print(f"âœ… torch_gcuå¯¼å…¥æˆåŠŸï¼Œå¯ç”¨è®¾å¤‡æ•°: {torch_gcu.device_count()}")
        print("âœ… torch_gcu.distributedæ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True, torch_gcu, gcu_dist
    except ImportError as e:
        print(f"âŒ torch_gcuå¯¼å…¥å¤±è´¥: {e}")
        return False, None, None

def test_distributed_init():
    """æµ‹è¯•åˆ†å¸ƒå¼åˆå§‹åŒ–"""
    # è·å–åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    master_addr = os.environ.get('MASTER_ADDR', '127.0.0.1')
    master_port = os.environ.get('MASTER_PORT', '29500')
    
    print(f"ğŸŒ åˆ†å¸ƒå¼å‚æ•°: world_size={world_size}, rank={rank}, local_rank={local_rank}")
    print(f"ğŸ”§ Masteråœ°å€: {master_addr}:{master_port}")
    
    if world_size == 1:
        print("âš ï¸ å•è¿›ç¨‹æ¨¡å¼ï¼Œè·³è¿‡åˆ†å¸ƒå¼æµ‹è¯•")
        return True
    
    # æµ‹è¯•torch_gcuå¯¼å…¥
    has_gcu, torch_gcu, gcu_dist = test_gcu_import()
    
    # è®¾ç½®GCUè®¾å¤‡
    if has_gcu and torch_gcu is not None:
        try:
            torch_gcu.set_device(local_rank)
            print(f"âœ… è®¾ç½®GCUè®¾å¤‡: {local_rank}")
        except Exception as e:
            print(f"âš ï¸ è®¾ç½®GCUè®¾å¤‡å¤±è´¥: {e}")
    
    # å°è¯•ä¸åŒçš„åç«¯
    backends_to_try = ['eccl', 'gloo'] if has_gcu else ['gloo']
    
    for backend in backends_to_try:
        print(f"\nğŸ”§ æµ‹è¯•åç«¯: {backend}")
        
        if dist.is_initialized():
            dist.destroy_process_group()
        
        try:
            # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
            dist.init_process_group(
                backend=backend,
                init_method=f"tcp://{master_addr}:{master_port}",
                world_size=world_size,
                rank=rank,
                timeout=dist.default_pg_timeout * 2  # type: ignore
            )
            print(f"âœ… {backend}åç«¯åˆå§‹åŒ–æˆåŠŸ")
            
            # æµ‹è¯•åŸºæœ¬çš„åˆ†å¸ƒå¼æ“ä½œ
            test_tensor = torch.tensor([rank], dtype=torch.float32)
            if has_gcu and torch_gcu is not None:
                try:
                    device = torch_gcu.device(local_rank)
                    test_tensor = test_tensor.to(device)
                    print(f"âœ… å¼ é‡ç§»åŠ¨åˆ°GCUè®¾å¤‡: {device}")
                except Exception as e:
                    print(f"âš ï¸ å¼ é‡ç§»åŠ¨åˆ°GCUå¤±è´¥: {e}")
            
            # æµ‹è¯•all_reduceæ“ä½œ
            try:
                dist.all_reduce(test_tensor, op=dist.ReduceOp.SUM)
                expected_sum = sum(range(world_size))
                if abs(test_tensor.item() - expected_sum) < 1e-6:
                    print(f"âœ… all_reduceæµ‹è¯•æˆåŠŸ: {test_tensor.item()} == {expected_sum}")
                else:
                    print(f"âš ï¸ all_reduceç»“æœä¸æ­£ç¡®: {test_tensor.item()} != {expected_sum}")
            except Exception as e:
                print(f"âŒ all_reduceæµ‹è¯•å¤±è´¥: {e}")
                continue
            
            # æµ‹è¯•barrieråŒæ­¥
            try:
                dist.barrier()
                print("âœ… barrieråŒæ­¥æˆåŠŸ")
            except Exception as e:
                print(f"âŒ barrieråŒæ­¥å¤±è´¥: {e}")
                continue
            
            print(f"âœ… {backend}åç«¯æ‰€æœ‰æµ‹è¯•é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âŒ {backend}åç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            continue
    
    print("âŒ æ‰€æœ‰åç«¯æµ‹è¯•éƒ½å¤±è´¥")
    return False

def cleanup():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        try:
            dist.destroy_process_group()
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„æ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸ åˆ†å¸ƒå¼æ¸…ç†å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª å¼€å§‹torch_gcuåˆ†å¸ƒå¼æµ‹è¯•...")
    
    try:
        success = test_distributed_init()
        
        if success:
            print("\nğŸ‰ åˆ†å¸ƒå¼æµ‹è¯•æˆåŠŸï¼")
            print("âœ… å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
        else:
            print("\nâŒ åˆ†å¸ƒå¼æµ‹è¯•å¤±è´¥")
            print("âš ï¸ è¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
            
        return success
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False
    finally:
        cleanup()

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)