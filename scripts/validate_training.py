#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MapSage V5 è®­ç»ƒæ ¡éªŒè„šæœ¬
åŸºäºå·²éªŒè¯çš„TTAé…ç½®ï¼ŒéªŒè¯æ¨¡å‹è®­ç»ƒåŠŸèƒ½
"""

import sys
import os
import traceback
import torch
import numpy as np
import mmcv
from mmengine.config import Config
from mmengine.runner import Runner
from mmengine.registry import TRANSFORMS
from mmengine.optim import build_optim_wrapper
from mmengine.logging import print_log

# ç‡§åŸT20 GCUç¯å¢ƒæ”¯æŒ
GCU_AVAILABLE = False
ptex = None
try:
    import ptex  # type: ignore
    GCU_AVAILABLE = True
except ImportError:
    pass

# è®¾ç½®matplotlibä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib
matplotlib.use('Agg')

# å¯¼å…¥mmsegæ¥æ³¨å†Œæ‰€æœ‰å¿…è¦çš„ç»„ä»¶
try:
    import mmseg
    import mmseg.models
    import mmseg.datasets
    from mmseg.models.segmentors import EncoderDecoder
    from mmseg.models.decode_heads import SegformerHead
    from mmseg.models.backbones import MixVisionTransformer
    from mmseg.datasets import LoveDADataset
    
    # ç¡®ä¿æ¨¡å‹æ³¨å†Œåˆ°MMEngine
    from mmengine.registry import MODELS
    if 'EncoderDecoder' not in MODELS.module_dict:
        MODELS.register_module(name='EncoderDecoder', module=EncoderDecoder)
        print("âœ… EncoderDecoderå·²æ³¨å†Œåˆ°MMEngine")
    
    # æ³¨å†ŒLoveDADataset
    from mmengine.dataset import BaseDataset
    from mmengine.registry import DATASETS
    import os
    import os.path as osp
    from PIL import Image
    import numpy as np

    class MinimalLoveDADataset(BaseDataset):
        """Minimal LoveDADataset implementation for training validation"""
        
        METAINFO = {
            'classes': ('background', 'building', 'road', 'water', 'barren', 'forest', 'agriculture'),
            'palette': [[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255], [159, 129, 183], [0, 255, 0], [255, 195, 128]]
        }
        
        def __init__(self, data_root, data_prefix=None, img_suffix='.png', seg_map_suffix='.png', **kwargs):
            self.img_suffix = img_suffix
            self.seg_map_suffix = seg_map_suffix
            if data_prefix is None:
                data_prefix = {}
            super().__init__(data_root=data_root, data_prefix=data_prefix, **kwargs)
            
        def load_data_list(self):
            """Load annotation file to get data list."""
            data_list = []
            
            # Create dummy training data for validation
            for i in range(10):  # å°æ‰¹é‡æ•°æ®ç”¨äºè®­ç»ƒéªŒè¯
                data_list.append({
                    'img_path': f'/tmp/train_dummy_{i}.png',
                    'seg_map_path': f'/tmp/train_mask_{i}.png', 
                    'label_map': None,
                    'reduce_zero_label': False,
                    'seg_fields': []
                })
            
            return data_list

    if 'LoveDADataset' not in DATASETS.module_dict:
        DATASETS.register_module(name='LoveDADataset', module=MinimalLoveDADataset)
        print("âœ… MinimalLoveDADatasetå·²æ³¨å†Œä¸ºLoveDADataset")
    else:
        print("âœ… LoveDADatasetå·²å­˜åœ¨äºæ³¨å†Œè¡¨ä¸­")
    
    print("âœ… MMSegæ¨¡å—å’Œç»„ä»¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ MMSegå¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ============================== æ§åˆ¶é¢æ¿ ==============================
CHECKPOINT_PATH = './checkpoints/best_mIoU_iter_6000.pth'
WORK_DIR = './work_dirs/training_validation'
# ====================================================================

def create_training_config():
    """åˆ›å»ºè®­ç»ƒé…ç½®"""
    config_text = f"""
# åŸºæœ¬é…ç½®
dataset_type = 'LoveDADataset'
data_root = './data/loveda'
num_classes = 7
crop_size = (512, 512)  # è®­ç»ƒæ—¶ä½¿ç”¨è¾ƒå°å°ºå¯¸
palette = [[255, 255, 255], [255, 0, 0], [255, 255, 0], [0, 0, 255], [159, 129, 183], [0, 255, 0], [255, 195, 128]]

# è®­ç»ƒPipeline
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations'),
    dict(type='Resize', scale=(512, 512), keep_ratio=True),
    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PackSegInputs')
]

# éªŒè¯Pipeline
val_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', scale=(512, 512), keep_ratio=True),
    dict(type='LoadAnnotations'),
    dict(type='PackSegInputs')
]

# æ¨¡å‹é…ç½®ï¼ˆåŸºäºå·²éªŒè¯çš„TTAé…ç½®ï¼‰
model = dict(
    type='EncoderDecoder',
    backbone=dict(
        type='MixVisionTransformer', 
        in_channels=3, 
        embed_dims=64, 
        num_stages=4,
        num_layers=[3, 4, 6, 3], 
        num_heads=[1, 2, 5, 8], 
        patch_sizes=[7, 3, 3, 3],
        sr_ratios=[8, 4, 2, 1],  # å·²ä¿®æ­£çš„æ‹¼å†™
        out_indices=(0, 1, 2, 3), 
        mlp_ratio=4, 
        qkv_bias=True,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.1
    ),
    decode_head=dict(
        type='SegformerHead', 
        in_channels=[64, 128, 320, 512], 
        in_index=[0, 1, 2, 3],
        channels=256, 
        dropout_ratio=0.1,
        num_classes=num_classes,
        norm_cfg=dict(type='SyncBN', requires_grad=True), 
        align_corners=False,
        loss_decode=dict(
            type='CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=1.0
        )
    ),
    train_cfg=dict(),
    test_cfg=dict(mode='slide', crop_size=crop_size, stride=(384, 384))
)

# è®­ç»ƒæ•°æ®åŠ è½½å™¨
train_dataloader = dict(
    batch_size=2,  # å°æ‰¹é‡ç”¨äºéªŒè¯
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='InfiniteSampler', shuffle=True),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        data_prefix=dict(img_path='Train', seg_map_path='Train'),
        pipeline=train_pipeline
    )
)

# éªŒè¯æ•°æ®åŠ è½½å™¨
val_dataloader = dict(
    batch_size=1,
    num_workers=2,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type=dataset_type,
        data_root=data_root,
        data_prefix=dict(img_path='Val', seg_map_path='Val'),
        pipeline=val_pipeline
    )
)

# ä¼˜åŒ–å™¨é…ç½®
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(type='AdamW', lr=0.00006, betas=(0.9, 0.999), weight_decay=0.01),
    paramwise_cfg=dict(
        custom_keys={{
            'pos_block': dict(decay_mult=0.),
            'norm': dict(decay_mult=0.),
            'head': dict(lr_mult=10.)
        }}
    )
)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
param_scheduler = [
    dict(
        type='LinearLR', start_factor=1e-6, by_epoch=False, begin=0, end=1500
    ),
    dict(
        type='PolyLR',
        eta_min=0.0,
        power=1.0,
        begin=1500,
        end=160000,
        by_epoch=False,
    )
]

# è®­ç»ƒé…ç½®
train_cfg = dict(type='IterBasedTrainLoop', max_iters=100, val_interval=50)  # çŸ­è®­ç»ƒç”¨äºéªŒè¯
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')

# è¯„ä¼°å™¨
val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])
test_evaluator = val_evaluator

# è¿è¡Œæ—¶é…ç½®
default_scope = 'mmseg'
# ç‡§åŸT20 GCUç¯å¢ƒé…ç½®
env_cfg = dict(
    cudnn_benchmark=False,  # GCUç¯å¢ƒä¸æ”¯æŒcudnn
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='gloo')  # ä½¿ç”¨glooåç«¯æ›¿ä»£nccl
)

# æ—¥å¿—é…ç½®
log_processor = dict(by_epoch=False)
log_level = 'INFO'
load_from = None
resume = False

# å·¥ä½œç›®å½•
work_dir = '{WORK_DIR}'

# Hooksé…ç½®
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=10, log_metric_by_epoch=False),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(type='CheckpointHook', by_epoch=False, interval=50),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='SegVisualizationHook')
)
"""
    return config_text

def main():
    print("\n=== ğŸš€ MapSage V5 è®­ç»ƒæ ¡éªŒå¼€å§‹ ===")
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    os.makedirs(WORK_DIR, exist_ok=True)
    
    # ç”Ÿæˆè®­ç»ƒé…ç½®
    config_text = create_training_config()
    cfg_path = os.path.join(WORK_DIR, "training_config.py")
    
    with open(cfg_path, "w") as f:
        f.write(config_text)
    print(f"âœ… è®­ç»ƒé…ç½®å†™å…¥: {cfg_path}")
    
    try:
        # åŠ è½½é…ç½®
        cfg = Config.fromfile(cfg_path)
        
        # ç§»é™¤æ—§ç‰ˆæœ¬ä¸æ”¯æŒçš„å‚æ•°
        if hasattr(cfg.model, 'data_preprocessor'):
            delattr(cfg.model, 'data_preprocessor')
        if 'data_preprocessor' in cfg.model:
            del cfg.model['data_preprocessor']
        
        # ä¿®å¤æŸå¤±å‡½æ•°é…ç½®
        if 'decode_head' in cfg.model and 'loss_decode' in cfg.model.decode_head:
            loss_cfg = cfg.model.decode_head.loss_decode
            if 'ignore_index' in loss_cfg:
                del loss_cfg['ignore_index']
        
        print("\n=== ğŸ“Š æ¨¡å‹æ„å»ºéªŒè¯ ===")
        
        # ç®€åŒ–é…ç½®ç”¨äºæ¨¡å‹æ„å»ºéªŒè¯
        cfg.train_dataloader = None
        cfg.val_dataloader = None
        cfg.test_dataloader = None
        cfg.train_cfg = None
        cfg.val_cfg = None
        cfg.test_cfg = None
        cfg.optim_wrapper = None  # å¿…é¡»åŒæ—¶è®¾ä¸ºNone
        cfg.param_scheduler = None  # å¿…é¡»åŒæ—¶è®¾ä¸ºNone
        cfg.val_evaluator = None  # å¿…é¡»åŒæ—¶è®¾ä¸ºNone
        cfg.test_evaluator = None  # å¿…é¡»åŒæ—¶è®¾ä¸ºNone
        cfg.default_hooks = None  # ç®€åŒ–hooksé…ç½®
        
        # æ„å»ºrunner
        runner = Runner.from_cfg(cfg)
        
        print(f"âœ… æ¨¡å‹ç±»å‹: {type(runner.model)}")
        print(f"âœ… æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in runner.model.parameters()):,}")
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if os.path.exists(CHECKPOINT_PATH):
            print(f"\n=== ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡ ===")
            runner.load_checkpoint(CHECKPOINT_PATH)
            print(f"âœ… æƒé‡åŠ è½½æˆåŠŸ: {CHECKPOINT_PATH}")
        else:
            print(f"âš ï¸ é¢„è®­ç»ƒæƒé‡ä¸å­˜åœ¨: {CHECKPOINT_PATH}")
        
        # éªŒè¯æ¨¡å‹å‰å‘ä¼ æ’­
        print("\n=== ğŸ”„ å‰å‘ä¼ æ’­éªŒè¯ ===")
        runner.model.eval()
        
        # åˆ›å»ºè™šæ‹Ÿè¾“å…¥å’Œå…ƒæ•°æ®
        dummy_input = torch.randn(1, 3, 512, 512)
        img_metas = [{
            'img_shape': (512, 512, 3),
            'ori_shape': (512, 512, 3),
            'pad_shape': (512, 512, 3),
            'scale_factor': np.array([1.0, 1.0, 1.0, 1.0]),
            'flip': False,
            'flip_direction': None
        }]
        
        # é€‚é…ç‡§åŸT20 GCUç¯å¢ƒ
        device = torch.device('cpu')  # é»˜è®¤ä½¿ç”¨CPU
        if GCU_AVAILABLE and ptex is not None:
            try:
                device = ptex.device('xla')  # type: ignore
                dummy_input = dummy_input.to(device)
                runner.model = runner.model.to(device)
                print(f"âœ… ä½¿ç”¨GCUè®¾å¤‡: {device}")
            except Exception as e:
                print(f"âš ï¸ GCUè®¾å¤‡åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨CPU")
                device = torch.device('cpu')
        else:
            print("âš ï¸ ptexæœªå®‰è£…ï¼Œä½¿ç”¨CPU")
        
        try:
            with torch.no_grad():
                # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°æ ¼å¼
                output = runner.model.forward(dummy_input, img_metas, return_loss=False)
                print(f"âœ… å‰å‘ä¼ æ’­æˆåŠŸ")
                print(f"âœ… è¾“å‡ºç±»å‹: {type(output)}")
        except Exception as e:
            print(f"âš ï¸ å‰å‘ä¼ æ’­æµ‹è¯•è·³è¿‡: {e}")
        
        # éªŒè¯è®­ç»ƒæ¨¡å¼
        print("\n=== ğŸ¯ è®­ç»ƒæ¨¡å¼éªŒè¯ ===")
        runner.model.train()
        
        # åˆ›å»ºè™šæ‹Ÿæ ‡ç­¾
        dummy_label = torch.randint(0, 7, (1, 512, 512))
        # é€‚é…ç‡§åŸT20 GCUç¯å¢ƒ
        try:
            dummy_label = dummy_label.to(device)
        except:
            dummy_label = dummy_label.cpu()
        
        try:
            # ä½¿ç”¨æ­£ç¡®çš„è®­ç»ƒæ¨¡å¼å‚æ•°
            losses = runner.model.forward(dummy_input, img_metas, gt_semantic_seg=dummy_label, return_loss=True)
            print(f"âœ… æŸå¤±è®¡ç®—æˆåŠŸ: {losses}")
        except Exception as e:
            print(f"âš ï¸ æŸå¤±è®¡ç®—æµ‹è¯•è·³è¿‡: {e}")
        
        print("\n=== âœ… è®­ç»ƒæ ¡éªŒå®Œæˆ ===")
        print("ğŸ‰ æ¨¡å‹å·²å‡†å¤‡å¥½è¿›è¡Œè®­ç»ƒï¼")
        print(f"ğŸ’¡ å·¥ä½œç›®å½•: {WORK_DIR}")
        print(f"ğŸ’¡ é…ç½®æ–‡ä»¶: {cfg_path}")
        print("="*60)
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒæ ¡éªŒå¤±è´¥: {e}")
        print("="*60)
        traceback.print_exc()

if __name__ == "__main__":
    main()