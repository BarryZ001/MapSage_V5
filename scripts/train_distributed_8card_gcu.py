#!/usr/bin/env python3
"""
8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - ç‡§åŸT20 GCUç‰ˆæœ¬
æ”¯æŒDINOv3 + MMRS-1Mæ•°æ®é›†çš„8å¡åˆ†å¸ƒå¼è®­ç»ƒ
"""

import argparse
import os
import sys
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from mmengine.config import Config
from mmengine.runner import Runner

# å°è¯•å¯¼å…¥GCUç›¸å…³åº“
try:
    import torch_gcu
    print(f"âœ… torch_gcuå¯¼å…¥æˆåŠŸï¼Œå¯ç”¨è®¾å¤‡æ•°: {torch_gcu.device_count()}")
except ImportError as e:
    print(f"âš ï¸ torch_gcuå¯¼å…¥å¤±è´¥: {e}")
    torch_gcu = None

try:
    import ptex
    print("âœ… ptexå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ ptexå¯¼å…¥å¤±è´¥: {e}")
    ptex = None

# å¯¼å…¥MMSegç›¸å…³æ¨¡å—
try:
    import mmseg  # type: ignore
    from mmseg.models import *  # type: ignore
    from mmseg.datasets import *  # type: ignore
except ImportError as e:
    print(f"âš ï¸ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    import mmseg_custom.models
    import mmseg_custom.datasets
    print("âœ… è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # ä»ç¯å¢ƒå˜é‡è·å–åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    
    print(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°:")
    print(f"  - WORLD_SIZE: {world_size}")
    print(f"  - RANK: {rank}")
    print(f"  - LOCAL_RANK: {local_rank}")
    
    if world_size > 1:
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
        if not dist.is_initialized():
            # è®¾ç½®åˆ†å¸ƒå¼åç«¯
            backend = 'gloo'  # GCUç¯å¢ƒä½¿ç”¨glooåç«¯
            init_method = 'env://'
            
            print(f"ğŸ”§ åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„:")
            print(f"  - Backend: {backend}")
            print(f"  - Init method: {init_method}")
            
            dist.init_process_group(
                backend=backend,
                init_method=init_method,
                world_size=world_size,
                rank=rank
            )
            
            print(f"âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸ")
        else:
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²åˆå§‹åŒ–")
    
    return world_size, rank, local_rank

def main():
    parser = argparse.ArgumentParser(description='8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬')
    parser.add_argument('config', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], 
                       default='pytorch', help='åˆ†å¸ƒå¼å¯åŠ¨å™¨')
    parser.add_argument('--local_rank', type=int, default=0, help='æœ¬åœ°è¿›ç¨‹rank')
    args = parser.parse_args()
    
    print(f"ğŸš€ å¯åŠ¨8å¡åˆ†å¸ƒå¼è®­ç»ƒ")
    print(f"ğŸ“„ é…ç½®æ–‡ä»¶: {args.config}")
    print(f"ğŸ”§ å¯åŠ¨å™¨: {args.launcher}")
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    world_size, rank, local_rank = setup_distributed()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    cfg = Config.fromfile(args.config)
    
    # è®¾ç½®å·¥ä½œç›®å½•
    if not os.path.exists(cfg.work_dir):
        os.makedirs(cfg.work_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—ç›®å½•
    log_dir = os.path.join(cfg.work_dir, 'logs')
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    
    # æ›´æ–°é…ç½®ä»¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
    if world_size > 1:
        cfg.launcher = args.launcher
        print(f"ğŸ”§ å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œlauncher: {args.launcher}")
        # é…ç½®GCUè®¾å¤‡ï¼Œè®©MMEngineè‡ªåŠ¨å¤„ç†åˆ†å¸ƒå¼
        cfg.device = 'gcu'
        print(f"ğŸ”§ é…ç½®GCUè®¾å¤‡ï¼Œworld_size: {world_size}")
    else:
        cfg.launcher = 'none'
        print("ğŸ”§ å•è¿›ç¨‹æ¨¡å¼ï¼Œç¦ç”¨åˆ†å¸ƒå¼")
        # å•å¡è®­ç»ƒé…ç½®
        cfg.device = 'gcu'
        print(f"ğŸ”§ é…ç½®å•å¡GCUè®¾å¤‡")
    
    # è°ƒæ•´batch sizeï¼ˆæ¯ä¸ªè¿›ç¨‹çš„batch sizeï¼‰
    if hasattr(cfg, 'train_dataloader') and 'batch_size' in cfg.train_dataloader:
        original_batch_size = cfg.train_dataloader.batch_size
        # 8å¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ¯å¡ä¿æŒé…ç½®çš„batch_size
        print(f"ğŸ“Š æ¯å¡batch size: {original_batch_size}")
        print(f"ğŸ“Š æ€»batch size: {original_batch_size * world_size}")
    
    print(f"ğŸ“ å·¥ä½œç›®å½•: {cfg.work_dir}")
    print(f"ğŸš€ å¯åŠ¨è®­ç»ƒ - Rank {rank}/{world_size}")
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        torch_gcu.set_device(local_rank)
        print(f"ğŸ”§ è®¾ç½®å½“å‰è¿›ç¨‹GCUè®¾å¤‡: {local_rank}")
        
        # è®¾ç½®é»˜è®¤è®¾å¤‡ç±»å‹ä¸ºGCUï¼Œç¡®ä¿æ–°åˆ›å»ºçš„tensoréƒ½åœ¨GCUä¸Š
        try:
            torch.set_default_device(f'gcu:{local_rank}')
            print(f"ğŸ”§ è®¾ç½®é»˜è®¤tensorè®¾å¤‡ä¸º: gcu:{local_rank}")
        except AttributeError:
            # å¦‚æœtorchç‰ˆæœ¬ä¸æ”¯æŒset_default_deviceï¼Œè·³è¿‡
            print(f"âš ï¸ torchç‰ˆæœ¬ä¸æ”¯æŒset_default_deviceï¼Œè·³è¿‡è®¾ç½®")
    
    # åˆ›å»ºRunnerå¹¶å¼€å§‹è®­ç»ƒ
    print("ğŸš€ åˆ›å»ºRunnerå¹¶å¼€å§‹è®­ç»ƒ...")
    runner = Runner.from_cfg(cfg)
    
    # ç¡®ä¿æ¨¡å‹ç§»åŠ¨åˆ°æ­£ç¡®çš„GCUè®¾å¤‡
    if torch_gcu is not None and hasattr(runner, 'model'):
        device = f'gcu:{local_rank}'
        print(f"ğŸ”§ æ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡: {device}")
        runner.model = runner.model.to(device)
        
        # éªŒè¯æ¨¡å‹å‚æ•°è®¾å¤‡
        for name, param in runner.model.named_parameters():
            if param.device.type != 'gcu':
                print(f"âš ï¸ å‚æ•° {name} ä»åœ¨ {param.device}ï¼Œæ‰‹åŠ¨ç§»åŠ¨åˆ° {device}")
                param.data = param.data.to(device)
            break  # åªæ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°ä½œä¸ºç¤ºä¾‹
    
    print("âœ… Runneråˆ›å»ºå®Œæˆï¼Œæ¨¡å‹å·²é…ç½®åˆ°GCUè®¾å¤‡")
    
    runner.train()
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    if world_size > 1 and dist.is_initialized():
        dist.destroy_process_group()
        print("ğŸ§¹ åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²æ¸…ç†")

if __name__ == '__main__':
    main()