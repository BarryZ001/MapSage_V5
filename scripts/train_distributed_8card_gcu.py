#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - ç‡§åŸT20 GCUç‰ˆæœ¬
æ”¯æŒDINOv3 + MMRS-1Mæ•°æ®é›†çš„8å¡åˆ†å¸ƒå¼è®­ç»ƒ
"""

import argparse
import os
import sys
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from mmengine.config import Config
from mmengine.runner import Runner

# å°è¯•å¯¼å…¥GCUç›¸å…³åº“
try:
    import torch_gcu
    print("âœ… torch_gcuå¯¼å…¥æˆåŠŸï¼Œå¯ç”¨è®¾å¤‡æ•°: {}".format(torch_gcu.device_count()))
except ImportError as e:
    print("âš ï¸ torch_gcuå¯¼å…¥å¤±è´¥: {}".format(e))
    torch_gcu = None

try:
    import ptex
    print("âœ… ptexå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print("âš ï¸ ptexå¯¼å…¥å¤±è´¥: {}".format(e))
    ptex = None

# å°è¯•å¯¼å…¥MMSegç›¸å…³æ¨¡å—
try:
    import mmseg  # type: ignore
    from mmseg.models import *  # type: ignore
    from mmseg.datasets import *  # type: ignore
except ImportError as e:
    print("âš ï¸ æ¨¡å—å¯¼å…¥å¤±è´¥: {}".format(e))

# å°è¯•å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from mmseg_custom.models import *  # type: ignore
    from mmseg_custom.datasets import *  # type: ignore
except ImportError as e:
    print("âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {}".format(e))

# å°è¯•å¯¼å…¥MMSegæ¨¡å‹ç»„ä»¶å¹¶æ³¨å†Œ
try:
    import mmseg
    import mmseg.models
    from mmseg.models.backbones import MixVisionTransformer
    from mmseg.models.decode_heads import SegformerHead
    from mmseg.models.segmentors import EncoderDecoder
    
    from mmengine.registry import MODELS
    if 'MixVisionTransformer' not in MODELS.module_dict:
        MODELS.register_module(name='MixVisionTransformer', module=MixVisionTransformer)
        print("âœ… MixVisionTransformerå·²æ³¨å†Œåˆ°MMEngine")
    
    if 'SegformerHead' not in MODELS.module_dict:
        MODELS.register_module(name='SegformerHead', module=SegformerHead)
        print("âœ… SegformerHeadå·²æ³¨å†Œåˆ°MMEngine")
        
    if 'EncoderDecoder' not in MODELS.module_dict:
        MODELS.register_module(name='EncoderDecoder', module=EncoderDecoder)
        print("âœ… EncoderDecoderå·²æ³¨å†Œåˆ°MMEngine")
        
    print("âœ… MMSegæ¨¡å‹ç»„ä»¶å¯¼å…¥å’Œæ³¨å†ŒæˆåŠŸ")
except ImportError as e:
    print("âš ï¸ MMSegå¯¼å…¥å¤±è´¥: {}".format(e))
    print("âš ï¸ å°†ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç»„ä»¶")

def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # è·å–åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    world_size = int(os.environ.get('WORLD_SIZE', 8))
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    
    print("ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°:")
    print("  - WORLD_SIZE: {}".format(world_size))
    print("  - RANK: {}".format(rank))
    print("  - LOCAL_RANK: {}".format(local_rank))
    
    # æ ¹æ®ç‡§åŸå®˜æ–¹æ–‡æ¡£é…ç½®ECCLåç«¯
    print("ğŸ” æ£€æŸ¥torch_gcuå’ŒECCLåç«¯æ”¯æŒ...")
    
    # æ£€æŸ¥torch_gcuæ˜¯å¦å¯ç”¨
    try:
        import torch_gcu
        if torch_gcu.is_available():
            print("âœ… torch_gcuå¯ç”¨ï¼Œè®¾å¤‡æ•°: {}".format(torch_gcu.device_count()))
            
            # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨ECCLåç«¯
            # ECCLæ˜¯ç‡§åŸå®˜æ–¹ä¸ºGCUè®¾å¤‡ä¸“é—¨ä¼˜åŒ–çš„åˆ†å¸ƒå¼é€šä¿¡åç«¯
            backend = 'eccl'
            print("ğŸ¯ ä½¿ç”¨ç‡§åŸå®˜æ–¹ECCLåç«¯ (ä¸“ä¸ºGCUè®¾å¤‡ä¼˜åŒ–)")
            
            # æ£€æŸ¥ECCLåç«¯æ˜¯å¦å¯ç”¨
            try:
                # å°è¯•å¯¼å…¥ECCLç›¸å…³æ¨¡å—
                import torch_gcu.distributed
                print("âœ… ECCLåˆ†å¸ƒå¼æ¨¡å—å¯¼å…¥æˆåŠŸ")
            except ImportError as e:
                print("âš ï¸ ECCLæ¨¡å—å¯¼å…¥å¤±è´¥: {}".format(e))
                print("ğŸ”„ å›é€€åˆ°glooåç«¯")
                backend = 'gloo'
        else:
            print("âš ï¸ torch_gcuä¸å¯ç”¨ï¼Œä½¿ç”¨å¤‡ç”¨åç«¯")
            backend = 'gloo'
    except ImportError as e:
        print("âŒ torch_gcuæœªå®‰è£…: {}".format(e))
        print("ğŸ”„ ä½¿ç”¨glooåç«¯")
        backend = 'gloo'
    except Exception as e:
        print("âŒ torch_gcuæ£€æŸ¥å¤±è´¥: {}".format(e))
        print("ğŸ”„ ä½¿ç”¨glooåç«¯")
        backend = 'gloo'
    
    init_method = 'env://'
    
    print("ğŸ”§ åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„:")
    print("  - Backend: {}".format(backend))
    print("  - Init method: {}".format(init_method))
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„
    try:
        # å…³é”®ä¿®å¤ï¼šå¯¹äºECCLåç«¯ï¼Œéœ€è¦ç‰¹æ®Šçš„åˆå§‹åŒ–æ–¹å¼
        if backend == 'eccl':
            print("ğŸ”§ ä½¿ç”¨ECCLåç«¯ç‰¹æ®Šåˆå§‹åŒ–...")
            # è®¾ç½®ECCLç¯å¢ƒå˜é‡
            os.environ['ECCL_BACKEND'] = 'eccl'
            os.environ['ECCL_DEVICE_TYPE'] = 'gcu'
            
        dist.init_process_group(
            backend=backend,
            init_method=init_method,
            world_size=world_size,
            rank=rank
        )
        print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print("âŒ åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–å¤±è´¥: {}".format(e))
        # å¦‚æœECCLåç«¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨glooä½œä¸ºå¤‡é€‰
        if backend == 'eccl':
            print("ğŸ”„ ECCLåç«¯å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨glooåç«¯ä½œä¸ºå¤‡é€‰...")
            try:
                dist.init_process_group(
                    backend='gloo',
                    init_method=init_method,
                    world_size=world_size,
                    rank=rank
                )
                print("âœ… ä½¿ç”¨glooåç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                print("âŒ glooåç«¯ä¹Ÿå¤±è´¥: {}".format(e2))
                raise
        else:
            raise
    
    return world_size, rank, local_rank

def main():
    parser = argparse.ArgumentParser(description='8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬')
    parser.add_argument('config', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', help='å·¥ä½œç›®å½•è·¯å¾„')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], 
                       default='pytorch', help='åˆ†å¸ƒå¼å¯åŠ¨å™¨')
    parser.add_argument('--local_rank', type=int, default=0, help='æœ¬åœ°è¿›ç¨‹rank')
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨8å¡åˆ†å¸ƒå¼è®­ç»ƒ")
    print("ğŸ“„ é…ç½®æ–‡ä»¶: {}".format(args.config))
    print("ğŸ”§ å¯åŠ¨å™¨: {}".format(args.launcher))
    
    # ===== START: FORCE ECCL BACKEND =====
    # å¼ºåˆ¶ä½¿ç”¨ECCLåç«¯ï¼Œç¡®ä¿ä¸GCUè®¾å¤‡å…¼å®¹
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        print(f'ğŸ”§ å¼ºåˆ¶ä½¿ç”¨ECCLåç«¯è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ - Rank {rank}/{world_size}')
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»åˆå§‹åŒ–
        if not dist.is_initialized():
            try:
                # è®¾ç½®ECCLç¯å¢ƒå˜é‡
                os.environ['ECCL_BACKEND'] = 'eccl'
                os.environ['ECCL_DEVICE_TYPE'] = 'gcu'
                
                # å¼ºåˆ¶åˆå§‹åŒ–ECCLåç«¯
                dist.init_process_group(
                    backend='eccl', 
                    init_method='env://', 
                    world_size=world_size, 
                    rank=rank
                )
                print("âœ… ECCLåç«¯åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âŒ ECCLåç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•ä½¿ç”¨setup_distributedå‡½æ•°")
                # å¦‚æœå¼ºåˆ¶åˆå§‹åŒ–å¤±è´¥ï¼Œå›é€€åˆ°åŸæœ‰é€»è¾‘
                world_size, rank, local_rank = setup_distributed()
        else:
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²åˆå§‹åŒ–")
    else:
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        world_size, rank, local_rank = setup_distributed()
    # ===== END: FORCE ECCL BACKEND =====
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    cfg = Config.fromfile(args.config)
    
    # æ£€æŸ¥å¹¶åˆ›å»ºå·¥ä½œç›®å½•
    if args.work_dir:
        # ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„å·¥ä½œç›®å½•
        cfg.work_dir = args.work_dir
        if not os.path.exists(cfg.work_dir):
            os.makedirs(cfg.work_dir, exist_ok=True)
            print("ğŸ“ åˆ›å»ºå·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    elif hasattr(cfg, 'work_dir') and cfg.work_dir:
        if not os.path.exists(cfg.work_dir):
            os.makedirs(cfg.work_dir, exist_ok=True)
            print("ğŸ“ åˆ›å»ºå·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    else:
        # å¦‚æœé…ç½®æ–‡ä»¶æ²¡æœ‰work_dirï¼Œè®¾ç½®é»˜è®¤å€¼
        cfg.work_dir = './work_dirs/train_distributed_8card_gcu'
        os.makedirs(cfg.work_dir, exist_ok=True)
        print("ğŸ“ è®¾ç½®é»˜è®¤å·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    
    # è®¾ç½®æ—¥å¿—ç›®å½•
    log_dir = os.path.join(cfg.work_dir, 'logs')
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    
    # æ›´æ–°é…ç½®ä»¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
    if world_size > 1:
        cfg.launcher = args.launcher
        print("ğŸ”§ å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œlauncher: {}".format(args.launcher))
        
        # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶MMEngineä½¿ç”¨ECCLåç«¯
        # æ£€æŸ¥å½“å‰ä½¿ç”¨çš„åç«¯
        if dist.is_initialized():
            current_backend = dist.get_backend()
            print("ğŸ” å½“å‰åˆ†å¸ƒå¼åç«¯: {}".format(current_backend))
            
            # å¦‚æœå½“å‰åç«¯æ˜¯ECCLï¼Œé…ç½®MMEngineä½¿ç”¨å®ƒ
            if current_backend == 'eccl':
                # ç¡®ä¿MMEngineçš„åˆ†å¸ƒå¼é…ç½®ä½¿ç”¨ECCL
                if not hasattr(cfg, 'env_cfg'):
                    cfg.env_cfg = {}
                if not hasattr(cfg.env_cfg, 'dist_cfg'):
                    cfg.env_cfg.dist_cfg = {}
                
                # è®¾ç½®åç«¯é…ç½®
                cfg.env_cfg.dist_cfg['backend'] = 'eccl'
                print("âœ… å¼ºåˆ¶MMEngineä½¿ç”¨ECCLåç«¯")
            else:
                print("âš ï¸ å½“å‰åç«¯ä¸æ˜¯ECCL: {}ï¼Œå¯èƒ½å¯¼è‡´XLAè®¾å¤‡å…¼å®¹æ€§é—®é¢˜".format(current_backend))
        
        # é…ç½®GCUè®¾å¤‡
        cfg.device = 'gcu'
        print("ğŸ”§ é…ç½®GCUè®¾å¤‡ï¼Œworld_size: {}".format(world_size))
    else:
        cfg.launcher = 'none'
        print("ğŸ”§ å•è¿›ç¨‹æ¨¡å¼ï¼Œç¦ç”¨åˆ†å¸ƒå¼")
        # å•å¡è®­ç»ƒé…ç½®
        cfg.device = 'gcu'
        print("ğŸ”§ é…ç½®å•å¡GCUè®¾å¤‡")
    
    # è°ƒæ•´batch sizeï¼ˆæ¯ä¸ªè¿›ç¨‹çš„batch sizeï¼‰
    if hasattr(cfg, 'train_dataloader') and 'batch_size' in cfg.train_dataloader:
        original_batch_size = cfg.train_dataloader.batch_size
        # 8å¡åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ¯å¡ä¿æŒé…ç½®çš„batch_size
        print("ğŸ“Š æ¯å¡batch size: {}".format(original_batch_size))
        print("ğŸ“Š æ€»batch size: {}".format(original_batch_size * world_size))
    
    print("ğŸ“ å·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    print("ğŸš€ å¯åŠ¨è®­ç»ƒ - Rank {}/{}".format(rank, world_size))
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        torch_gcu.set_device(local_rank)
        print("ğŸ”§ è®¾ç½®å½“å‰è¿›ç¨‹GCUè®¾å¤‡: {}".format(local_rank))
        
        # æ³¨é‡Šæ‰set_default_deviceè°ƒç”¨ï¼Œå› ä¸ºå®ƒå¯èƒ½ä¸åˆ†å¸ƒå¼é€šä¿¡å†²çª
        # è®©MMEngineè‡ªåŠ¨å¤„ç†è®¾å¤‡é…ç½®
        print("ğŸ”§ è·³è¿‡è®¾ç½®é»˜è®¤è®¾å¤‡ï¼Œè®©MMEngineè‡ªåŠ¨å¤„ç†è®¾å¤‡é…ç½®")
    
    # ä¿®æ”¹é…ç½®ä»¥é¿å…MMEngineçš„è®¾å¤‡ä¸åŒ¹é…é—®é¢˜
    print("ğŸ”§ ä¿®æ”¹é…ç½®ä»¥é€‚é…GCUè®¾å¤‡...")
    
    # å…³é”®ä¿®å¤ï¼šé…ç½®MMEngineä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
    if torch_gcu is not None:
        device = "gcu:{}".format(local_rank)
        
        # 1. è®¾ç½®å½“å‰GCUè®¾å¤‡
        torch_gcu.set_device(local_rank)
        
        # 2. é…ç½®åˆ†å¸ƒå¼è®­ç»ƒè®¾å¤‡
        cfg.device = device
        
        # 3. ç¦ç”¨CUDAç›¸å…³è®¾ç½®ï¼Œé¿å…è®¾å¤‡å†²çª
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        
        # 4. é…ç½®MMEngineçš„è®¾å¤‡è®¾ç½® - T20 XLAè®¾å¤‡å…¼å®¹æ€§ä¿®å¤
        if not hasattr(cfg, 'env_cfg'):
            cfg.env_cfg = {}
        
        # å…³é”®ä¿®å¤ï¼šå¯¹äºXLAè®¾å¤‡ï¼Œä½¿ç”¨ECCLåç«¯è¿›è¡Œåˆ†å¸ƒå¼é€šä¿¡
        if torch_gcu is not None:
            # å¯¹äºT20 GCUè®¾å¤‡ï¼Œå¼ºåˆ¶ä½¿ç”¨ECCLåç«¯
            cfg.env_cfg['dist_cfg'] = {'backend': 'eccl', 'init_method': 'env://'}
            print("ğŸ”§ T20ä¿®å¤ï¼šå¼ºåˆ¶é…ç½®ECCLåç«¯ç”¨äºXLAè®¾å¤‡åˆ†å¸ƒå¼é€šä¿¡")
        else:
            cfg.env_cfg['dist_cfg'] = {'backend': 'gloo'}
        
        # 5. ç¡®ä¿æ¨¡å‹åŒ…è£…å™¨ä½¿ç”¨æ­£ç¡®è®¾å¤‡
        if hasattr(cfg, 'model_wrapper_cfg'):
            if cfg.model_wrapper_cfg is None:
                cfg.model_wrapper_cfg = {}
            # ä¸è®¾ç½®device_idsï¼Œè®©MMEngineè‡ªåŠ¨æ£€æµ‹
            cfg.model_wrapper_cfg.pop('device_ids', None)
            cfg.model_wrapper_cfg.pop('output_device', None)
        
        print("ğŸ”§ é…ç½®è®¾å¤‡ä¸º: {}".format(device))
        print("ğŸ”§ é…ç½®åˆ†å¸ƒå¼åç«¯ä¸º: eccl")
    
    # å…³é”®ä¿®å¤ï¼šåœ¨åˆ›å»ºRunnerå‰å¼ºåˆ¶è®¾ç½®è®¾å¤‡é…ç½®
    if torch_gcu is not None:
        print("ğŸ”§ T20å…³é”®ä¿®å¤ï¼šåœ¨Runneråˆ›å»ºå‰é…ç½®è®¾å¤‡...")
        
        # å¼ºåˆ¶è®¾ç½®å½“å‰è®¾å¤‡
        torch_gcu.set_device(local_rank)
        
        # å…³é”®ä¿®å¤ï¼šå¯¹äºXLAè®¾å¤‡ï¼Œä½¿ç”¨GCUè®¾å¤‡è¿›è¡Œåˆ†å¸ƒå¼é€šä¿¡å’Œæ¨¡å‹è®¡ç®—
        device = f'gcu:{local_rank}'  # ç»Ÿä¸€ä½¿ç”¨GCUè®¾å¤‡
        
        # ç¡®ä¿é…ç½®ä¸­çš„è®¾å¤‡è®¾ç½®æ­£ç¡®
        cfg.device = device  # MMEngineä½¿ç”¨GCUè®¾å¤‡
        
        # å…³é”®ä¿®å¤ï¼šå®Œå…¨ç¦ç”¨MMEngineçš„DDP device_idsè®¾ç½®
        # è®©MMEngineè‡ªåŠ¨å¤„ç†è®¾å¤‡é…ç½®ï¼Œé¿å…è®¾å¤‡ä¸åŒ¹é…é”™è¯¯
        if not hasattr(cfg, 'model_wrapper_cfg') or cfg.model_wrapper_cfg is None:
            cfg.model_wrapper_cfg = {}
        
        # å®Œå…¨ç§»é™¤device_idså’Œoutput_deviceé…ç½®
        # è¿™æ ·MMEngineä¼šè‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ‰€åœ¨è®¾å¤‡å¹¶æ­£ç¡®é…ç½®DDP
        cfg.model_wrapper_cfg.pop('device_ids', None)
        
        # ä¸è®¾ç½®device_idsï¼Œè®©MMEngineæ ¹æ®æ¨¡å‹å®é™…è®¾å¤‡è‡ªåŠ¨é…ç½®
        print("ğŸ”§ ç¦ç”¨DDP device_idsè‡ªåŠ¨é…ç½®ï¼Œè®©MMEngineè‡ªåŠ¨æ£€æµ‹è®¾å¤‡")
        print("ğŸ”§ é…ç½®æ¨¡å‹è®¾å¤‡: {}".format(device))
    
    # åˆ›å»ºRunnerå¹¶å¼€å§‹è®­ç»ƒ
    print("ğŸš€ åˆ›å»ºRunner...")
    
    # è®©Runnerè‡ªå·±æ ¹æ®é…ç½®å­—å…¸æ„å»ºæ¨¡å‹ï¼Œä¸è¦æå‰æ„å»º
    # è¿™æ ·å¯ä»¥é¿å…yapfæ ¼å¼åŒ–é”™è¯¯ï¼Œå› ä¸ºcfg.modelä¿æŒä¸ºå­—å…¸æ ¼å¼
    print("ğŸ”§ è®©Runnerè‡ªåŠ¨æ„å»ºæ¨¡å‹ï¼Œä¿æŒcfg.modelä¸ºé…ç½®å­—å…¸æ ¼å¼")
    
    # ===== START: ç¦ç”¨DDPçš„device_idsè‡ªåŠ¨é…ç½® =====
    if cfg.get('launcher') == 'pytorch':
        # åœ¨ MMDistributedDataParallel çš„é…ç½®ä¸­ç¦ç”¨ device_ids
        # ä½¿ç”¨model_wrapper_cfgè€Œä¸æ˜¯model_wrapperï¼Œä¿æŒä¸MMEngineçš„ä¸€è‡´æ€§
        if not hasattr(cfg, 'model_wrapper_cfg') or cfg.model_wrapper_cfg is None:
            cfg.model_wrapper_cfg = {}
        
        # æ˜ç¡®è®¾ç½®DDPé…ç½®ï¼Œç¦ç”¨device_idså’Œoutput_device
        cfg.model_wrapper_cfg.update({
            'type': 'MMDistributedDataParallel',
            'find_unused_parameters': False,
            'device_ids': None,  # å…³é”®ï¼šæ˜¾å¼è®¾ç½®device_idsä¸ºNone
            'output_device': None  # å…³é”®ï¼šæ˜¾å¼è®¾ç½®output_deviceä¸ºNone
        })
        print("ğŸ”§ å·²æ›´æ–°model_wrapper_cfgé…ç½®ï¼Œç¦ç”¨device_idså’Œoutput_deviceè‡ªåŠ¨é…ç½®")
    # ===== END: ç¦ç”¨DDPçš„device_idsè‡ªåŠ¨é…ç½® =====
    
    # ===== START: ç¦ç”¨SyncBatchNorm for GCUå…¼å®¹æ€§ =====
    # å…³é”®ä¿®å¤ï¼šåœ¨Runneråˆ›å»ºå‰ç¦ç”¨SyncBatchNormï¼Œé¿å…GCUè®¾å¤‡å…¼å®¹æ€§é—®é¢˜
    print("ğŸ”§ å¼€å§‹ç¦ç”¨SyncBatchNormä»¥å…¼å®¹GCUåˆ†å¸ƒå¼è®­ç»ƒ...")
    
    def disable_sync_batchnorm_in_config(config_dict):
        """é€’å½’ç¦ç”¨é…ç½®ä¸­çš„SyncBatchNorm"""
        if isinstance(config_dict, dict):
            for key, value in config_dict.items():
                if key == 'norm_cfg' and isinstance(value, dict):
                    if value.get('type') == 'SyncBN':
                        print(f"ğŸ”§ å‘ç°SyncBNé…ç½®ï¼Œæ›¿æ¢ä¸ºBN: {value}")
                        value['type'] = 'BN'  # ä½¿ç”¨æ™®é€šBatchNormæ›¿ä»£SyncBatchNorm
                        print(f"âœ… å·²æ›¿æ¢ä¸º: {value}")
                elif isinstance(value, (dict, list)):
                    disable_sync_batchnorm_in_config(value)
        elif isinstance(config_dict, list):
            for item in config_dict:
                disable_sync_batchnorm_in_config(item)
    
    # ç¦ç”¨æ¨¡å‹é…ç½®ä¸­çš„SyncBatchNorm
    if hasattr(cfg, 'model') and cfg.model is not None:
        disable_sync_batchnorm_in_config(cfg.model)
        print("âœ… å·²ç¦ç”¨æ¨¡å‹é…ç½®ä¸­çš„SyncBatchNorm")
    
    # ç¦ç”¨å…¶ä»–å¯èƒ½çš„SyncBatchNormé…ç½®
    disable_sync_batchnorm_in_config(cfg._cfg_dict)
    print("âœ… SyncBatchNormç¦ç”¨å®Œæˆï¼Œç°åœ¨ä½¿ç”¨æ™®é€šBatchNormå…¼å®¹GCU")
    # ===== END: ç¦ç”¨SyncBatchNorm for GCUå…¼å®¹æ€§ =====
    
    runner = Runner.from_cfg(cfg)
    
    # éªŒè¯Runneråˆ›å»ºåçš„æ¨¡å‹è®¾å¤‡çŠ¶æ€
    if torch_gcu is not None and hasattr(runner, 'model'):
        print("ğŸ” éªŒè¯Runneråˆ›å»ºåçš„æ¨¡å‹è®¾å¤‡çŠ¶æ€...")
        
        # æ£€æŸ¥æ¨¡å‹å‚æ•°è®¾å¤‡
        model_devices = set()
        param_count = 0
        for name, param in runner.model.named_parameters():
            model_devices.add(str(param.device))
            param_count += 1
            if param_count >= 5:  # æ£€æŸ¥æ›´å¤šå‚æ•°ç¡®ä¿å‡†ç¡®æ€§
                break
        
        print("ğŸ” æ¨¡å‹è®¾å¤‡åˆ†å¸ƒ: {}".format(model_devices))
        print("ğŸ” æ£€æŸ¥äº† {} ä¸ªå‚æ•°".format(param_count))
        
        # å¦‚æœæ¨¡å‹åœ¨CPUä¸Šï¼Œä½¿ç”¨æ­£ç¡®çš„GCU APIç§»åŠ¨åˆ°è®¾å¤‡
        if any('cpu' in device_str for device_str in model_devices):
            print("ğŸ”§ æ¨¡å‹åœ¨CPUä¸Šï¼Œç§»åŠ¨åˆ°GCUè®¾å¤‡...")
            
            # è®¾ç½®å½“å‰GCUè®¾å¤‡
            torch_gcu.set_device(local_rank)
            
            # ä½¿ç”¨XLAè®¾å¤‡æ¥å£ç§»åŠ¨æ¨¡å‹åˆ°GCUè®¾å¤‡ï¼ˆT20æœåŠ¡å™¨æ ‡å‡†æ–¹å¼ï¼‰
            xla_device = f'xla:{local_rank}'
            runner.model = runner.model.to(xla_device)
            
            # å†æ¬¡éªŒè¯
            verification_devices = set()
            for name, param in runner.model.named_parameters():
                verification_devices.add(str(param.device))
                if len(verification_devices) >= 2:
                    break
            
            print("âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GCUè®¾å¤‡: {}".format(verification_devices))
        else:
            print("âœ… æ¨¡å‹å·²æ­£ç¡®é…ç½®åœ¨è®¾å¤‡ä¸Š: {}".format(model_devices))
    
    print("âœ… Runneråˆ›å»ºå®Œæˆï¼Œè®¾å¤‡é…ç½®éªŒè¯é€šè¿‡")
    
    runner.train()
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    if world_size > 1 and dist.is_initialized():
        dist.destroy_process_group()
        print("ğŸ§¹ åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²æ¸…ç†")

if __name__ == '__main__':
    main()