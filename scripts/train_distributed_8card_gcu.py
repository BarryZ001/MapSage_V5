#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - ç‡§åŸT20 GCUç‰ˆæœ¬
æ”¯æŒDINOv3 + MMRS-1Mæ•°æ®é›†çš„8å¡åˆ†å¸ƒå¼è®­ç»ƒ
"""

import argparse
import os
import sys
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from mmengine.config import Config
from mmengine.runner import Runner

# å°è¯•å¯¼å…¥GCUç›¸å…³åº“
try:
    import torch_gcu
    print("âœ… torch_gcuå¯¼å…¥æˆåŠŸï¼Œå¯ç”¨è®¾å¤‡æ•°: {}".format(torch_gcu.device_count()))
except ImportError as e:
    print("âš ï¸ torch_gcuå¯¼å…¥å¤±è´¥: {}".format(e))
    torch_gcu = None

try:
    import ptex
    print("âœ… ptexå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print("âš ï¸ ptexå¯¼å…¥å¤±è´¥: {}".format(e))
    ptex = None

# å°è¯•å¯¼å…¥MMSegç›¸å…³æ¨¡å—
try:
    import mmseg  # type: ignore
    from mmseg.models import *  # type: ignore
    from mmseg.datasets import *  # type: ignore
except ImportError as e:
    print("âš ï¸ æ¨¡å—å¯¼å…¥å¤±è´¥: {}".format(e))

# å°è¯•å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from mmseg_custom.models import *  # type: ignore
    from mmseg_custom.datasets import *  # type: ignore
    print("âœ… è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print("âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {}".format(e))

# å°è¯•å¯¼å…¥MMSegæ¨¡å‹ç»„ä»¶å¹¶æ³¨å†Œ
try:
    import mmseg
    import mmseg.models
    from mmseg.models.backbones import MixVisionTransformer
    from mmseg.models.decode_heads import SegformerHead
    from mmseg.models.segmentors import EncoderDecoder
    
    from mmengine.registry import MODELS
    if 'MixVisionTransformer' not in MODELS.module_dict:
        MODELS.register_module(name='MixVisionTransformer', module=MixVisionTransformer)
        print("âœ… MixVisionTransformerå·²æ³¨å†Œåˆ°MMEngine")
    
    if 'SegformerHead' not in MODELS.module_dict:
        MODELS.register_module(name='SegformerHead', module=SegformerHead)
        print("âœ… SegformerHeadå·²æ³¨å†Œåˆ°MMEngine")
        
    if 'EncoderDecoder' not in MODELS.module_dict:
        MODELS.register_module(name='EncoderDecoder', module=EncoderDecoder)
        print("âœ… EncoderDecoderå·²æ³¨å†Œåˆ°MMEngine")
        
    print("âœ… MMSegæ¨¡å‹ç»„ä»¶å¯¼å…¥å’Œæ³¨å†ŒæˆåŠŸ")
except ImportError as e:
    print("âš ï¸ MMSegå¯¼å…¥å¤±è´¥: {}".format(e))
    print("âš ï¸ å°†ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç»„ä»¶")

def main():
    parser = argparse.ArgumentParser(description='8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬')
    parser.add_argument('config', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', help='å·¥ä½œç›®å½•è·¯å¾„')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], 
                       default='pytorch', help='åˆ†å¸ƒå¼å¯åŠ¨å™¨')
    parser.add_argument('--local_rank', type=int, default=0, help='æœ¬åœ°è¿›ç¨‹rank')
    args = parser.parse_args()
    
    print("ğŸš€ å¯åŠ¨8å¡åˆ†å¸ƒå¼è®­ç»ƒ")
    print("ğŸ“„ é…ç½®æ–‡ä»¶: {}".format(args.config))
    print("ğŸ”§ å¯åŠ¨å™¨: {}".format(args.launcher))
    
    # 1. åŠ è½½é…ç½®æ–‡ä»¶
    cfg = Config.fromfile(args.config)
    
    # æ£€æŸ¥å¹¶åˆ›å»ºå·¥ä½œç›®å½•
    if args.work_dir:
        # ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„å·¥ä½œç›®å½•
        cfg.work_dir = args.work_dir
        if not os.path.exists(cfg.work_dir):
            os.makedirs(cfg.work_dir, exist_ok=True)
            print("ğŸ“ åˆ›å»ºå·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    elif hasattr(cfg, 'work_dir') and cfg.work_dir:
        if not os.path.exists(cfg.work_dir):
            os.makedirs(cfg.work_dir, exist_ok=True)
            print("ğŸ“ åˆ›å»ºå·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    else:
        # å¦‚æœé…ç½®æ–‡ä»¶æ²¡æœ‰work_dirï¼Œè®¾ç½®é»˜è®¤å€¼
        cfg.work_dir = './work_dirs/train_distributed_8card_gcu'
        os.makedirs(cfg.work_dir, exist_ok=True)
        print("ğŸ“ è®¾ç½®é»˜è®¤å·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    
    # è®¾ç½®æ—¥å¿—ç›®å½•
    log_dir = os.path.join(cfg.work_dir, 'logs')
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)
    
    # è·å–åˆ†å¸ƒå¼å‚æ•°
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    
    print("ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°:")
    print("  - WORLD_SIZE: {}".format(world_size))
    print("  - RANK: {}".format(rank))
    print("  - LOCAL_RANK: {}".format(local_rank))
    
    # é…ç½®åˆ†å¸ƒå¼è®­ç»ƒ
    if world_size > 1:
        cfg.launcher = args.launcher
        print("ğŸ”§ å¯ç”¨åˆ†å¸ƒå¼è®­ç»ƒï¼Œlauncher: {}".format(args.launcher))
        
        # é…ç½®ç¯å¢ƒå˜é‡
        if not hasattr(cfg, 'env_cfg'):
            cfg.env_cfg = {}
        if not hasattr(cfg.env_cfg, 'dist_cfg'):
            cfg.env_cfg.dist_cfg = {}
        
        # è®¾ç½®ECCLåç«¯é…ç½®
        cfg.env_cfg.dist_cfg['backend'] = 'eccl'
        cfg.env_cfg.dist_cfg['init_method'] = 'env://'
        print("âœ… é…ç½®MMEngineä½¿ç”¨ECCLåç«¯")
        
        # é…ç½®GCUè®¾å¤‡
        cfg.device = 'gcu'
        print("ğŸ”§ é…ç½®GCUè®¾å¤‡ï¼Œworld_size: {}".format(world_size))
    else:
        cfg.launcher = 'none'
        print("ğŸ”§ å•è¿›ç¨‹æ¨¡å¼ï¼Œç¦ç”¨åˆ†å¸ƒå¼")
        cfg.device = 'gcu'
        print("ğŸ”§ é…ç½®å•å¡GCUè®¾å¤‡")
    
    # è°ƒæ•´batch size
    if hasattr(cfg, 'train_dataloader') and 'batch_size' in cfg.train_dataloader:
        original_batch_size = cfg.train_dataloader.batch_size
        print("ğŸ“Š æ¯å¡batch size: {}".format(original_batch_size))
        print("ğŸ“Š æ€»batch size: {}".format(original_batch_size * world_size))
    
    print("ğŸ“ å·¥ä½œç›®å½•: {}".format(cfg.work_dir))
    print("ğŸš€ å¯åŠ¨è®­ç»ƒ - Rank {}/{}".format(rank, world_size))
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        torch_gcu.set_device(local_rank)
        print("ğŸ”§ è®¾ç½®å½“å‰è¿›ç¨‹GCUè®¾å¤‡: {}".format(local_rank))
        
        device = f"xla:{local_rank}"
        cfg.device = device
        
        # ç¦ç”¨CUDAç›¸å…³è®¾ç½®
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("ğŸ”§ é…ç½®è®¾å¤‡ä¸º: {}".format(device))
        
        # é…ç½®MMEngineä»¥æ­£ç¡®å¤„ç†GCUè®¾å¤‡
        # ç¦ç”¨device_idså‚æ•°ï¼Œè®©MMEngineè‡ªåŠ¨å¤„ç†è®¾å¤‡
        if hasattr(cfg, 'model_wrapper_cfg'):
            cfg.model_wrapper_cfg = dict(type='MMDistributedDataParallel', find_unused_parameters=False)
        else:
            cfg.model_wrapper_cfg = dict(type='MMDistributedDataParallel', find_unused_parameters=False)
        print("ğŸ”§ é…ç½®MMEngineæ¨¡å‹åŒ…è£…å™¨ï¼Œç¦ç”¨device_ids")
    
    # ç¦ç”¨SyncBatchNorm
    def disable_sync_batchnorm_in_config(config_dict):
        """é€’å½’ç¦ç”¨é…ç½®ä¸­çš„SyncBatchNorm"""
        if isinstance(config_dict, dict):
            for key, value in config_dict.items():
                if key == 'norm_cfg' and isinstance(value, dict):
                    if value.get('type') == 'SyncBN':
                        print(f"ğŸ”§ å‘ç°SyncBNé…ç½®ï¼Œæ›¿æ¢ä¸ºBN: {value}")
                        value['type'] = 'BN'
                        print(f"âœ… å·²æ›¿æ¢ä¸º: {value}")
                elif isinstance(value, (dict, list)):
                    disable_sync_batchnorm_in_config(value)
        elif isinstance(config_dict, list):
            for item in config_dict:
                disable_sync_batchnorm_in_config(item)
    
    if hasattr(cfg, 'model') and cfg.model is not None:
        disable_sync_batchnorm_in_config(cfg.model)
        print("âœ… å·²ç¦ç”¨æ¨¡å‹é…ç½®ä¸­çš„SyncBatchNorm")
    
    disable_sync_batchnorm_in_config(cfg._cfg_dict)
    print("âœ… SyncBatchNormç¦ç”¨å®Œæˆï¼Œç°åœ¨ä½¿ç”¨æ™®é€šBatchNormå…¼å®¹GCU")
    
    # 2. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ (ç»•è¿‡MMEngineçš„CUDAè°ƒç”¨ï¼Œç›´æ¥ä½¿ç”¨torch.distributed)
    def init_process_group_with_fallback(init_method='env://'):
        """å°è¯•å¤šç§backendåˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
        candidates = ['gloo', 'nccl']  # ç§»é™¤ecclï¼Œå› ä¸ºå®ƒä¸æ˜¯æ ‡å‡†çš„PyTorchåˆ†å¸ƒå¼åç«¯
        errors = {}
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨ECCLåŒ…è£…å™¨
        try:
            print("ğŸ”„ å°è¯•ä½¿ç”¨ECCLåŒ…è£…å™¨åˆå§‹åŒ–åˆ†å¸ƒå¼")
            # å¯¼å…¥ECCLåŒ…è£…å™¨
            import sys
            sys.path.append('/opt/tops/eccl/lib/python3.8/site-packages')
            import eccl
            
            # ä½¿ç”¨glooä½œä¸ºåŸºç¡€backendï¼Œä½†é€šè¿‡ECCLè¿›è¡Œé€šä¿¡
            dist.init_process_group(
                backend='gloo', 
                init_method=init_method,
                rank=int(os.environ.get('RANK', 0)),
                world_size=int(os.environ.get('WORLD_SIZE', 1))
            )
            print("âœ… åˆ†å¸ƒå¼åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨ECCLåŒ…è£…å™¨ + gloo backend")
            return 'eccl_gloo'
            
        except Exception as e:
            error_msg = f"ECCLåŒ…è£…å™¨å¤±è´¥: {type(e).__name__}: {e}"
            errors['eccl'] = error_msg
            print(f"âš ï¸ {error_msg}")
        
        for backend in candidates:
            try:
                print(f"ğŸ”„ å°è¯•åˆå§‹åŒ–åˆ†å¸ƒå¼backend: {backend}")
                
                # å°ä¼˜åŒ–ï¼šå¦‚æœå°è¯• ncclï¼Œåˆ™å…ˆæ£€æŸ¥æ˜¯å¦å¯ç”¨
                if backend == 'nccl' and not getattr(dist, "is_nccl_available", lambda: False)():
                    errors[backend] = "nccl not available"
                    print(f"âš ï¸ {backend}: ncclä¸å¯ç”¨ï¼Œè·³è¿‡")
                    continue
                
                dist.init_process_group(
                    backend=backend, 
                    init_method=init_method,
                    rank=int(os.environ.get('RANK', 0)),
                    world_size=int(os.environ.get('WORLD_SIZE', 1))
                )
                print(f"âœ… åˆ†å¸ƒå¼åˆå§‹åŒ–æˆåŠŸï¼Œä½¿ç”¨backend: {backend}")
                return backend
                
            except Exception as e:
                error_msg = f"{type(e).__name__}: {e}"
                errors[backend] = error_msg
                print(f"âŒ {backend} åˆå§‹åŒ–å¤±è´¥: {error_msg}")
                
                # æ¸…ç†å¤±è´¥çš„åˆå§‹åŒ–
                try:
                    if dist.is_initialized():
                        # ä½¿ç”¨torch_gcu.distributed.destroy_process_group
                        try:
                            import torch_gcu.distributed as gcu_dist
                            gcu_dist.destroy_process_group()
                            print("âœ… ä½¿ç”¨torch_gcu.distributed.destroy_process_groupæ¸…ç†å®Œæˆ")
                        except ImportError:
                            # å›é€€åˆ°æ ‡å‡†æ–¹æ³•
                            dist.destroy_process_group()
                            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„æ¸…ç†å®Œæˆ")
                except Exception:
                    pass
        
        # å…¨éƒ¨å¤±è´¥ -> æŠ›é”™å¹¶æ‰“å°è¯Šæ–­
        msg = ["âŒ æ‰€æœ‰åˆ†å¸ƒå¼backendåˆå§‹åŒ–å¤±è´¥:"]
        for b, e in errors.items():
            msg.append(f"  - {b}: {e}")
        msg.append(f"torch.distributed.is_available(): {dist.is_available()}")
        
        # å®‰å…¨æ£€æŸ¥torchæ¨¡å—
        try:
            import torch
            msg.append(f"torch.cuda.is_available(): {torch.cuda.is_available()}")
        except Exception as e:
            msg.append(f"torch.cudaæ£€æŸ¥å¤±è´¥: {e}")
        
        # æ£€æŸ¥torch_gcuçŠ¶æ€
        try:
            import torch_gcu
            msg.append(f"torch_gcu.device_count(): {torch_gcu.device_count()}")
        except ImportError:
            msg.append("torch_gcu: æœªå®‰è£…")
        
        raise RuntimeError("\n".join(msg))
    
    if cfg.get('launcher', 'none') == 'pytorch':
        # è·å–åˆ†å¸ƒå¼å‚æ•°
        rank = int(os.environ.get('RANK', 0))
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        world_size = int(os.environ.get('WORLD_SIZE', 1))
        
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡
        os.environ['MASTER_ADDR'] = os.environ.get('MASTER_ADDR', '127.0.0.1')
        os.environ['MASTER_PORT'] = os.environ.get('MASTER_PORT', '29500')
        
        # ä½¿ç”¨fallbacké€»è¾‘åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
        if not dist.is_initialized():
            init_method = f"tcp://{os.environ['MASTER_ADDR']}:{os.environ['MASTER_PORT']}"
            backend_used = init_process_group_with_fallback(init_method=init_method)
            print(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒå·²å¯åŠ¨ï¼Œä½¿ç”¨backend: {backend_used}")
            print(f"ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank {rank}/{world_size}, Backend: {dist.get_backend()}")
        else:
            print("ğŸ”§ åˆ†å¸ƒå¼ç¯å¢ƒå·²åˆå§‹åŒ–")
    
    # 3. åˆ›å»º Runner å®ä¾‹
    print("ğŸš€ åˆ›å»ºRunner...")
    
    # åœ¨åˆ›å»ºRunnerä¹‹å‰ï¼Œç¡®ä¿æ¨¡å‹ä¼šè¢«æ­£ç¡®ç§»åŠ¨åˆ°GCUè®¾å¤‡
    # é€šè¿‡è®¾ç½®ç¯å¢ƒå˜é‡æ¥ç¡®ä¿æ¨¡å‹åˆå§‹åŒ–æ—¶å°±åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    if torch_gcu is not None:
        # å¼ºåˆ¶æ¨¡å‹åœ¨GCUè®¾å¤‡ä¸Šåˆå§‹åŒ–
        import torch
        torch.set_default_tensor_type('torch.FloatTensor')  # ç¡®ä¿ä½¿ç”¨CPU tensorä½œä¸ºé»˜è®¤
        
        # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„GCU tensoræ¥ç¡®ä¿è®¾å¤‡å¯ç”¨
        try:
            test_tensor = torch.tensor([1.0]).to(f"xla:{local_rank}")
            print(f"âœ… GCUè®¾å¤‡ xla:{local_rank} å¯ç”¨ï¼Œæµ‹è¯•tensor: {test_tensor.device}")
        except Exception as e:
            print(f"âš ï¸ GCUè®¾å¤‡æµ‹è¯•å¤±è´¥: {e}")
    
    # å…³é”®ä¿®å¤ï¼šåœ¨åˆ›å»ºRunnerä¹‹å‰è®¾ç½®æ­£ç¡®çš„æ¨¡å‹åŒ…è£…å™¨é…ç½®
    print("ğŸ”§ é…ç½®MMEngineæ¨¡å‹åŒ…è£…å™¨ï¼Œå®Œå…¨ç¦ç”¨device_idså’Œoutput_device...")
    
    # å¼ºåˆ¶è®¾ç½®æ¨¡å‹åŒ…è£…å™¨é…ç½®ä¸ºNoneï¼Œè®©MMEngineä½¿ç”¨é»˜è®¤çš„DDPåŒ…è£…
    cfg.model_wrapper_cfg = dict(
        type='MMDistributedDataParallel',
        find_unused_parameters=False,
        broadcast_buffers=False,
        # å…³é”®ä¿®å¤ï¼šå®Œå…¨ä¸è®¾ç½®device_idså’Œoutput_deviceï¼Œè®©DDPè‡ªåŠ¨å¤„ç†
    )
    print("âœ… è®¾ç½®äº†å…¼å®¹GCUçš„model_wrapper_cfgé…ç½®")
    print(f"ğŸ” æœ€ç»ˆmodel_wrapper_cfg: {cfg.model_wrapper_cfg}")
    
    # åœ¨åˆ›å»ºRunnerä¹‹å‰ï¼Œé¢„å…ˆè®¾ç½®GCUè®¾å¤‡ç¯å¢ƒ
    if torch_gcu is not None:
        print(f"ğŸ”§ é¢„è®¾ç½®GCUè®¾å¤‡ç¯å¢ƒï¼Œlocal_rank: {local_rank}")
        torch_gcu.set_device(local_rank)
        
        # è®¾ç½®é»˜è®¤è®¾å¤‡ä¸ºå½“å‰GCUè®¾å¤‡
        import torch
        if hasattr(torch, 'set_default_device'):
            try:
                torch.set_default_device(f'xla:{local_rank}')
                print(f"âœ… è®¾ç½®é»˜è®¤è®¾å¤‡ä¸º: xla:{local_rank}")
            except:
                print("âš ï¸ æ— æ³•è®¾ç½®é»˜è®¤è®¾å¤‡ï¼Œç»§ç»­ä½¿ç”¨CPUåˆå§‹åŒ–")
    
    # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶è®¾ç½®æ¨¡å‹åˆå§‹åŒ–è®¾å¤‡
    print("ğŸ”§ å¼ºåˆ¶è®¾ç½®æ¨¡å‹åˆå§‹åŒ–åœ¨GCUè®¾å¤‡ä¸Š...")
    if torch_gcu is not None:
        # ä¸´æ—¶ä¿®æ”¹torchçš„é»˜è®¤tensorç±»å‹ï¼Œç¡®ä¿æ¨¡å‹å‚æ•°åœ¨GCUä¸Šåˆå§‹åŒ–
        original_default_tensor_type = torch.get_default_dtype()
        try:
            # åˆ›å»ºä¸€ä¸ªGCUä¸Šçš„tensorä½œä¸ºæ¨¡æ¿
            device_str = f'xla:{local_rank}'
            print(f"ğŸ”§ è®¾ç½®æ¨¡å‹åˆå§‹åŒ–è®¾å¤‡: {device_str}")
            
            # åœ¨é…ç½®ä¸­æ˜ç¡®æŒ‡å®šè®¾å¤‡
            cfg.device = device_str
            
            # åˆ›å»ºRunner
            runner = Runner.from_cfg(cfg)
            print("âœ… Runneråˆ›å»ºå®Œæˆ")
            
            # ç«‹å³æ£€æŸ¥å¹¶ç§»åŠ¨æ¨¡å‹åˆ°æ­£ç¡®è®¾å¤‡
            if hasattr(runner, 'model') and runner.model is not None:
                print("ğŸ”§ æ£€æŸ¥æ¨¡å‹è®¾å¤‡çŠ¶æ€...")
                
                # è·å–æ¨¡å‹å½“å‰è®¾å¤‡
                try:
                    current_device = next(runner.model.parameters()).device
                    print(f"ğŸ” æ¨¡å‹å½“å‰è®¾å¤‡: {current_device}")
                    
                    # å¦‚æœæ¨¡å‹ä¸åœ¨æ­£ç¡®çš„GCUè®¾å¤‡ä¸Šï¼Œå¼ºåˆ¶ç§»åŠ¨
                    if str(current_device) != device_str:
                        print(f"âš ï¸ æ¨¡å‹è®¾å¤‡ä¸åŒ¹é…ï¼Œä» {current_device} ç§»åŠ¨åˆ° {device_str}")
                        runner.model = runner.model.to(device_str)
                        print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device_str}")
                        
                        # å†æ¬¡éªŒè¯
                        new_device = next(runner.model.parameters()).device
                        print(f"ğŸ” ç§»åŠ¨åæ¨¡å‹è®¾å¤‡: {new_device}")
                    else:
                        print(f"âœ… æ¨¡å‹å·²åœ¨æ­£ç¡®è®¾å¤‡: {current_device}")
                        
                except Exception as e:
                    print(f"âš ï¸ æ£€æŸ¥æ¨¡å‹è®¾å¤‡æ—¶å‡ºé”™: {e}")
                    
        except Exception as e:
            print(f"âŒ è®¾ç½®æ¨¡å‹åˆå§‹åŒ–è®¾å¤‡å¤±è´¥: {e}")
            # å›é€€åˆ°é»˜è®¤åˆ›å»ºæ–¹å¼
            runner = Runner.from_cfg(cfg)
            print("âœ… Runneråˆ›å»ºå®Œæˆï¼ˆå›é€€æ¨¡å¼ï¼‰")
    else:
         runner = Runner.from_cfg(cfg)
         print("âœ… Runneråˆ›å»ºå®Œæˆ")
    
    # ===== START: æœ€ç»ˆä¿®å¤é€»è¾‘ (åœ¨Runneråˆ›å»ºåï¼Œè®­ç»ƒå¼€å§‹å‰) =====
    print("ğŸ”§ å¼€å§‹æ‰§è¡Œæœ€ç»ˆä¿®å¤é€»è¾‘...")
    
    # 3.1 å¼ºåˆ¶ä¿®æ­£åˆ†å¸ƒå¼åç«¯ä¸º ECCL
    if dist.is_initialized() and dist.get_backend() != 'eccl':
        print(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯åç«¯: {dist.get_backend()}ï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ° ECCL...")
        current_rank = dist.get_rank()
        current_world_size = dist.get_world_size()
        
        # é”€æ¯å½“å‰è¿›ç¨‹ç»„
        dist.destroy_process_group()
        print("ğŸ§¹ å·²é”€æ¯å½“å‰è¿›ç¨‹ç»„")
        
        # é‡æ–°åˆå§‹åŒ–ECCLåç«¯
        try:
            # è®¾ç½®ECCLç¯å¢ƒå˜é‡
            os.environ['ECCL_BACKEND'] = 'eccl'
            os.environ['ECCL_DEVICE_TYPE'] = 'gcu'
            
            dist.init_process_group(
                backend='eccl', 
                init_method='env://', 
                world_size=current_world_size, 
                rank=current_rank
            )
            print(f"âœ… æˆåŠŸåˆ‡æ¢åˆ° ECCL åç«¯")
        except Exception as e:
            print(f"âŒ ECCLåç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ å›é€€åˆ°glooåç«¯")
            dist.init_process_group(
                backend='gloo', 
                init_method='env://', 
                world_size=current_world_size, 
                rank=current_rank
            )
    elif dist.is_initialized():
        print(f"âœ… å½“å‰åç«¯å·²æ˜¯æ­£ç¡®çš„: {dist.get_backend()}")
    
    # 3.2 å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶å°†æ¨¡å‹ç§»åŠ¨åˆ°æ­£ç¡®çš„GCUè®¾å¤‡
    if torch_gcu is not None and hasattr(runner, 'model') and runner.model is not None:
        # è®¾ç½®GCUè®¾å¤‡
        torch_gcu.set_device(local_rank)
        device = f'xla:{local_rank}'
        
        print(f"ğŸ”§ å¼€å§‹å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
        
        # æ£€æŸ¥æ¨¡å‹å½“å‰è®¾å¤‡çŠ¶æ€
        try:
            current_device = next(runner.model.parameters()).device
            print(f"ğŸ” æ¨¡å‹å½“å‰è®¾å¤‡: {current_device}")
        except StopIteration:
            print("âš ï¸ æ¨¡å‹æ²¡æœ‰å‚æ•°ï¼Œè·³è¿‡è®¾å¤‡æ£€æŸ¥")
            current_device = None
        
        # å¼ºåˆ¶å°†æ¨¡å‹ç§»åŠ¨åˆ°GCUè®¾å¤‡
        try:
            runner.model = runner.model.to(device)
            print(f"âœ… æ¨¡å‹å·²å¼ºåˆ¶ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
            
            # éªŒè¯æ¨¡å‹è®¾å¤‡
            model_device = next(runner.model.parameters()).device
            print(f"ğŸ” éªŒè¯æ¨¡å‹è®¾å¤‡: {model_device}")
            
            # ç¡®ä¿æ‰€æœ‰å‚æ•°éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            device_count = {}
            for name, param in runner.model.named_parameters():
                param_device = str(param.device)
                device_count[param_device] = device_count.get(param_device, 0) + 1
            
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°è®¾å¤‡åˆ†å¸ƒ: {device_count}")
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è®¾å¤‡è¿ç§»å¤±è´¥: {e}")
            print(f"âŒ é”™è¯¯è¯¦æƒ…: {str(e)}")
            raise e
    
    # 3.3 è½¬æ¢SyncBatchNormå±‚ä»¥å…¼å®¹DDP
    if hasattr(runner, 'model') and runner.model is not None and world_size > 1:
        try:
            from mmengine.model import convert_sync_batchnorm
            runner.model = convert_sync_batchnorm(runner.model)
            print("ğŸ”§ SyncBatchNormå±‚å·²è½¬æ¢ä¸ºDDPå…¼å®¹")
        except Exception as e:
            print(f"âš ï¸ SyncBatchNormè½¬æ¢å¤±è´¥: {e}")
    
    # 3.4 å…³é”®ä¿®å¤ï¼šé‡æ–°ç”¨DDPåŒ…è£…æ¨¡å‹ï¼ˆä½¿ç”¨æ­£ç¡®çš„å‚æ•°ï¼‰
    if world_size > 1 and hasattr(runner, 'model') and runner.model is not None:
        try:
            from mmengine.model import MMDistributedDataParallel
            
            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»è¢«DDPåŒ…è£…
            if not isinstance(runner.model, MMDistributedDataParallel):
                print(f"ğŸ”§ å¼€å§‹DDPåŒ…è£…ï¼Œå½“å‰æ¨¡å‹ç±»å‹: {type(runner.model)}")
                
                # è·å–æ¨¡å‹å½“å‰è®¾å¤‡
                try:
                    model_device = next(runner.model.parameters()).device
                    print(f"ğŸ” DDPåŒ…è£…å‰æ¨¡å‹è®¾å¤‡: {model_device}")
                except StopIteration:
                    print("âš ï¸ æ¨¡å‹æ²¡æœ‰å‚æ•°")
                    model_device = None
                
                # å…³é”®ï¼šè®¾ç½®device_ids=Noneå’Œoutput_device=Noneä»¥é¿å…è®¾å¤‡ä¸åŒ¹é…é”™è¯¯
                # è¿™æ˜¯ä¿®å¤DDPè®¾å¤‡ä¸åŒ¹é…é”™è¯¯çš„æ ¸å¿ƒé€»è¾‘
                runner.model = MMDistributedDataParallel(
                    runner.model,
                    device_ids=None,  # å…³é”®ï¼šè®¾ä¸ºNoneè®©DDPä½¿ç”¨æ¨¡å‹å½“å‰è®¾å¤‡
                    output_device=None,  # å…³é”®ï¼šè®¾ä¸ºNoneé¿å…è®¾å¤‡å†²çª
                    find_unused_parameters=False,  # ä»é…ç½®æ–‡ä»¶è·å–
                    broadcast_buffers=False,  # ä»é…ç½®æ–‡ä»¶è·å–
                    # æ·»åŠ é¢å¤–çš„GCUå…¼å®¹æ€§é…ç½®
                    static_graph=False,  # ç¦ç”¨é™æ€å›¾ä¼˜åŒ–ï¼Œé¿å…GCUå…¼å®¹æ€§é—®é¢˜
                )
                print("âœ… æ¨¡å‹å·²åœ¨æ­£ç¡®çš„GCUè®¾å¤‡ä¸Šé‡æ–°åŒ…è£…ä¸ºDDP")
                
                # éªŒè¯DDPåŒ…è£…åçš„æ¨¡å‹è®¾å¤‡
                try:
                    model_device = next(runner.model.parameters()).device
                    print(f"ğŸ” DDPåŒ…è£…åæ¨¡å‹è®¾å¤‡: {model_device}")
                    
                    # æ£€æŸ¥DDPåŒ…è£…åçš„å‚æ•°è®¾å¤‡åˆ†å¸ƒ
                    device_count = {}
                    for name, param in runner.model.named_parameters():
                        param_device = str(param.device)
                        device_count[param_device] = device_count.get(param_device, 0) + 1
                    
                    print(f"ğŸ“Š DDPåŒ…è£…åå‚æ•°è®¾å¤‡åˆ†å¸ƒ: {device_count}")
                    
                except StopIteration:
                    print("âš ï¸ DDPåŒ…è£…åæ¨¡å‹æ²¡æœ‰å‚æ•°")
                    
            else:
                print("âœ… æ¨¡å‹å·²ç»æ˜¯DDPåŒ…è£…")
                # éªŒè¯å·²åŒ…è£…æ¨¡å‹çš„è®¾å¤‡
                try:
                    model_device = next(runner.model.parameters()).device
                    print(f"ğŸ” å·²åŒ…è£…DDPæ¨¡å‹è®¾å¤‡: {model_device}")
                except StopIteration:
                    print("âš ï¸ å·²åŒ…è£…DDPæ¨¡å‹æ²¡æœ‰å‚æ•°")
                    
        except Exception as e:
            print(f"âš ï¸ DDPåŒ…è£…å¤±è´¥: {e}")
            print(f"âš ï¸ é”™è¯¯è¯¦æƒ…: {str(e)}")
            print(f"âš ï¸ é”™è¯¯ç±»å‹: {type(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©è®­ç»ƒç»§ç»­è¿›è¡Œ
    
    # ===== END: æœ€ç»ˆä¿®å¤é€»è¾‘ =====
    
    # éªŒè¯æœ€ç»ˆçŠ¶æ€
    if dist.is_initialized():
        print(f"ğŸ” æœ€ç»ˆéªŒè¯ - åç«¯: {dist.get_backend()}, Rank: {dist.get_rank()}/{dist.get_world_size()}")
    
    if hasattr(runner, 'model') and runner.model is not None:
        model_device = next(runner.model.parameters()).device
        print(f"ğŸ” æœ€ç»ˆéªŒè¯ - æ¨¡å‹è®¾å¤‡: {model_device}")
    
    # 4. å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    runner.train()
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    if dist.is_initialized():
        print("ğŸ§¹ æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ...")
        dist.destroy_process_group()
        print("âœ… åˆ†å¸ƒå¼ç¯å¢ƒæ¸…ç†å®Œæˆ")

if __name__ == '__main__':
    main()