#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - ç‡§åŸT20 GCUæœ€ç»ˆä¿®æ­£ç‰ˆ V3
é€šè¿‡åœ¨Runneråˆå§‹åŒ–å‰æ³¨å…¥æ­£ç¡®é…ç½®ï¼Œå¼•å¯¼MMEngineä½¿ç”¨æ­£ç¡®çš„DDPå‚æ•°ã€‚
"""

import argparse
import os
import sys
import torch
import torch.distributed as dist

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from mmengine.config import Config
from mmengine.runner import Runner
from mmengine.model import convert_sync_batchnorm, MMDistributedDataParallel
from mmengine.registry import MODELS

try:
    import torch_gcu  # type: ignore
    print(f"âœ… torch_gcu å¯¼å…¥æˆåŠŸï¼Œè®¾å¤‡æ•°é‡: {torch_gcu.device_count()}")
except ImportError as e:
    print(f"âŒ é”™è¯¯: torch_gcu å¯¼å…¥å¤±è´¥: {e}")
    print("âš ï¸ è¯·ç¡®ä¿åœ¨ç‡§åŸT20ç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description='8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬-ç‡§åŸT20 GCUæœ€ç»ˆä¿®æ­£ç‰ˆ V3')
    parser.add_argument('config', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', help='å·¥ä½œç›®å½•è·¯å¾„')
    parser.add_argument('--launcher', choices=['pytorch'], default='pytorch', help='åˆ†å¸ƒå¼å¯åŠ¨å™¨')
    args = parser.parse_args()

    # --- æ­¥éª¤ 1: åŠ è½½é…ç½®æ–‡ä»¶ ---
    cfg = Config.fromfile(args.config)
    if args.work_dir:
        cfg.work_dir = args.work_dir
    if hasattr(cfg, 'work_dir') and cfg.work_dir:
        os.makedirs(cfg.work_dir, exist_ok=True)

    # --- æ­¥éª¤ 2: æ‰‹åŠ¨åˆå§‹åŒ– ECCL åˆ†å¸ƒå¼åç«¯ ---
    # è¿™æ˜¯æ‰€æœ‰åˆ†å¸ƒå¼æ“ä½œçš„ç¬¬ä¸€æ­¥ï¼Œå¹¶ä¸”å¿…é¡»åœ¨MMEngineçš„ä»»ä½•æ“ä½œä¹‹å‰å®Œæˆã€‚
    if 'RANK' in os.environ:
        dist.init_process_group(backend='eccl', init_method='env://')
        rank = dist.get_rank()
        local_rank = int(os.environ.get('LOCAL_RANK', rank))
        print(f"âœ… [Rank {rank}] ECCL åˆ†å¸ƒå¼åç«¯åˆå§‹åŒ–æˆåŠŸã€‚")
    else:
        local_rank = 0
        
    # --- æ­¥éª¤ 3: æ‰‹åŠ¨è®¾ç½®å½“å‰è®¾å¤‡ ---
    # å‘Šè¯‰torch_gcuå’ŒPyTorchå½“å‰è¿›ç¨‹åº”è¯¥ä½¿ç”¨å“ªå¼ å¡ã€‚
    torch_gcu.set_device(local_rank)
    device = f'gcu:{local_rank}'
    print(f"ğŸ”§ [Rank {local_rank}] å½“å‰è®¾å¤‡å·²è®¾ç½®ä¸º: {device}")
    
    # --- æ­¥éª¤ 4: æ‰‹åŠ¨æ„å»ºæ¨¡å‹å¹¶å®Œæˆæ‰€æœ‰é€‚é… ---
    # æˆ‘ä»¬è‡ªå·±æ„å»ºæ¨¡å‹ï¼Œä¸å†ä¾èµ– runner.from_cfg() çš„è‡ªåŠ¨æ„å»ºã€‚
    
    # 4.1 ä»é…ç½®å­—å…¸æ„å»ºæ¨¡å‹å®ä¾‹ (æ­¤æ—¶æ¨¡å‹åœ¨CPUä¸Š)
    model = MODELS.build(cfg.model)
    print(f"ğŸ”§ [Rank {local_rank}] æ¨¡å‹å·²ä»é…ç½®æ„å»º (ä½äºCPU)")

    # 4.2 å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„GCUè®¾å¤‡
    model.to(device)
    print(f"ğŸ”§ [Rank {local_rank}] æ¨¡å‹å·²ç§»åŠ¨åˆ°: {device}")

    # 4.3 è½¬æ¢SyncBatchNormå±‚ (å¦‚æœæ˜¯å¤šå¡è®­ç»ƒ)
    if dist.is_initialized():
        model = convert_sync_batchnorm(model)
        print(f"ğŸ”§ [Rank {local_rank}] æ¨¡å‹ä¸­çš„BatchNormå±‚å·²è½¬æ¢ä¸ºSyncBatchNorm")

    # 4.4 æ‰‹åŠ¨ç”¨DDPåŒ…è£…æ¨¡å‹ (å¦‚æœæ˜¯å¤šå¡è®­ç»ƒ)
    if dist.is_initialized():
        model = MMDistributedDataParallel(
            model,
            device_ids=None,
            output_device=None
        )
        print(f"âœ… [Rank {local_rank}] æ¨¡å‹å·²æˆåŠŸåŒ…è£…ä¸ºDDP")

    # --- æ­¥éª¤ 5: åˆ›å»º Runner å¹¶ä¼ å…¥å‡†å¤‡å¥½çš„æ¨¡å‹ ---
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸å†ä½¿ç”¨ Runner.from_cfg()ï¼Œè€Œæ˜¯ç›´æ¥åˆå§‹åŒ–Runnerï¼Œ
    # å¹¶å°†æˆ‘ä»¬æ‰‹åŠ¨å‡†å¤‡å¥½çš„ã€å®Œå…¨é…ç½®æ­£ç¡®çš„æ¨¡å‹ä½œä¸ºå‚æ•°ä¼ å…¥ã€‚
    runner = Runner(
        model=model,
        work_dir=cfg.work_dir,
        train_dataloader=cfg.train_dataloader,
        val_dataloader=cfg.val_dataloader,
        val_evaluator=cfg.val_evaluator,
        train_cfg=cfg.train_cfg,
        val_cfg=cfg.val_cfg,
        test_cfg=cfg.test_cfg,
        optim_wrapper=cfg.optim_wrapper,
        param_scheduler=cfg.param_scheduler,
        default_hooks=cfg.default_hooks,
        env_cfg=cfg.env_cfg,
        visualizer=cfg.visualizer,
        log_processor=cfg.log_processor,
        launcher=args.launcher
        # ... å…¶ä»–ä½ éœ€è¦çš„å‚æ•° ...
    )
    
    # --- æ­¥éª¤ 6: å¼€å§‹è®­ç»ƒ ---
    print(f"ğŸ‰ [Rank {local_rank}] æ‰€æœ‰å‡†å¤‡å·¥ä½œå®Œæˆï¼Œå³å°†å¼€å§‹è®­ç»ƒï¼")
    runner.train()

    # --- æ­¥éª¤ 7: æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ ---
    if dist.is_initialized():
        dist.destroy_process_group()

if __name__ == '__main__':
    main()