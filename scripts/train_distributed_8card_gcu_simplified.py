#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - ç‡§åŸT20 GCUæœ€ç»ˆä¿®æ­£ç‰ˆ
é›†æˆäº†æ‰€æœ‰å¿…è¦çš„ä¿®å¤ï¼Œä»¥ç¡®ä¿åœ¨T20ç¡¬ä»¶ä¸Šç¨³å®šè¿è¡Œ
åŸºäºç”¨æˆ·æä¾›çš„ç®€æ´ç‰ˆæœ¬ä¼˜åŒ–
"""

import argparse
import os
import sys
import torch
import torch.distributed as dist

# --------------------------------------------------------------------
# è„šæœ¬åˆå§‹åŒ–
# --------------------------------------------------------------------
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„ï¼Œç¡®ä¿è‡ªå®šä¹‰æ¨¡å—èƒ½è¢«æ‰¾åˆ°
sys.path.insert(0, '.')

# å¯¼å…¥æ¡†æ¶å’ŒGCUç›¸å…³åº“
from mmengine.config import Config
from mmengine.runner import Runner
from mmengine.model import convert_sync_batchnorm, MMDistributedDataParallel

try:
    import torch_gcu
    print(f"âœ… torch_gcu å¯¼å…¥æˆåŠŸï¼Œè®¾å¤‡æ•°é‡: {torch_gcu.device_count()}")
except ImportError as e:
    print(f"âŒ é”™è¯¯: torch_gcu å¯¼å…¥å¤±è´¥: {e}")
    print("âš ï¸ å°†ä½¿ç”¨CPUæ¨¡å¼è¿è¡Œ")
    torch_gcu = None

# --------------------------------------------------------------------
# ä¸»å‡½æ•°
# --------------------------------------------------------------------
def main():
    parser = argparse.ArgumentParser(description='8å¡åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬-ç‡§åŸT20 GCUæœ€ç»ˆä¿®æ­£ç‰ˆ')
    parser.add_argument('config', help='è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--work-dir', help='å·¥ä½œç›®å½•è·¯å¾„ï¼Œå¦‚æœæŒ‡å®šå°†è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], 
                       default='pytorch', help='åˆ†å¸ƒå¼å¯åŠ¨å™¨')
    # torchrun ä¼šè‡ªåŠ¨æä¾› LOCAL_RANKï¼Œè¿™é‡Œä¿ç•™ä»¥å…¼å®¹æ—§ç”¨æ³•
    parser.add_argument('--local_rank', type=int, default=0)
    args = parser.parse_args()

    # --- æ­¥éª¤ 1: åŠ è½½é…ç½®æ–‡ä»¶ ---
    print("ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶...")
    cfg = Config.fromfile(args.config)
    if args.work_dir is not None:
        cfg.work_dir = args.work_dir
    
    # ç¡®ä¿å·¥ä½œç›®å½•å­˜åœ¨
    if hasattr(cfg, 'work_dir') and cfg.work_dir:
        os.makedirs(cfg.work_dir, exist_ok=True)
        print(f"ğŸ“ å·¥ä½œç›®å½•: {cfg.work_dir}")

    # --- æ­¥éª¤ 2: å¼ºåˆ¶åˆå§‹åŒ– ECCL åˆ†å¸ƒå¼åç«¯ (è¿™æ˜¯ç¬¬ä¸€ä¸ªå…³é”®ä¿®å¤) ---
    # å¿…é¡»åœ¨ MMEngine Runner åˆå§‹åŒ–ä¹‹å‰ï¼Œæ‰‹åŠ¨å»ºç«‹æ­£ç¡®çš„åˆ†å¸ƒå¼é€šä¿¡åç«¯
    rank = 0
    world_size = 1
    local_rank = 0
    
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ['RANK'])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ.get('LOCAL_RANK', 0))
        
        print(f"ğŸŒ åˆ†å¸ƒå¼ç¯å¢ƒæ£€æµ‹:")
        print(f"   - RANK: {rank}")
        print(f"   - WORLD_SIZE: {world_size}")
        print(f"   - LOCAL_RANK: {local_rank}")
        
        if not dist.is_initialized():
            print(f"ğŸ”§ [Rank {rank}] åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ï¼Œä½¿ç”¨ç‡§åŸå®˜æ–¹æ¨èçš„ 'eccl' åç«¯...")
            try:
                # è®¾ç½®ECCLç¯å¢ƒå˜é‡
                os.environ['ECCL_BACKEND'] = 'eccl'
                os.environ['ECCL_DEVICE_TYPE'] = 'gcu'
                
                dist.init_process_group(backend='eccl', init_method='env://')
                print(f"âœ… [Rank {rank}] ECCL åç«¯åˆå§‹åŒ–æˆåŠŸ")
                
                # éªŒè¯åç«¯
                actual_backend = dist.get_backend()
                if actual_backend != 'eccl':
                    print(f"âš ï¸ åç«¯éªŒè¯å¤±è´¥: æœŸæœ› 'eccl'ï¼Œå®é™… '{actual_backend}'")
                    
            except Exception as e:
                print(f"âŒ ECCL åç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•ä½¿ç”¨ gloo åç«¯ä½œä¸ºå¤‡é€‰...")
                try:
                    dist.init_process_group(backend='gloo', init_method='env://')
                    print(f"âœ… [Rank {rank}] gloo åç«¯åˆå§‹åŒ–æˆåŠŸ")
                except Exception as e2:
                    print(f"âŒ gloo åç«¯ä¹Ÿå¤±è´¥: {e2}")
                    raise
        else:
            current_backend = dist.get_backend()
            print(f"âœ… [Rank {rank}] åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²åˆå§‹åŒ–ï¼Œåç«¯: {current_backend}")
    else:
        print("âš ï¸ æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼ç¯å¢ƒå˜é‡ï¼Œå°†ä»¥å•å¡æ¨¡å¼è¿è¡Œã€‚")

    # --- æ­¥éª¤ 3: é…ç½®MMEngineä½¿ç”¨æ­£ç¡®çš„åˆ†å¸ƒå¼è®¾ç½® ---
    if world_size > 1:
        cfg.launcher = args.launcher
        
        # é…ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        if not hasattr(cfg, 'env_cfg'):
            cfg.env_cfg = {}
        if not hasattr(cfg.env_cfg, 'dist_cfg'):
            cfg.env_cfg.dist_cfg = {}
        
        # æ ¹æ®å®é™…åˆå§‹åŒ–çš„åç«¯è®¾ç½®é…ç½®
        if dist.is_initialized():
            actual_backend = dist.get_backend()
            cfg.env_cfg.dist_cfg['backend'] = actual_backend
            cfg.env_cfg.dist_cfg['init_method'] = 'env://'
            print(f"ğŸ”§ é…ç½®MMEngineä½¿ç”¨ {actual_backend} åç«¯")
    
    # é…ç½®è®¾å¤‡
    if torch_gcu is not None:
        cfg.device = f'gcu:{local_rank}'
        print(f"ğŸ”§ é…ç½®è®¾å¤‡: {cfg.device}")
        
        # å…³é”®ä¿®å¤ï¼šåœ¨Runneråˆ›å»ºå‰è®¾ç½®å½“å‰GCUè®¾å¤‡
        torch_gcu.set_device(local_rank)
        print(f"ğŸ”§ [Rank {rank}] é¢„è®¾å½“å‰GCUè®¾å¤‡: gcu:{local_rank}")
        
        # æ³¨æ„ï¼šä¸ä½¿ç”¨torch.cuda.set_deviceï¼Œå› ä¸ºT20ç¯å¢ƒæ²¡æœ‰NVIDIAé©±åŠ¨
        # åªä½¿ç”¨GCUç‰¹å®šçš„è®¾å¤‡è®¾ç½®
        print(f"ğŸ”§ [Rank {rank}] ä½¿ç”¨GCUè®¾å¤‡è®¾ç½®ï¼Œè·³è¿‡CUDAè°ƒç”¨")
        
    else:
        cfg.device = 'cpu'
        print("ğŸ”§ é…ç½®è®¾å¤‡: CPU")

    # --- æ­¥éª¤ 4: åˆ›å»º MMEngine Runner ---
    print("ğŸš€ åˆ›å»º MMEngine Runner...")
    runner = Runner.from_cfg(cfg)
    print("âœ… Runner åˆ›å»ºæˆåŠŸ")

    # ===== START: FINAL FIX LOGIC (åŸºäºç”¨æˆ·æä¾›çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ) =====
    
    if torch_gcu is not None and hasattr(runner, 'model') and runner.model is not None:
        # 1. Get the local rank and set the current device for this process
        device = f'gcu:{local_rank}'
        torch_gcu.set_device(local_rank)
        print(f"ğŸ”§ [Rank {rank}] è®¾ç½®å½“å‰è®¾å¤‡ä¸º: {device}")

        # 2. Force the model onto the correct GCU
        print(f"ğŸ”§ [Rank {rank}] å¼ºåˆ¶å°†æ¨¡å‹ç§»åŠ¨åˆ°GCUè®¾å¤‡...")
        runner.model.to(device)
        
        # éªŒè¯æ¨¡å‹è®¾å¤‡
        model_device = next(runner.model.parameters()).device
        print(f"âœ… [Rank {rank}] æ¨¡å‹ç°åœ¨ä½äºè®¾å¤‡: {model_device}")

        # 3. Convert BatchNorm layers to be DDP-compatible
        if world_size > 1:
            print(f"ğŸ”§ [Rank {rank}] è½¬æ¢BatchNormå±‚ä¸ºSyncBatchNorm...")
            runner.model = convert_sync_batchnorm(runner.model)
            print(f"âœ… [Rank {rank}] BatchNormå±‚è½¬æ¢å®Œæˆ")

            # 4. Manually re-wrap the model with the correct settings
            print(f"ğŸ”§ [Rank {rank}] æ‰‹åŠ¨é‡æ–°åŒ…è£…æ¨¡å‹ä¸ºDDP...")
            runner.model = MMDistributedDataParallel(
                runner.model,
                device_ids=None,  # Critical: Set to None to use the current device
                output_device=None  # Critical: Also set to None
            )
            print(f"âœ… [Rank {rank}] æ¨¡å‹å·²æˆåŠŸåŒ…è£…ä¸ºMMDistributedDataParallel")
        else:
            print(f"âœ… [Rank {rank}] å•å¡è®­ç»ƒï¼Œè·³è¿‡DDPåŒ…è£…")
    else:
        print("âš ï¸ è·³è¿‡GCUè®¾å¤‡é…ç½®ï¼ˆtorch_gcuä¸å¯ç”¨æˆ–æ¨¡å‹ä¸ºç©ºï¼‰")

    # ===== END: FINAL FIX LOGIC =====
    
    # --- æ­¥éª¤ 6: æœ€ç»ˆéªŒè¯ ---
    if dist.is_initialized():
        print(f"ğŸ” æœ€ç»ˆéªŒè¯ - åç«¯: {dist.get_backend()}, Rank: {dist.get_rank()}/{dist.get_world_size()}")
    
    if hasattr(runner, 'model') and runner.model is not None:
        model_device = next(runner.model.parameters()).device
        print(f"ğŸ” æœ€ç»ˆéªŒè¯ - æ¨¡å‹è®¾å¤‡: {model_device}")
        print(f"ğŸ” æœ€ç»ˆéªŒè¯ - æ¨¡å‹ç±»å‹: {type(runner.model).__name__}")
    
    # --- æ­¥éª¤ 7: å¼€å§‹è®­ç»ƒ ---
    # æ­¤æ—¶ runner.model å·²ç»æ˜¯å®Œå…¨é…ç½®æ­£ç¡®çš„DDPæ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥å¼€å§‹è®­ç»ƒ
    print(f"ğŸ‰ [Rank {rank}] æ‰€æœ‰å‡†å¤‡å·¥ä½œå®Œæˆï¼Œå¼€å§‹è®­ç»ƒï¼")
    runner.train()

    # --- æ­¥éª¤ 8: æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ ---
    if dist.is_initialized():
        dist.destroy_process_group()
        print(f"ğŸ§¹ [Rank {rank}] åˆ†å¸ƒå¼è¿›ç¨‹ç»„å·²æ¸…ç†")

# --------------------------------------------------------------------
# è„šæœ¬å…¥å£
# --------------------------------------------------------------------
if __name__ == '__main__':
    main()