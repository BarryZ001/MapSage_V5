# scripts/train_distributed.py - åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬

import argparse
import os
import sys
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from mmengine.config import Config
from mmengine.runner import Runner
from mmengine.dist import init_dist

# å¯¼å…¥GCUæ”¯æŒ
try:
    import torch_gcu
    print(f"âœ… torch_gcuå¯¼å…¥æˆåŠŸï¼Œå¯ç”¨è®¾å¤‡æ•°: {torch_gcu.device_count()}")
except ImportError as e:
    print(f"âš ï¸ torch_gcuå¯¼å…¥å¤±è´¥: {e}")
    torch_gcu = None

# å¯¼å…¥mmsegæ¥è§¦å‘æ‰€æœ‰æ³¨å†Œ
try:
    import mmseg  # type: ignore
    from mmseg.models import *  # type: ignore
    from mmseg.datasets import *  # type: ignore
except ImportError as e:
    print(f"âš ï¸ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥æˆ‘ä»¬çš„è‡ªå®šä¹‰æ¨¡å—
try:
    import mmseg_custom.models
    import mmseg_custom.datasets
    print("âœ… è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # è·å–ç¯å¢ƒå˜é‡
    world_size = int(os.environ.get('WORLD_SIZE', 8))  # é»˜è®¤8ä¸ªGCU
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    
    print(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®: world_size={world_size}, rank={rank}, local_rank={local_rank}")
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        # è®¾ç½®å½“å‰è¿›ç¨‹ä½¿ç”¨çš„GCUè®¾å¤‡
        torch_gcu.set_device(local_rank)
        print(f"ğŸ”§ è®¾ç½®GCUè®¾å¤‡: {local_rank}")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ç¡®ä¿ä½¿ç”¨GCU
        os.environ['TOPS_VISIBLE_DEVICES'] = str(local_rank)
        print(f"ğŸ”§ è®¾ç½®TOPS_VISIBLE_DEVICES: {local_rank}")
        
        # ç¦ç”¨CUDAç›¸å…³çš„ç¯å¢ƒå˜é‡
        os.environ['CUDA_VISIBLE_DEVICES'] = ''
        print("ğŸ”§ ç¦ç”¨CUDA_VISIBLE_DEVICES")
    else:
        print("âš ï¸ torch_gcuä¸å¯ç”¨ï¼Œå¯èƒ½ä¼šä½¿ç”¨CPUè®­ç»ƒ")
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ - ä½¿ç”¨è‡ªå®šä¹‰æ–¹å¼é¿å…CUDAè°ƒç”¨
    if not dist.is_initialized():
        # è®¾ç½®åˆ†å¸ƒå¼åç«¯ä¸ºglooï¼Œé¿å…NCCL
        os.environ['MASTER_ADDR'] = '127.0.0.1'
        os.environ['MASTER_PORT'] = '29500'
        
        # ç›´æ¥ä½¿ç”¨torch.distributed.init_process_groupè€Œä¸æ˜¯MMEngineçš„init_dist
        dist.init_process_group(
            backend='eccl',  # ä½¿ç”¨ECCLåç«¯ï¼Œæ”¯æŒGCU
            rank=rank,
            world_size=world_size
        )
        print(f"âœ… åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank {rank}/{world_size}")
    
    return rank, local_rank, world_size

def main():
    parser = argparse.ArgumentParser(description='MMSegmentation distributed training script')
    parser.add_argument('config', help='train config file path')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], default='pytorch', help='job launcher')
    args = parser.parse_args()

    print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–åˆ†å¸ƒå¼MMSegmentationè®­ç»ƒ...")
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, local_rank, world_size = setup_distributed()
    
    # ä»æ–‡ä»¶åŠ è½½é…ç½®
    if not os.path.exists(args.config):
        raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
    
    print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    cfg = Config.fromfile(args.config)
    
    # è®¾ç½®å·¥ä½œç›®å½•
    if cfg.get('work_dir', None) is None:
        cfg.work_dir = './work_dirs'
    
    # æ›´æ–°é…ç½®ä»¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
    cfg.launcher = args.launcher
    
    # è°ƒæ•´batch sizeï¼ˆæ¯ä¸ªè¿›ç¨‹çš„batch sizeï¼‰
    if hasattr(cfg, 'train_dataloader') and 'batch_size' in cfg.train_dataloader:
        original_batch_size = cfg.train_dataloader.batch_size
        # ä¿æŒæ€»batch sizeä¸å˜ï¼Œåˆ†é…åˆ°å„ä¸ªè¿›ç¨‹
        cfg.train_dataloader.batch_size = max(1, original_batch_size // world_size)
        print(f"ğŸ“Š è°ƒæ•´batch size: {original_batch_size} -> {cfg.train_dataloader.batch_size} (per process)")
    
    print(f"ğŸ“ å·¥ä½œç›®å½•: {cfg.work_dir}")
    print(f"ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ - Rank {rank}/{world_size}")
    
    # åˆ›å»ºRunnerå¹¶å¼€å§‹è®­ç»ƒ
    runner = Runner.from_cfg(cfg)
    runner.train()

if __name__ == '__main__':
    main()