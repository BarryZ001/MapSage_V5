#!/bin/bash
# scripts/start_distributed_training_simple.sh - ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

set -e

# é…ç½®å‚æ•°
CONFIG_FILE="configs/train_dinov3_mmrs1m_t20_gcu.py"
NPROC_PER_NODE=8  # 8ä¸ªGCU
MASTER_PORT=29500

echo "ğŸš€ å¯åŠ¨DINOv3åˆ†å¸ƒå¼è®­ç»ƒ - ä½¿ç”¨${NPROC_PER_NODE}ä¸ªGCU"
echo "ğŸ“ é…ç½®æ–‡ä»¶: ${CONFIG_FILE}"

# è®¾ç½®ç¯å¢ƒå˜é‡
export TORCH_DEVICE=xla
export XLA_USE_BF16=1
export MMENGINE_DEVICE=xla
export CUDA_VISIBLE_DEVICES=""

# ä½¿ç”¨torchrunå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
python3 -m torch.distributed.run \
    --nproc_per_node=${NPROC_PER_NODE} \
    --master_port=${MASTER_PORT} \
    scripts/train_distributed.py ${CONFIG_FILE} \
    --launcher pytorch

echo "ğŸ‰ åˆ†å¸ƒå¼è®­ç»ƒå·²å¯åŠ¨"