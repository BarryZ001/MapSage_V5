#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import argparse
import os
import sys
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, '.')

from mmengine.config import Config
from mmengine.runner import Runner

# å°è¯•å¯¼å…¥torch_gcuå’Œptex
try:
    import torch_gcu
    import torch_gcu.distributed as gcu_dist  # å¯¼å…¥torch_gcuåˆ†å¸ƒå¼æ¨¡å—
    print(f"âœ… torch_gcuå¯¼å…¥æˆåŠŸï¼Œå¯ç”¨è®¾å¤‡æ•°: {torch_gcu.device_count()}")
    print("âœ… torch_gcu.distributedæ¨¡å—å¯¼å…¥æˆåŠŸ")
    USE_GCU_DISTRIBUTED = True
except ImportError as e:
    print(f"âš ï¸ torch_gcuå¯¼å…¥å¤±è´¥: {e}")
    torch_gcu = None
    gcu_dist = None
    USE_GCU_DISTRIBUTED = False

try:
    import ptex
    print("âœ… ptexå¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ ptexå¯¼å…¥å¤±è´¥: {e}")
    ptex = None

# å¯¼å…¥MMSegç›¸å…³æ¨¡å—
try:
    import mmseg  # type: ignore
    from mmseg.models import *  # type: ignore
    from mmseg.datasets import *  # type: ignore
except ImportError as e:
    print(f"âš ï¸ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    import mmseg_custom.models
    import mmseg_custom.datasets
    print("âœ… è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âš ï¸ è‡ªå®šä¹‰æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")

def setup_distributed():
    """è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    # è·å–åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    rank = int(os.environ.get('RANK', 0))
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = int(os.environ.get('WORLD_SIZE', 1))
    
    print(f"ğŸŒ åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®: world_size={world_size}, rank={rank}, local_rank={local_rank}")
    
    # å¦‚æœæ˜¯å¤šè¿›ç¨‹åˆ†å¸ƒå¼è®­ç»ƒï¼Œåˆå§‹åŒ–è¿›ç¨‹ç»„
    if world_size > 1:
        # è®¾ç½®åˆ†å¸ƒå¼åç«¯ - ç»Ÿä¸€ä½¿ç”¨glooåç«¯
        os.environ['MMENGINE_DDP_BACKEND'] = 'gloo'
        print("ğŸ”§ è®¾ç½®MMEngine DDPåç«¯ä¸ºgloo")
        
        # åˆå§‹åŒ–åˆ†å¸ƒå¼è¿›ç¨‹ç»„ - ç»Ÿä¸€ä½¿ç”¨glooåç«¯
        if not dist.is_initialized():
            backend = 'gloo'
            dist.init_process_group(
                backend=backend,
                init_method=f"tcp://{os.environ.get('MASTER_ADDR', '127.0.0.1')}:{os.environ.get('MASTER_PORT', '29500')}",
                world_size=world_size,
                rank=rank
            )
            print(f"âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„åˆå§‹åŒ–å®Œæˆ - Backend: {backend}")
    
    # è®¾ç½®GCUè®¾å¤‡
    if torch_gcu is not None:
        try:
            torch_gcu.set_device(local_rank)
            print(f"ğŸ”§ è®¾ç½®GCUè®¾å¤‡: {local_rank}")
        except Exception as e:
            print(f"âš ï¸ è®¾ç½®GCUè®¾å¤‡å¤±è´¥: {e}")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['TOPS_VISIBLE_DEVICES'] = str(local_rank)
    print(f"ğŸ”§ è®¾ç½®TOPS_VISIBLE_DEVICES: {local_rank}")
    
    # ç¦ç”¨CUDA
    os.environ['CUDA_VISIBLE_DEVICES'] = ''
    print("ğŸ”§ ç¦ç”¨CUDA_VISIBLE_DEVICES")
    
    print(f"âœ… åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–å®Œæˆ - Rank {rank}/{world_size}")
    return rank, local_rank, world_size

def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        if USE_GCU_DISTRIBUTED and gcu_dist is not None:
            # ä½¿ç”¨torch_gcu.distributed.destroy_process_group
            try:
                gcu_dist.destroy_process_group()
                print("âœ… ä½¿ç”¨torch_gcu.distributed.destroy_process_groupæ¸…ç†å®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ torch_gcuåˆ†å¸ƒå¼æ¸…ç†å¤±è´¥: {e}")
                # å›é€€åˆ°æ ‡å‡†æ–¹æ³•
                dist.destroy_process_group()
        else:
            dist.destroy_process_group()
            print("âœ… åˆ†å¸ƒå¼è¿›ç¨‹ç»„æ¸…ç†å®Œæˆ")

def main():
    parser = argparse.ArgumentParser(description='MMSegmentation distributed training script for GCU')
    parser.add_argument('config', help='train config file path')
    parser.add_argument('--launcher', choices=['none', 'pytorch', 'slurm', 'mpi'], default='pytorch', help='job launcher')
    args = parser.parse_args()

    print("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–åˆ†å¸ƒå¼MMSegmentationè®­ç»ƒ...")
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, local_rank, world_size = setup_distributed()
    
    try:
        # ä»æ–‡ä»¶åŠ è½½é…ç½®
        if not os.path.exists(args.config):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        
        print(f"ğŸ“ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        cfg = Config.fromfile(args.config)
        
        # è®¾ç½®å·¥ä½œç›®å½•
        if cfg.get('work_dir', None) is None:
            cfg.work_dir = './work_dirs'
        
        # è®¾ç½®GCUè®¾å¤‡
        device_str = None
        if ptex is not None:
            try:
                device_str = "xla"
                print(f"ğŸ”§ è®¾ç½®GCUè®¾å¤‡ä¸º: {device_str}")
            except Exception as e:
                print(f"âš ï¸ è®¾ç½®ptexè®¾å¤‡å¤±è´¥: {e}")
        elif torch_gcu is not None:
            try:
                device_str = f"gcu:{local_rank}"
                print(f"ğŸ”§ è®¾ç½®GCUè®¾å¤‡ä¸º: {device_str}")
            except Exception as e:
                print(f"âš ï¸ è®¾ç½®torch_gcuè®¾å¤‡å¤±è´¥: {e}")
        
        # æ›´æ–°é…ç½®ä»¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
        if world_size > 1:
            cfg.launcher = args.launcher
        else:
            cfg.launcher = 'none'
            print("ğŸ”§ å•è¿›ç¨‹æ¨¡å¼ï¼Œç¦ç”¨åˆ†å¸ƒå¼")
        
        # è°ƒæ•´batch sizeï¼ˆæ¯ä¸ªè¿›ç¨‹çš„batch sizeï¼‰
        if hasattr(cfg, 'train_dataloader') and 'batch_size' in cfg.train_dataloader:
            original_batch_size = cfg.train_dataloader.batch_size
            # ä¿æŒæ€»batch sizeä¸å˜ï¼Œåˆ†é…åˆ°å„ä¸ªè¿›ç¨‹
            cfg.train_dataloader.batch_size = max(1, original_batch_size // world_size)
            print(f"ğŸ“Š è°ƒæ•´batch size: {original_batch_size} -> {cfg.train_dataloader.batch_size} (per process)")
        
        print(f"ğŸ“ å·¥ä½œç›®å½•: {cfg.work_dir}")
        print(f"ğŸš€ å¯åŠ¨è®­ç»ƒ - Rank {rank}/{world_size}")
        
        # åˆ›å»ºRunner
        runner = Runner.from_cfg(cfg)
        
        # åœ¨Runneråˆ›å»ºåï¼Œæ‰‹åŠ¨å°†æ¨¡å‹ç§»åŠ¨åˆ°GCUè®¾å¤‡
        if device_str is not None and hasattr(runner, 'model'):
            print(f"ğŸ”§ å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡: {device_str}")
            try:
                if ptex is not None:
                    # ä½¿ç”¨ptexè®¾å¤‡
                    device = ptex.device("xla")
                    runner.model = runner.model.to(device)
                    print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°ptexè®¾å¤‡: {device}")
                elif torch_gcu is not None:
                    # ä½¿ç”¨torch_gcuè®¾å¤‡
                    device = torch_gcu.device(local_rank)
                    runner.model = runner.model.to(device)
                    print(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GCUè®¾å¤‡: {device}")
            except Exception as e:
                print(f"âš ï¸ ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡å¤±è´¥: {e}")
                print("ğŸ”„ å°è¯•ä½¿ç”¨CPUè®­ç»ƒ")
        
        runner.train()
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        cleanup_distributed()

if __name__ == '__main__':
    main()