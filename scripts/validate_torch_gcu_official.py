#!/usr/bin/env python3
"""
åŸºäºç‡§åŸå®˜æ–¹æ–‡æ¡£çš„torch_gcuç¯å¢ƒéªŒè¯è„šæœ¬
å®˜æ–¹æ–‡æ¡£: https://support.enflame-tech.com/onlinedoc_dev_3.5/3-model/infer/torch_gcu/torch_gcu2.5/content/source/torch_gcu_user_guide.html#id4

æ­¤è„šæœ¬è®¾è®¡ä¸ºåœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½è¿è¡Œï¼ŒåŒ…æ‹¬æ²¡æœ‰torch_gcuçš„ç¯å¢ƒ
"""

import sys
import os
import torch
import subprocess
from typing import Dict, Any, Optional

def print_section(title: str):
    """æ‰“å°ç« èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*60}")
    print(f" {title}")
    print(f"{'='*60}")

def print_result(item: str, status: str, details: str = ""):
    """æ‰“å°æ£€æŸ¥ç»“æœ"""
    status_symbol = "âœ…" if status == "PASS" else "âŒ" if status == "FAIL" else "âš ï¸"
    print(f"{status_symbol} {item}: {status}")
    if details:
        print(f"   è¯¦æƒ…: {details}")

def check_basic_environment() -> Dict[str, Any]:
    """æ£€æŸ¥åŸºç¡€ç¯å¢ƒ"""
    print_section("åŸºç¡€ç¯å¢ƒæ£€æŸ¥")
    
    results = {}
    
    # Pythonç‰ˆæœ¬
    python_version = f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"
    print_result("Pythonç‰ˆæœ¬", f"{python_version}")
    results['python_version'] = python_version
    
    # PyTorchç‰ˆæœ¬
    torch_version = torch.__version__
    print_result("PyTorchç‰ˆæœ¬", f"{torch_version}")
    results['torch_version'] = torch_version
    
    # CUDAå¯ç”¨æ€§
    cuda_available = torch.cuda.is_available()
    print_result("CUDAå¯ç”¨æ€§", "PASS" if cuda_available else "FAIL", 
                f"CUDAè®¾å¤‡æ•°: {torch.cuda.device_count()}" if cuda_available else "CUDAä¸å¯ç”¨")
    results['cuda_available'] = cuda_available
    
    return results

def check_torch_gcu_availability() -> Dict[str, Any]:
    """æ£€æŸ¥torch_gcuå¯ç”¨æ€§"""
    print_section("torch_gcuå¯ç”¨æ€§æ£€æŸ¥")
    
    results = {}
    
    try:
        # åŠ¨æ€å¯¼å…¥torch_gcuä»¥é¿å…é™æ€åˆ†æé”™è¯¯
        torch_gcu = __import__('torch_gcu')
        print_result("torch_gcuå¯¼å…¥", "PASS", f"ç‰ˆæœ¬: {torch_gcu.__version__}")
        results['torch_gcu_imported'] = True
        results['torch_gcu_version'] = torch_gcu.__version__
        
        # æ£€æŸ¥GCUå¯ç”¨æ€§
        gcu_available = torch_gcu.is_available()
        print_result("GCUè®¾å¤‡å¯ç”¨æ€§", "PASS" if gcu_available else "FAIL")
        results['gcu_available'] = gcu_available
        
        if gcu_available:
            device_count = torch_gcu.device_count()
            print_result("GCUè®¾å¤‡æ•°é‡", f"{device_count}")
            results['gcu_device_count'] = device_count
            
            # æµ‹è¯•åŸºæœ¬å¼ é‡æ“ä½œ
            try:
                # ä½¿ç”¨torch_gcuçš„è®¾å¤‡æ–¹æ³•
                current_device = torch_gcu.current_device()
                device_name = f'gcu:{current_device}'
                x = torch.randn(2, 3, device=device_name)
                y = torch.randn(2, 3, device=device_name)
                z = x + y
                print_result("åŸºæœ¬å¼ é‡æ“ä½œ", "PASS", "GCUå¼ é‡è¿ç®—æ­£å¸¸")
                results['basic_tensor_ops'] = True
            except Exception as e:
                print_result("åŸºæœ¬å¼ é‡æ“ä½œ", "FAIL", str(e))
                results['basic_tensor_ops'] = False
        
    except ImportError as e:
        print_result("torch_gcuå¯¼å…¥", "FAIL", f"å¯¼å…¥å¤±è´¥: {e}")
        results['torch_gcu_imported'] = False
        print("   æç¤º: torch_gcuåªåœ¨ç‡§åŸT20æœåŠ¡å™¨ä¸Šå¯ç”¨")
    except Exception as e:
        print_result("torch_gcuæ£€æŸ¥", "FAIL", f"æ£€æŸ¥å¤±è´¥: {e}")
        results['torch_gcu_imported'] = False
    
    return results

def check_distributed_support() -> Dict[str, Any]:
    """æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ"""
    print_section("åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒæ£€æŸ¥")
    
    results = {}
    
    # æ£€æŸ¥åˆ†å¸ƒå¼åŒ…
    try:
        import torch.distributed as dist
        print_result("torch.distributed", "PASS")
        results['distributed_available'] = True
        
        # æ£€æŸ¥ECCLåç«¯æ”¯æŒ
        try:
            # åŠ¨æ€å¯¼å…¥torch_gcuä»¥é¿å…é™æ€åˆ†æé”™è¯¯
            torch_gcu = __import__('torch_gcu')
            if torch_gcu.is_available():
                # åœ¨GCUç¯å¢ƒä¸­æ£€æŸ¥ECCLåç«¯
                available_backends = []
                for backend in ['eccl', 'gloo', 'nccl']:
                    try:
                        if backend == 'eccl':
                            # ECCLåç«¯éœ€è¦torch_gcuç¯å¢ƒ
                            available_backends.append(backend)
                        elif backend == 'gloo':
                            available_backends.append(backend)
                        elif backend == 'nccl' and torch.cuda.is_available():
                            available_backends.append(backend)
                    except:
                        pass
                
                print_result("å¯ç”¨åç«¯", f"{', '.join(available_backends)}")
                results['available_backends'] = available_backends
                results['eccl_supported'] = 'eccl' in available_backends
            else:
                print_result("åˆ†å¸ƒå¼åç«¯", "WARN", "torch_gcuä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨glooåç«¯")
                results['available_backends'] = ['gloo']
                results['eccl_supported'] = False
        except ImportError:
            print_result("åˆ†å¸ƒå¼åç«¯", "WARN", "torch_gcuä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨glooåç«¯")
            results['available_backends'] = ['gloo']
            results['eccl_supported'] = False
            
    except ImportError as e:
        print_result("torch.distributed", "FAIL", str(e))
        results['distributed_available'] = False
    
    return results

def check_amp_support() -> Dict[str, Any]:
    """æ£€æŸ¥è‡ªåŠ¨æ··åˆç²¾åº¦æ”¯æŒ"""
    print_section("è‡ªåŠ¨æ··åˆç²¾åº¦(AMP)æ”¯æŒæ£€æŸ¥")
    
    results = {}
    
    try:
        # ä¿®å¤å¯¼å…¥é”™è¯¯
        from torch.cuda.amp.autocast_mode import autocast
        from torch.cuda.amp.grad_scaler import GradScaler
        print_result("torch.cuda.amp", "PASS")
        results['amp_available'] = True
        
        # æ£€æŸ¥GCU AMPæ”¯æŒ
        try:
            # åŠ¨æ€å¯¼å…¥torch_gcuä»¥é¿å…é™æ€åˆ†æé”™è¯¯
            torch_gcu = __import__('torch_gcu')
            if torch_gcu.is_available():
                # æµ‹è¯•GCU AMP
                current_device = torch_gcu.current_device()
                device_name = f'gcu:{current_device}'
                x = torch.randn(2, 3, device=device_name)
                y = torch.randn(2, 3, device=device_name)
                
                with autocast():
                    z = x @ y.T
                
                print_result("GCU AMPæ”¯æŒ", "PASS")
                results['gcu_amp_supported'] = True
            else:
                print_result("GCU AMPæ”¯æŒ", "WARN", "torch_gcuä¸å¯ç”¨")
                results['gcu_amp_supported'] = False
        except Exception as e:
            print_result("GCU AMPæ”¯æŒ", "FAIL", str(e))
            results['gcu_amp_supported'] = False
            
    except ImportError as e:
        print_result("AMPæ”¯æŒ", "FAIL", str(e))
        results['amp_available'] = False
    
    return results

def check_profiler_support() -> Dict[str, Any]:
    """æ£€æŸ¥Profileræ”¯æŒ"""
    print_section("Profileræ”¯æŒæ£€æŸ¥")
    
    results = {}
    
    try:
        import torch.profiler
        print_result("torch.profiler", "PASS")
        results['profiler_available'] = True
        
        # æ£€æŸ¥GCU Profileræ”¯æŒ
        try:
            # åŠ¨æ€å¯¼å…¥torch_gcuä»¥é¿å…é™æ€åˆ†æé”™è¯¯
            torch_gcu = __import__('torch_gcu')
            if torch_gcu.is_available():
                # ç®€å•çš„profileræµ‹è¯•
                with torch.profiler.profile() as prof:
                    current_device = torch_gcu.current_device()
                    device_name = f'gcu:{current_device}'
                    x = torch.randn(10, 10, device=device_name)
                    y = x @ x.T
                
                print_result("GCU Profileræ”¯æŒ", "PASS")
                results['gcu_profiler_supported'] = True
                
                # æ£€æŸ¥profilerè¡¨æ ¼è¾“å‡º
                try:
                    table_output = prof.key_averages().table(sort_by="cpu_time_total", row_limit=5)
                    if table_output and len(str(table_output).strip()) > 0:
                        print_result("Profilerè¡¨æ ¼è¾“å‡º", "PASS")
                        results['profiler_table_output'] = True
                    else:
                        print_result("Profilerè¡¨æ ¼è¾“å‡º", "WARN", "è¡¨æ ¼ä¸ºç©º")
                        results['profiler_table_output'] = False
                except Exception as e:
                    print_result("Profilerè¡¨æ ¼è¾“å‡º", "FAIL", str(e))
                    results['profiler_table_output'] = False
            else:
                print_result("GCU Profileræ”¯æŒ", "WARN", "torch_gcuä¸å¯ç”¨")
                results['gcu_profiler_supported'] = False
        except Exception as e:
            print_result("GCU Profileræ”¯æŒ", "FAIL", str(e))
            results['gcu_profiler_supported'] = False
            
    except ImportError as e:
        print_result("Profileræ”¯æŒ", "FAIL", str(e))
        results['profiler_available'] = False
    
    return results

def check_dependencies() -> Dict[str, Any]:
    """æ£€æŸ¥ä¾èµ–é¡¹"""
    print_section("ä¾èµ–é¡¹æ£€æŸ¥")
    
    results = {}
    dependencies = ['numpy', 'opencv-python', 'pillow', 'matplotlib']
    
    for dep in dependencies:
        try:
            __import__(dep.replace('-', '_'))
            print_result(f"{dep}", "PASS")
            results[dep] = True
        except ImportError:
            print_result(f"{dep}", "FAIL", "æœªå®‰è£…")
            results[dep] = False
    
    return results

def print_recommendations():
    """æ‰“å°ä½¿ç”¨å»ºè®®"""
    print_section("ä½¿ç”¨å»ºè®®")
    
    print("ğŸ“‹ åŸºäºç‡§åŸå®˜æ–¹æ–‡æ¡£çš„è¿ç§»å»ºè®®:")
    print()
    print("1. ğŸ”„ åç«¯é…ç½®:")
    print("   - ä½¿ç”¨ torch_gcu.is_available() æ£€æµ‹GCUç¯å¢ƒ")
    print("   - GCUç¯å¢ƒä¸‹ä½¿ç”¨ backend='eccl'")
    print("   - éGCUç¯å¢ƒä¸‹é™çº§åˆ° backend='gloo'")
    print()
    print("2. ğŸ¯ è®¾å¤‡ç®¡ç†:")
    print("   - ä½¿ç”¨ device=f'gcu:{torch_gcu.current_device()}'")
    print("   - é¿å…ä½¿ç”¨ .gcu() æ–¹æ³•ï¼Œæ”¹ç”¨ .to(device)")
    print()
    print("3. âš¡ æ€§èƒ½ä¼˜åŒ–:")
    print("   - è®¾ç½®ç¯å¢ƒå˜é‡:")
    print("     export PYTORCH_GCU_ALLOC_CONF='backend:topsMallocAsync'")
    print("     export TORCH_ECCL_AVOID_RECORD_STREAMS='false'")
    print("     export TORCH_ECCL_ASYNC_ERROR_HANDLING='3'")
    print()
    print("4. ğŸš€ å¯åŠ¨è®­ç»ƒ:")
    print("   - ä½¿ç”¨äº¤äº’å¼å¯åŠ¨è„šæœ¬:")
    print("     ./scripts/start_8card_training_interactive_official.sh")

def main() -> bool:
    """ä¸»å‡½æ•°"""
    print("ğŸ” ç‡§åŸT20 torch_gcuç¯å¢ƒéªŒè¯")
    print("åŸºäºå®˜æ–¹æ–‡æ¡£: https://support.enflame-tech.com/onlinedoc_dev_3.5/3-model/infer/torch_gcu/torch_gcu2.5/content/source/torch_gcu_user_guide.html#id4")
    
    all_results = {}
    
    # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
    all_results['basic'] = check_basic_environment()
    all_results['torch_gcu'] = check_torch_gcu_availability()
    all_results['distributed'] = check_distributed_support()
    all_results['amp'] = check_amp_support()
    all_results['profiler'] = check_profiler_support()
    all_results['dependencies'] = check_dependencies()
    
    # æ‰“å°å»ºè®®
    print_recommendations()
    
    # æ€»ç»“
    print_section("éªŒè¯æ€»ç»“")
    
    torch_gcu_available = all_results['torch_gcu'].get('torch_gcu_imported', False)
    gcu_available = all_results['torch_gcu'].get('gcu_available', False)
    
    if torch_gcu_available and gcu_available:
        print("âœ… torch_gcuç¯å¢ƒå®Œå…¨å¯ç”¨")
        print("âœ… å¯ä»¥ä½¿ç”¨ECCLåç«¯è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        return True
    elif torch_gcu_available:
        print("âš ï¸ torch_gcuå·²å®‰è£…ä½†GCUè®¾å¤‡ä¸å¯ç”¨")
        print("âš ï¸ å°†ä½¿ç”¨glooåç«¯è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        return True
    else:
        print("âŒ torch_gcuä¸å¯ç”¨")
        print("âŒ å°†ä½¿ç”¨glooåç«¯è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒ")
        print("ğŸ’¡ è¿™åœ¨éT20æœåŠ¡å™¨ç¯å¢ƒä¸‹æ˜¯æ­£å¸¸çš„")
        return True  # åœ¨éGCUç¯å¢ƒä¸‹ä¹Ÿç®—æˆåŠŸ

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)