#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºç‡§åŸå®˜æ–¹æ–‡æ¡£çš„torch_gcuç¯å¢ƒéªŒè¯è„šæœ¬
å‚è€ƒ: https://support.enflame-tech.com/onlinedoc_dev_3.5/3-model/infer/torch_gcu/torch_gcu2.5/content/source/torch_gcu_user_guide.html
"""

import os
import sys
import subprocess

def check_basic_environment():
    """æ£€æŸ¥åŸºç¡€ç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥åŸºç¡€ç¯å¢ƒ...")
    print(f"  - Pythonç‰ˆæœ¬: {sys.version}")
    
    # æ£€æŸ¥PyTorch
    try:
        import torch
        print(f"  - PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œtorch_gcuæ”¯æŒPyTorch v2.5.1
        if torch.__version__.startswith('2.5'):
            print("  âœ… PyTorchç‰ˆæœ¬å…¼å®¹ (æ”¯æŒv2.5.1)")
        else:
            print(f"  âš ï¸ PyTorchç‰ˆæœ¬å¯èƒ½ä¸å…¼å®¹ï¼Œæ¨èv2.5.1ï¼Œå½“å‰: {torch.__version__}")
            
    except ImportError:
        print("  âŒ PyTorchæœªå®‰è£…")
        return False
    
    return True

def check_torch_gcu_official():
    """æ ¹æ®å®˜æ–¹æ–‡æ¡£æ£€æŸ¥torch_gcu"""
    print("\nğŸ” æ£€æŸ¥torch_gcu (å®˜æ–¹æ–¹æ³•)...")
    
    try:
        # å®˜æ–¹æ–‡æ¡£æ¨èçš„æ£€æŸ¥æ–¹æ³•
        import torch
        import torch_gcu
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨
        is_available = torch_gcu.is_available()
        print(f"  - torch_gcu.is_available(): {is_available}")
        
        if is_available:
            print("  âœ… torch_gcuå®‰è£…æˆåŠŸï¼Œä¸”è®¾å¤‡å¯ç”¨")
            
            # è·å–è®¾å¤‡æ•°é‡
            device_count = torch_gcu.device_count()
            print(f"  - å¯ç”¨GCUè®¾å¤‡æ•°: {device_count}")
            
            # æµ‹è¯•åŸºæœ¬æ“ä½œ
            try:
                # åˆ›å»ºtensorå¹¶ç§»åŠ¨åˆ°GCU
                a = torch.tensor([1, 2, 3]).gcu()
                b = torch.tensor([1, 2, 3]).to("gcu")
                c = torch.tensor([1, 2, 3], device="gcu")
                
                print("  âœ… GCU tensoråˆ›å»ºæˆåŠŸ")
                print(f"    - a.device: {a.device}")
                print(f"    - b.device: {b.device}")
                print(f"    - c.device: {c.device}")
                
                # æµ‹è¯•åŸºæœ¬è¿ç®—
                result = a + b
                print(f"  âœ… GCUè¿ç®—æµ‹è¯•æˆåŠŸ: {result}")
                
            except Exception as e:
                print(f"  âŒ GCUæ“ä½œæµ‹è¯•å¤±è´¥: {e}")
                return False
                
        else:
            print("  âŒ torch_gcuä¸å¯ç”¨")
            return False
            
    except ImportError as e:
        print(f"  âŒ torch_gcuå¯¼å…¥å¤±è´¥: {e}")
        print("  ğŸ’¡ è¯·æ£€æŸ¥torch_gcuæ˜¯å¦æ­£ç¡®å®‰è£…")
        return False
    except Exception as e:
        print(f"  âŒ torch_gcuæ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return True

def check_distributed_support():
    """æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ"""
    print("\nğŸ” æ£€æŸ¥åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ...")
    
    try:
        import torch.distributed as dist
        
        # æ£€æŸ¥ECCLåç«¯æ”¯æŒ
        print("  - æ£€æŸ¥ECCLåç«¯æ”¯æŒ...")
        
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œéœ€è¦å°†backendä»ncclæ”¹ä¸ºeccl
        print("  ğŸ’¡ æ ¹æ®å®˜æ–¹æ–‡æ¡£:")
        print("    torch.distributed.init_process_group(backend='eccl', ...)")
        
        # æ£€æŸ¥ç›¸å…³ç¯å¢ƒå˜é‡
        env_vars = [
            'TORCH_ECCL_AVOID_RECORD_STREAMS',
            'TORCH_ECCL_ASYNC_ERROR_HANDLING',
            'PYTORCH_GCU_ALLOC_CONF'
        ]
        
        print("  - æ£€æŸ¥ECCLç›¸å…³ç¯å¢ƒå˜é‡:")
        for var in env_vars:
            value = os.environ.get(var, 'æœªè®¾ç½®')
            print(f"    - {var}: {value}")
        
        print("  âœ… åˆ†å¸ƒå¼é…ç½®æ£€æŸ¥å®Œæˆ")
        
    except Exception as e:
        print(f"  âŒ åˆ†å¸ƒå¼æ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return True

def check_amp_support():
    """æ£€æŸ¥AMPæ”¯æŒ"""
    print("\nğŸ” æ£€æŸ¥AMPæ”¯æŒ...")
    
    try:
        import torch
        import torch_gcu
        
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œéœ€è¦ä½¿ç”¨torch.gcu.ampè€Œä¸æ˜¯torch.cuda.amp
        print("  ğŸ’¡ æ ¹æ®å®˜æ–¹æ–‡æ¡£:")
        print("    ä½¿ç”¨ torch.gcu.amp.autocast() æ›¿ä»£ torch.cuda.amp.autocast()")
        print("    ä½¿ç”¨ torch.gcu.amp.GradScaler() æ›¿ä»£ torch.cuda.amp.GradScaler()")
        
        # æµ‹è¯•AMPåŠŸèƒ½
        if torch_gcu.is_available():
            try:
                # æµ‹è¯•autocast
                with torch.gcu.amp.autocast():
                    a = torch.randn(10, 10).gcu()
                    b = torch.randn(10, 10).gcu()
                    c = torch.mm(a, b)
                
                print("  âœ… torch.gcu.amp.autocast() æµ‹è¯•æˆåŠŸ")
                
                # æµ‹è¯•GradScaler
                scaler = torch.gcu.amp.GradScaler()
                print("  âœ… torch.gcu.amp.GradScaler() åˆ›å»ºæˆåŠŸ")
                
            except Exception as e:
                print(f"  âŒ AMPåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
                return False
        else:
            print("  âš ï¸ torch_gcuä¸å¯ç”¨ï¼Œè·³è¿‡AMPæµ‹è¯•")
            
    except Exception as e:
        print(f"  âŒ AMPæ£€æŸ¥å¤±è´¥: {e}")
        return False
    
    return True

def check_profiler_support():
    """æ£€æŸ¥Profileræ”¯æŒ"""
    print("\nğŸ” æ£€æŸ¥Profileræ”¯æŒ...")
    
    try:
        import torch
        import torch_gcu
        
        if torch_gcu.is_available():
            # æ ¹æ®å®˜æ–¹æ–‡æ¡£çš„profilerç¤ºä¾‹
            size = (100, 100, 100)
            
            with torch.autograd.profiler.profile() as prof:
                a = torch.randn(size).gcu()
                b = torch.randn(size).gcu()
                for i in range(3):
                    c = a + b
            
            # è·å–æ€§èƒ½ç»Ÿè®¡
            table = prof.table()
            print("  âœ… Profileræµ‹è¯•æˆåŠŸ")
            print("  ğŸ“Š æ€§èƒ½ç»Ÿè®¡è¡¨æ ¼ (å‰5è¡Œ):")
            lines = table.split('\n')[:6]
            for line in lines:
                print(f"    {line}")
                
        else:
            print("  âš ï¸ torch_gcuä¸å¯ç”¨ï¼Œè·³è¿‡Profileræµ‹è¯•")
            
    except Exception as e:
        print(f"  âŒ Profileræµ‹è¯•å¤±è´¥: {e}")
        return False
    
    return True

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–é¡¹"""
    print("\nğŸ” æ£€æŸ¥ç‡§åŸè½¯ä»¶ä¾èµ–...")
    
    # æ ¹æ®å®˜æ–¹æ–‡æ¡£çš„ä¾èµ–é¡¹
    dependencies = [
        'topsruntime',
        'eccl', 
        'topsaten',
        'sdk'
    ]
    
    print("  ğŸ’¡ æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œéœ€è¦ä»¥ä¸‹ä¾èµ–:")
    for dep in dependencies:
        print(f"    - {dep}")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    tops_vars = [
        'TOPS_VISIBLE_DEVICES',
        'TOPS_HOME',
        'LD_LIBRARY_PATH'
    ]
    
    print("  - æ£€æŸ¥TOPSç›¸å…³ç¯å¢ƒå˜é‡:")
    for var in tops_vars:
        value = os.environ.get(var, 'æœªè®¾ç½®')
        if value != 'æœªè®¾ç½®':
            print(f"    âœ… {var}: {value}")
        else:
            print(f"    âš ï¸ {var}: æœªè®¾ç½®")

def print_recommendations():
    """æ‰“å°ä½¿ç”¨å»ºè®®"""
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®® (åŸºäºå®˜æ–¹æ–‡æ¡£):")
    print("=" * 50)
    
    print("1. ä»£ç è¿ç§»:")
    print("   - å°† .cuda() æ”¹ä¸º .gcu()")
    print("   - å°† .to('cuda') æ”¹ä¸º .to('gcu')")
    print("   - å°† torch.cuda.xxx æ”¹ä¸º torch.gcu.xxx")
    
    print("\n2. åˆ†å¸ƒå¼è®­ç»ƒ:")
    print("   - å°† backend='nccl' æ”¹ä¸º backend='eccl'")
    print("   - å…¶ä»–torch.distributed.xxxæ¥å£ä¿æŒä¸å˜")
    
    print("\n3. AMPä½¿ç”¨:")
    print("   - å°† torch.cuda.amp æ”¹ä¸º torch.gcu.amp")
    
    print("\n4. ç¯å¢ƒå˜é‡è®¾ç½®:")
    print("   - PYTORCH_GCU_ALLOC_CONF='backend:topsMallocAsync'")
    print("   - TORCH_ECCL_AVOID_RECORD_STREAMS=false")
    print("   - TORCH_ECCL_ASYNC_ERROR_HANDLING=3")
    
    print("\n5. è°ƒè¯•ä¿¡æ¯:")
    print("   - ENFLAME_LOG_DEBUG_LEVEL='DEBUG'")
    print("   - ENFLAME_LOG_DEBUG_MOD='TORCH_GCU/OP,TORCH_GCU/FALLBACK'")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ torch_gcuç¯å¢ƒéªŒè¯ (åŸºäºç‡§åŸå®˜æ–¹æ–‡æ¡£)")
    print("=" * 60)
    
    success = True
    
    # æ£€æŸ¥å„é¡¹åŠŸèƒ½
    success &= check_basic_environment()
    success &= check_torch_gcu_official()
    success &= check_distributed_support()
    success &= check_amp_support()
    success &= check_profiler_support()
    check_dependencies()
    
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ torch_gcuç¯å¢ƒéªŒè¯é€šè¿‡ï¼")
        print("âœ… å¯ä»¥å¼€å§‹ä½¿ç”¨ç‡§åŸT20è¿›è¡Œè®­ç»ƒ")
    else:
        print("âŒ torch_gcuç¯å¢ƒéªŒè¯å¤±è´¥")
        print("ğŸ’¡ è¯·å‚è€ƒä¸Šè¿°é”™è¯¯ä¿¡æ¯è¿›è¡Œä¿®å¤")
    
    print_recommendations()
    
    return success

if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)